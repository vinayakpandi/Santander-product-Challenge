# -*- coding: utf-8 -*-
"""Project-5-Bank-Churners.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mH3_TsV01hW--pobcjfiXIn-ElrTI5H2

# Problem Statement


## Background & Context

The Thera bank recently saw a steep decline in the number of users of their credit card, credit cards are a good source of income for banks because of different kinds of fees charged by the banks like annual fees, balance transfer fees, and cash advance fees, late payment fees, foreign transaction fees, and others. Some fees are charged to every user irrespective of usage, while others are charged under specified circumstances.

Customers’ leaving credit cards services would lead bank to loss, so the bank wants to analyze the data of customers and identify the customers who will leave their credit card services and reason for same – so that bank could improve upon those areas

You as a Data scientist at Thera bank need to come up with a classification model that will help the bank improve its services so that customers do not renounce their credit cards

You need to identify the best possible model that will give the required performance

## Objective

Explore and visualize the dataset.
Build a classification model to predict if the customer is going to churn or not
Optimize the model using appropriate techniques
Generate a set of insights and recommendations that will help the bank
Data Dictionary:

CLIENTNUM: Client number. Unique identifier for the customer holding the account

Attrition_Flag: Internal event (customer activity) variable - if the account is closed then "Attrited Customer" else "Existing Customer"

Customer_Age: Age in Years

Gender: Gender of the account holder

Dependent_count: Number of dependents

Education_Level:  Educational Qualification of the account holder - Graduate, High School, Unknown, Uneducated, College(refers to a college student), Post-Graduate, Doctorate.

Marital_Status: Marital Status of the account holder

Income_Category: Annual Income Category of the account holder

Card_Category: Type of Card

Months_on_book: Period of relationship with the bank

Total_Relationship_Count: Total no. of products held by the customer

Months_Inactive_12_mon: No. of months inactive in the last 12 months

Contacts_Count_12_mon: No. of Contacts between the customer and bank in the last 12 months

Credit_Limit: Credit Limit on the Credit Card

Total_Revolving_Bal: The balance that carries over from one month to the next is the revolving balance

Avg_Open_To_Buy: Open to Buy refers to the amount left on the credit card to use (Average of last 12 months)

Total_Trans_Amt: Total Transaction Amount (Last 12 months)

Total_Trans_Ct: Total Transaction Count (Last 12 months)

Total_Ct_Chng_Q4_Q1: Ratio of the total transaction count in 4th quarter and the total transaction count in 1st quarter

Total_Amt_Chng_Q4_Q1: Ratio of the total transaction amount in 4th quarter and the total transaction amount in 1st quarter

Avg_Utilization_Ratio: Represents how much of the available credit the customer spent

 

## Best Practices for Notebook : 

The notebook should be well-documented, with inline comments explaining the functionality of code and markdown cells containing comments on the observations and insights.
The notebook should be run from start to finish sequentially before submission.
It is preferable to remove all warnings and errors before submission.
 

## Submission Guidelines :

The submission should be: well commented Jupyter notebook [format - .HTML] - Please run the notebook sequentially before submitting.
Any assignment found copied/ plagiarized with other groups will not be graded and awarded zero marks
Please ensure timely submission as any submission post-deadline will not be accepted for evaluation
Submission will not be evaluated if,
it is submitted post-deadline, or,
more than 1 files are submitted
Happy Learning!!

Scoring guide (Rubric) - CreditCard Users Churn Prediction
Criteria	Points
Exploratory Data Analysis and Insights
- Problem definition, questions to be answered - Data background and contents - Univariate analysis - Bivariate analysis - Key meaningful observations on individual variables and the relationship between variables
8
Data Pre-processing
- Prepare the data for analysis - Feature Engineering - Missing value Treatment - Outlier Treatment Note: Please ensure no data leakage occurs among train-test and validation sets
6
Model building
Build 6 models (from logistic regression, decision trees, bagging and boosting methods) Note: You can choose not to build XGBoost if you are facing issues with the installation
6
Model tuning using Random search
- Choose metric of interest - Choose 3 best models and provide the reason for the same - Tune the best 3 models obtained above using randomized search and metric of interest - Check the performance of 3 tuned models
9
Model building - Oversampled data
- Oversample the train data - Fit 3 tuned models on oversampled data - Check the performance of the 3 models
6
Model building - Undersampled data
- Undersample the train data - Fit 3 tuned models on undersampled data - Check the performance of the 3 models
6
Model Performances
- Compare the performance of tuned models on the original, oversampled, and undersampled data - Choose the best model - Recall on the test set is expected to be > 0.95, and precision and accuracy is expected to be > 0.70
6
Productionize the model
- Create a final model using pipelines
3
Actionable Insights & Recommendations
- Business recommendations and insights
5
Notebook - Overall quality
- Structure and flow - Well commented code
5
Points	60

# The answer - Project Solution by Sanjib:

Author : Sanjib Basu

Date : July-25-2021

Note: The Solution has sections A, B, C, etc. and their sub-sections A.1, C.2, etc.


## <a id="link0">Index</a>


- <a href=#link1> A. Overview of the dataset </a>
    - <a href=#link1-1> A.1 Data Cleansing</a>
    - <a href=#link1-2> A.2 Optimize Memory of Dataframe</a>

- <a href=#link2> B. Exploratory Data Analysis</a> 
    - <a href=#link2-1> B.1 Univariate Analysis </a>
    - <a href=#link2-2> B.2 Multivariate Analysis </a>

- <a href =#link3> C. Data Engineering</a>
    - <a href =#link3-1> C.1 KNN Data Imputer Preparation</a>
    - <a href =#link3-2> C.2 Data Split</a>
    - <a href =#link3-3> C.3 Categorical Variable Encoding</a>
    - <a href =#link3-4> C.4 Scaling of Data</a>

- <a href =#link4> D. Build Models with various algorithms</a>
    - <a href=#link4-1> D.1 Logistics Regression Classifiers</a> 
    - <a href=#link4-2> D.2 Decision Tree Classifiers</a>
    - <a href=#link4-3> D.3 Ensemble Method : Bagging Classifiers</a>
    - <a href=#link4-4> D.4 Ensemble Method : Boosting Classifiers</a>
    - <a href=#link4-5> D.5 Stacking Classifiers</a>
    
- <a href =#link5> E. Model Selection and Analysis</a> 
    - <a href =#link5-1> E.1 Feature Importance</a> 
    - <a href =#link5-2> E.2 Model performance on unseen data</a> 

- <a href = #link6> F. Build Pipeline on the selected Model</a>

- <a href = #link7> G. Business Recomendation</a>
    - <a href = #link7-1>G.1 Insights on Customers of what profile (characteristics) are more likely to churn</a>
    - <a href = #link7-2>G.2 Insights on Customer profiles (characeristics) per Product</a>
    - <a href = #link7-3>G.3 Insights on prediction of Attrition</a>

- <a href = #link8> H. Appendix</a>

# <a id="link1">A. Overview of the dataset </a> 


*Go to <a href=#link0>Index</a>*


## General observations on the problem (use case) overall:

1. Following additional data would have helped both the descriptive and predictive analytics: date of attrition (seasonality etc.), various fees total and frequency, fees structure per credit card product category, variable income of cutomer, customer rating on the credit card products, customers other credit cards from compettitors and details, customer credit score, customer address and other geographic information, etc.

2. If following model predicts Customer to churn(attrition), and actually the customer does not churn (FP), business wise that situation is not too bad; because the targetting mitigations of attrition to some additional customers costs low. Also, the model would help to target optimized (smaller) number of customers for mitigation.


**If Customer actually churn, and model prediction is negetive (FN), business wise that situation is severly impactful, costs high as missing oppertunity. Hence, the model need to minimize FN, i.e. improve Recall score. Also, overall cost of mitigation would reduce with higher Accuracy and ROC-AUC of the model prediction. The model having best Recall and ROC-AUC on test data would be most valuable to business, Precision is not so important.**

3. The population size is unkown. We assume that given sample data is a good representation of overall population.

4. Besides churning, the insights based on Customer profiles and Product profiles (market segments) could be valuable to business.
"""

# Commented out IPython magic to ensure Python compatibility.
# python code style
# %load_ext nb_black

# Commented out IPython magic to ensure Python compatibility.
# import necessary packages

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas_profiling as pf

from IPython.display import display, HTML

import json

# from re import search

from numpy import median

# sklearn lib for Logistic Regression
from sklearn.linear_model import LogisticRegression, Ridge, Lasso
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve

import category_encoders as ce

# sklearn library for Feature Scaling
from sklearn.preprocessing import MinMaxScaler

# from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split 

# To build sklearn model
from sklearn.linear_model import LogisticRegression

# To get diferent metric scores
from sklearn import metrics
from sklearn.metrics import f1_score,accuracy_score, recall_score, precision_score, \
roc_auc_score, roc_curve, confusion_matrix, precision_recall_curve, make_scorer

# imbalanced data-set
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

# To check model performance
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# extended lib for feature selection
from mlxtend.feature_selection import SequentialFeatureSelector as SFS

# to plot the performance with addition of each feature
from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs

# Library to impute missing values
from sklearn.impute import KNNImputer

# Tree based classifiers
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier

# Model hyper parameter tuning
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# Boosting classifiers
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
import xgboost as xgb


from sklearn.ensemble import StackingClassifier

from sklearn.pipeline import Pipeline, make_pipeline

import warnings
warnings.filterwarnings("ignore")

# matplotlib plot fits into jupyter notebook cell, without plt.show()
# %matplotlib inline 

pd.set_option('display.max_colwidth', None) # prints the DataFrame cells with full text, none of its values truncated.
pd.set_option('display.max_columns', 100) # prints upto 100 columns
pd.set_option('display.max_rows', 200) # prints upto 200 rows
pd.set_option('display.colheader_justify', 'left') # dataframe column header left justified

pd.options.display.float_format = (
    "{:,.4f}".format
)  # format upto 4 decimals of float numbers in DataFrame

sns.set(color_codes=True) # keeps the colored grid in the plot background 
sns.set_context("talk") # talk : bolds the plots
print(plt.style.available)

plt.style.use("seaborn-notebook")  # set a default style while plotting

df_bank = pd.read_csv("BankChurners.csv")
df_bank_original = df_bank.copy(deep=True)  # save a backup
df_bank  # first look

df_bank.describe(include="all").T  # data distribution

"""
Observation by Sanjib:
    1. Above values of Min and Max seemed feasible, no need of correction.
    2. Attrition_Flag is the target varibale.
"""
df_bank.info()

df_bank[df_bank.duplicated()].count()  # check duplicate

"""### Observation by Sanjib:

1. Column Education Level and Marital Status have missing values. The proportion of missing values are significant (>5% of sample  size).
2. Attrition_Flag is the target vaiable with no missing values. It needs be converted into boolean type.
3. We would try to fill up missing values.
"""

# check class imbalance
df_bank["Attrition_Flag"].value_counts(normalize=True)

"""### Observation by Sanjib:

1. The class Attrition has imblalance 
2. Attrited customers are minority about 16% of the dataset

##  <a id ="link1-1">A.1 Data Cleansing</a>

*Go to [Index](#link0)*

Treat missing values per variable
"""

# CLIENTNUM is irrelevant column containing no information.

df_bank.drop(labels=["CLIENTNUM"], axis=1, inplace=True)
df_bank.sample(10)

"""### A.1.1 Analysis on Variable 'Education Level'

Clean variable Education Level
"""

# number of missing values in Education_Level
df_bank.shape[0] - df_bank["Education_Level"].count()

df_bank[df_bank["Education_Level"].isnull()].sample(
    10
)  # Education_Level is missing, sample 10 records

# check Education_Level distribution for various Income_Category
pd.pivot_table(
    df_bank,
    columns="Education_Level",
    index="Income_Category",
    values="Customer_Age",
    aggfunc=np.count_nonzero,
)

"""
    The value of Income_Category does not conclusively indicate Education_Level, by majority.
    Also, Education_Level is an ordinal categorical variable.
    We'll try later KNN imputer to fill missing values, after spliting dataset into train-validation-test.
"""

df_bank["Education_Level"].value_counts()

# Ordinal variable label encoding
replace_dict = {
    "Education_Level": {
        "Uneducated": 1,
        "High School": 2,
        "College": 3,
        "Graduate": 4,
        "Post-Graduate": 5,
        "Doctorate": 6,
        np.nan: np.nan,  # missing values would be treated later
    },
}
df_bank = df_bank.replace(replace_dict)
# We can not downcast the data type into "uint8",
# because "uint8" data type does not support value np.nan
df_bank.info()

df_bank["Education_Level"].value_counts(sort=True, ascending=True)

"""### A.1.2 Analyze Variable 'Marital_Status'

Missing values are present in variable Marital Status
"""

df_bank["Marital_Status"].value_counts()

# number of missing values in Education_Level
df_bank.shape[0] - df_bank["Marital_Status"].count()

df_bank[df_bank["Marital_Status"].isnull()].sample(
    10
)  # Marital_Status is missing, sample 10 records

# check Education_Level distribution for various Dependent_count
pd.pivot_table(
    df_bank,
    columns="Marital_Status",
    index="Dependent_count",
    values="Customer_Age",
    aggfunc=np.count_nonzero,
)

# check Education_Level distribution for various Dependent_count
pd.pivot_table(
    df_bank,
    columns="Marital_Status",
    index="Gender",
    values="Customer_Age",
    aggfunc=np.count_nonzero,
)

"""
    The value of Dependent_count or Marital_Status do not conclusively indicate Marital_Status, by majority.
    We'll fill missing values later by KNN imputing later, after spliting the dataset train-validation-test.
    Also, Marital_Status is an nominal categorical variable.
"""

df_bank["Marital_Status"].value_counts()

"""### A.1.3 Analyze Variable 'Income_Category '

This categorical varibale may be important.
"""

df_bank["Income_Category"].value_counts()

"""
    Income Category appears to be ordinal categorical variable.
    The value "abc" needs to be replaced, which seems to be unknown.
    Let us check distrubution of Total_Trans_Amt, on Income_Category
""" 
df_bank[["Income_Category","Total_Trans_Amt"]].groupby(by="Income_Category").describe()

# Let us check distrubution of Credit_Limit, on Income_Category
df_bank[["Income_Category", "Credit_Limit"]].groupby(by="Income_Category").describe()

# Let us check distrubution of Card_Category, on Income_Category
pd.pivot_table(
    df_bank,
    index="Income_Category",
    columns="Card_Category",
    values="Customer_Age",
    aggfunc=np.count_nonzero,
)

df_bank.query("Income_Category == 'abc'").sample(10)  # check 10 sample records of "abc"

"""
    There are sigficant number of missing values ("abc"). 
    We'll replace later by imputing with KNN, after spliting data into train-validation-test.
    Income_category is an ordinal variable. Let us put ordinal labelling.
"""

# Ordinal variable label encoding
replace_dict = {
    "Income_Category": {
        "Less than $40K": 1,
        "$40K - $60K": 2,
        "$60K - $80K": 3,
        "$80K - $120K": 4,
        "$120K +": 5,
        "abc": np.nan,  # missing values would be treated later
    },
}
df_bank = df_bank.replace(replace_dict)
df_bank.info()

df_bank["Income_Category"].value_counts()

"""## <a id="link1-2">A.2 Optimize memory of Dataframe </a>

*Go To <a href=#link0>index</a>*

Convert Data Types to smaller ones.

### Reference of all NumPy Standard Data Type:


| Data type	| Description |
| --- | --- |
| bool_	| Boolean (True or False) stored as a byte |
| int_ | Default integer type (same as C long; normally either int64 or int32) |
| intc | Identical to C int (normally int32 or int64) |
| intp | Integer used for indexing (same as C ssize_t; normally either int32 or int64) |
| int8 | Byte (-128 to 127) |
| int16 | Integer (-32768 to 32767)
| int32 | Integer (-2147483648 to 2147483647) |
| int64 | Integer (-9223372036854775808 to 9223372036854775807) |
| uint8 | Unsigned integer (0 to 255) |
| uint16 | Unsigned integer (0 to 65535) |
| uint32 | Unsigned integer (0 to 4294967295) |
| uint64 | Unsigned integer (0 to 18446744073709551615)|
| float_ | Shorthand for float64. |
| float16 | Half precision float: sign bit, 5 bits exponent, 10 bits mantissa |
| float32 | Single precision float: sign bit, 8 bits exponent, 23 bits mantissa |
| float64 | Double precision float: sign bit, 11 bits exponent, 52 bits mantissa |
| complex_ | Shorthand for complex128. |
| complex64 |	Complex number, represented by two 32-bit floats |
| complex128 |	Complex number, represented by two 64-bit floats |


source : http://omz-software.com/pythonista/numpy/user/basics.types.html

Reference : https://numpy.org/doc/stable/user/basics.types.html
"""

df_bank.describe().T  # observe min and max

df_bank.info()

"""
    Reduce memmory by down casting data types
"""
df_bank = df_bank.replace(
    {"Attrition_Flag": {"Attrited Customer": True, "Existing Customer": False}}
)
df_bank["Customer_Age"] = df_bank["Customer_Age"].astype("uint8")
df_bank["Dependent_count"] = df_bank["Dependent_count"].astype("uint8")
# Edication Level has np.nan values, and hence would be treated later.
# df_bank["Education_Level"] = df_bank["Education_Level"].astype("uint8")
df_bank["Months_on_book"] = df_bank["Months_on_book"].astype("uint8")
df_bank["Total_Relationship_Count"] = df_bank["Total_Relationship_Count"].astype(
    "uint8"
)
df_bank["Months_Inactive_12_mon"] = df_bank["Months_Inactive_12_mon"].astype("uint8")
df_bank["Contacts_Count_12_mon"] = df_bank["Contacts_Count_12_mon"].astype("uint8")
df_bank["Credit_Limit"] = df_bank["Credit_Limit"].astype("float32")
df_bank["Total_Revolving_Bal"] = df_bank["Total_Revolving_Bal"].astype("uint16")
df_bank["Avg_Open_To_Buy"] = df_bank["Avg_Open_To_Buy"].astype("float32")
df_bank["Total_Amt_Chng_Q4_Q1"] = df_bank["Total_Amt_Chng_Q4_Q1"].astype("float16")
df_bank["Total_Trans_Amt"] = df_bank["Total_Trans_Amt"].astype("uint16")
df_bank["Total_Trans_Ct"] = df_bank["Total_Trans_Ct"].astype("uint8")
df_bank["Total_Ct_Chng_Q4_Q1"] = df_bank["Total_Ct_Chng_Q4_Q1"].astype("float16")
df_bank["Avg_Utilization_Ratio"] = df_bank["Avg_Utilization_Ratio"].astype("float16")

for col in df_bank.select_dtypes(include="object"):
    df_bank[col] = df_bank[col].astype("category")

df_bank.info()  # note the memory usage below

"""# <a id="link2"> B. Exploratory Data Analysis </a>

*Go to <a href = #link0>Index</a>*

Descriptive Analytics

## <a id="link2-1"> B.1 Univariate Analysis </a>
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Univariate analysis of all numeric variables, focused on Attrition_Flag
# 
# for col in df_bank.select_dtypes(exclude="category"):
#     if col != "Attrition_Flag":  # Attrition_Flag is a hue
#         print("*" * 80)
#         plt.title("Following plots are on " + col)
#         sns.histplot(data=df_bank, hue="Attrition_Flag", x=col, kde=True, multiple="stack")
#         plt.show()
#         sns.catplot(
#             data=df_bank.query("Attrition_Flag == True"),
#             x=col,
#             height=6.5,
#             kind="box",
#             color="Orange",
#         )
#         plt.ylabel("Box plot (Attrition) : \n" + col)
#         plt.show()
#         sns.catplot(
#             data=df_bank.query("Attrition_Flag == False"),
#             x=col,
#             height=6,
#             kind="box",
#             color="Blue",
#         )
#         plt.ylabel("Box plot (No Attrition) : \n" + col)
#         plt.show()
#         
#         plt.title("Attrition vs. Non Attrition")
#         sns.pointplot(data=df_bank, x="Attrition_Flag", y=col, hue="Card_Category",
#                       size=6, dodge=True)
#         plt.show()
#                 
#         plt.title("Income Categories")    
#         sns.pointplot(data=df_bank,
#               x="Income_Category", y=col, hue="Card_Category",
#               size=7, dodge=True)
#         
#         plt.show()
# 
# # Note: Please ignore some plots below

"""### Observation by Sanjib:
    1. Some of the independent variables have outliers.
    2. Oulier treatment is necessary for Logistics Regression model, not for models based on Decision Trees
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Univariate analysis of all categorical variables, focused on Products 
# 
# for col in df_bank.select_dtypes(exclude="category"):
#     if col != "Card_Category":  # Card_Category is a hue
#         plt.figure(figsize=(10,5))
#         sns.histplot(
#             data=df_bank, hue="Card_Category", x=col, kde=True, bins="auto", multiple="dodge"
#         )
#         plt.show()
#         
# #         plt.figure(figsize=(10,5))
#         sns.catplot(data=df_bank, y="Card_Category", x=col, height=9, kind="box")
#         plt.ylabel("Box plot : \n" + col)
#         plt.show()
#         
#

df_bank["Card_Category"].sample(10)

# plot only "Attrition_Flag == True", hue products
# Observe the characteristics where Attrition is high


for col in df_bank.select_dtypes(exclude="category"):
    
    if col not in ["Credit_Limit", "Total_Revolving_Bal", "Avg_Open_To_Buy", 
                   "Total_Amt_Chng_Q4_Q1", "Total_Trans_Amt", "Total_Trans_Ct", 
                   "Total_Ct_Chng_Q4_Q1", "Avg_Utilization_Ratio"]:  

        plt.figure(figsize=(20, 5))
        plt.xticks(rotation=45)
        plt.tick_params(axis='x', which='major', labelsize=10)
        sns.countplot(    
            #             data=df_tour.query("ProdTaken == 1"),
            data=df_bank[df_bank["Attrition_Flag"] == True],
            hue="Card_Category",
            x=col,

        )
        plt.title("Only Attrition")
        plt.show()
        
        plt.title("Only Attrition")
        sns.barplot(
            data=df_bank[df_bank["Attrition_Flag"] == True],
            x="Card_Category",
            y=col,
        )
        plt.ylabel("Box plot (No Attrition) : \n" + col)
        plt.show()
        
        plt.title("Only Attrition")    
        sns.pointplot(data=df_bank[df_bank["Attrition_Flag"] == True],
                      x="Income_Category", y=col, hue="Card_Category",
                      size=7, dodge=True)
        plt.show()

        
    else: # following variables need bins to represnt x axis
        
        plt.figure(figsize=(20,5))
        sns.histplot(
            data=df_bank[df_bank["Attrition_Flag"] == True], 
            hue="Card_Category", x=col, kde=True, bins="auto", multiple="dodge"
        )
        plt.title("Only Attrition")
        plt.show()
        
        plt.title("Only Attrition")
        sns.barplot(
            data=df_bank[df_bank["Attrition_Flag"] == True],
            x="Card_Category",
            y=col,
        )
        plt.ylabel("Box plot (No Attrition) : \n" + col)
        plt.show()
        
        plt.title("Only Attrition")
        sns.pointplot(data=df_bank[df_bank["Attrition_Flag"] == True],
              x="Income_Category", y=col, hue="Card_Category",
              size=7, dodge=True)
        
        plt.show()

"""## <a id="link2-2"> B.2 Multivariate Analysis </a>

*Go to <a href=#link0>Index</a>*

To observe linear corelations between variables.
"""

# Bivariate plot of categorical variables

for col in df_bank.select_dtypes(include="category"):
    print("#" * 80)
    print(df_bank[col].value_counts())
    print("-" * 40)
    print(
        pd.pivot_table(
            df_bank,
            values="Customer_Age",
            index=col,
            columns="Attrition_Flag",
            aggfunc=np.count_nonzero,
        )
    )

    sns.countplot(data=df_bank, x=col, hue="Attrition_Flag")
    plt.show()

for col in df_bank.select_dtypes(include="category"):
    plt.figure(figsize=(20, 5))
    sns.countplot(
        data=df_bank.query("Attrition_Flag == True"),
        hue="Card_Category",
        x=col,
        #                  multiple="dodge"
    )
    plt.ylabel("Count of \n" + col)
    plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# df_bank.profile_report(title="Pandas Profiling of Bank Data")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # display correlation heat-map, only on numeric variables
# plt.figure(figsize=(16, 12))
# 
# sns.pairplot(
#     df_bank.select_dtypes(exclude="category"),
#     hue="Attrition_Flag",
#     diag_kind="auto",
#     markers=["o", "s"],
#     corner=True,
#     height=3,
# )

"""### Observation by Sanjib on Pair-plot and Heatmap correlations:

1. The attrition_flag values are not clearly segregated by any two pair of variables. Hence, no clear liner relationship exists between the target variable and the independent variables. The logistics regression algorithm is not expected to perform well.

2. Some of the independent variable pairs have roughly linear relationship. 
 Example: Customer_Age & Months_on_book, Credit_Limit & Avg_Open_To_Buy, Total_Revolving_Bal & Avg_Utilization_Ratio, Total_Trans_Amt & Total_Trans_Ct
 
3. Credit_Limit & Avg_Open_To_Buy are highly corelated. One of them needs to be dropped for logistics regression, but higher algorithms based on Decision Trees are not impacted by correlation among vaiables.  
"""

# display annoted heat-map, only on numeric variables
plt.figure(figsize=(14, 10))

sns.heatmap(
    df_bank.select_dtypes(exclude="category").corr(),
    annot=True,
    cmap="RdBu",
    vmin=-1,
    vmax=1,
    fmt=".2f",
    linewidths=5,
)

"""## <a id ="link3"> C. Data Engineering </a>

*Go To <a href=#link0>Index</a>*

### <a id ="link3-1">  C.1 KNN Data Imputer Preparation</a>

KNN Imputer is prepared only along with dataset; would be applied later at appropriate step.
"""

df_bank.isnull().sum()  # check columns with np.nan

imputer = KNNImputer(n_neighbors=5)

reqd_col_for_impute = ["Education_Level", "Marital_Status", "Income_Category"]

df_bank[reqd_col_for_impute].sample(10)

df_bank_impute = df_bank.copy(deep=True)
df_bank_impute.info()

# Note: The following steps only prepares the dataset independently, not applying KNN imputer,
# which should be applied after split of dataset into Train and Test.

# we need to pass numerical values for each categorical column for KNN imputation so we will label encode them

Attrition_Flag_dict = {True: 1, False: 0}
df_bank_impute["Attrition_Flag"] = (
    df_bank_impute["Attrition_Flag"].map(Attrition_Flag_dict).astype("uint8")
)

gender_dict = {"F": 0, "M": 1}
df_bank_impute["Gender"] = df_bank_impute["Gender"].map(gender_dict)


Marital_Status_dict = {"Single": 0, "Married": 1, "Divorced": 2}
df_bank_impute["Marital_Status"] = df_bank_impute["Marital_Status"].map(
    Marital_Status_dict
)


Card_Category_dict = {"Blue": 0, "Silver": 1, "Gold": 2, "Platinum": 3}
df_bank_impute["Card_Category"] = df_bank_impute["Card_Category"].map(
    Card_Category_dict
)

df_bank_impute.sample(10)

"""## <a id="link3-2"> C.2 Data Split </a>

*Go To <a href=#link0>Index</a>*

Spit the dataset into Train, Validation and Test.
"""

y = df_bank_impute["Attrition_Flag"]
X = df_bank_impute.drop(labels=["Attrition_Flag"], axis=1)
X.sample(5)

# Splitting data into training and test set:
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=10, stratify=y
)
print(X_train.shape, X_test.shape)

"""### Observation by Sanjib:

1. During model tunnning in subsequent steps, we would observe the performace on Test dataset, 
and may want to go back on hyper paramater tunning. Example, we may run Randomized Search CV on a model, and observe overfitting with Test dataset. We may want to adjust the set of hyper parameters by going back to the Randomized Search CV, or choose hyper parameters differntly for next Grid Search CV.

2. The above phonenomenon would lead to data leakage by model.

3. Thus we want to futher split the dataset into validation and test datasets, where vadiation dataset would be used during hyper parameter tuning of various models; and test dataset would be saved as unseen dataset. The test dataset would be used towards end, after model selection, to validate the model performance in truely unseen dataset.
"""

# Further split Test dataset into Validation and Test
X_val, X_test, y_val, y_test = train_test_split(
    X_test, y_test, test_size=0.5, random_state=10, stratify=y_test
)

print(X_val.shape, X_test.shape)

# 1. Fit and transform the train data
X_train[reqd_col_for_impute] = imputer.fit_transform(
    X_train[reqd_col_for_impute]
).astype("uint8")
# X_train[reqd_col_for_impute] = X_train[reqd_col_for_impute].astype("uint8")

# 2. Transform the validation data
X_val[reqd_col_for_impute] = imputer.transform(X_val[reqd_col_for_impute]).astype(
    "uint8"
)

# 3. Transform the test data
X_test[reqd_col_for_impute] = imputer.transform(X_test[reqd_col_for_impute]).astype(
    "uint8"
)

# Checking that no column has missing values in train or test sets
print("-" * 30, "Train dataset")
print(X_train.isna().sum())
print("-" * 30, "Validation dataset")
print(X_val.isna().sum())
print("-" * 30, "Test dataset")
print(X_test.isna().sum())

## Function to inverse the encoding
def inverse_mapping(dict_param, column, X_param):
    """
        Description: 
            Inverses the numeric values to categorical values after KNN Impute.
        Input:
            dict_param = dictionary of categorical values mapping used for KNN Impute
            column = The variable (or column) name needs to be reversed
            X = The dataset, such as training, validation, or test  
        Return:
            X_param = the inversed dataset
    """
    inv_dict = {v: k for k, v in dict_param.items()}
    #     print(inv_dict)
    #     X_train[y] = np.round(X_train[y]).map(inv_dict).astype('category')
    X_param[column] = X_param[column].map(inv_dict).astype("category")
    return X_param


reqd_col_for_impute

# x_p = inverse_mapping(gender_dict, "Gender", X_train)

data_sets = [X_train, X_val, X_test]

for x_p in data_sets:
    x_p = inverse_mapping(gender_dict, "Gender", x_p)
    x_p = inverse_mapping(Marital_Status_dict, "Marital_Status", x_p)
    x_p = inverse_mapping(Card_Category_dict, "Card_Category", x_p)

X_train.sample(10)

X_val.sample(10)

X_test.sample(10)

# check the categorical variable count after transformation
cols = X_train.select_dtypes(include=["object", "category"])
for i in cols.columns:
    print(X_train[i].value_counts())
    print("-" * 50)

X_train.info()

# reduce memory of dataframe
X_train["Education_Level"] = X_train["Education_Level"].astype("uint8")
X_val["Education_Level"] = X_val["Education_Level"].astype("uint8")
X_test["Education_Level"] = X_test["Education_Level"].astype("uint8")

X_train["Income_Category"] = X_train["Income_Category"].astype("uint8")
X_val["Income_Category"] = X_val["Income_Category"].astype("uint8")
X_test["Income_Category"] = X_test["Income_Category"].astype("uint8")

X_val.info()

y_val.dtype  # check the size of data type

"""### <a id ="link3-3"> C.3 Categorical Variable Encoding </a>

*Go To <a href=#link0>Index</a>*
    
Apply encoding to Train, Validation, and Test dataset.
"""

"""
    After EDA, we can conclude that Card_Category are in a order from basic mass product to niche limited product.
    Let us apply ordinal labeling to encoding.
"""


for x_p in data_sets:
    x_p["Card_Category"] = x_p["Card_Category"].map(
        {"Blue": 1, "Silver" : 2, "Gold" : 3, "Platinum" : 4 }).astype("uint8")
    
X_train["Card_Category"].value_counts()

X_train.select_dtypes(include="category").columns  # remaining categorical columns

# dummy one-hot encoding
X_train = pd.get_dummies(X_train, drop_first=True)
X_val = pd.get_dummies(X_val, drop_first=True)
X_test = pd.get_dummies(X_test, drop_first=True)

print(X_train.shape, X_val.shape, X_test.shape)

X_train.sample(5)

"""### <a id ="link3-4"> C.4 Scaling of Data </a>

*Go To <a href=#link0>Index</a>*

1. Scaling of Data is required for Logistics Regression, not Decision Tree based algorithms. 
2. Some of the independent variables have outliers, which impacts Logistics Regression, not Decision Tree based algorithms. As observed earlier, linear relationship with target variable was not prominent; therefore, logistics regression may not produce great model performances.  
3. Apply scaling to Train, Validation, and Test dataset.
"""

X_train_scaled = pd.DataFrame(
    data=MinMaxScaler().fit_transform(X_train), columns=X_train.columns,
)
X_train_scaled.sample(5)

X_val_scaled = pd.DataFrame(
    data=MinMaxScaler().fit_transform(X_val), columns=X_val.columns,
)
X_val_scaled.sample(5)

X_test_scaled = pd.DataFrame(
    data=MinMaxScaler().fit_transform(X_val), columns=X_test.columns,
)
X_test_scaled.sample(5)

# Scaled data would only be used in logistics regression, which is sensitive to highly correlated variables.
# Let us drop one highly corelated variable.

X_train_scaled.drop(labels=["Avg_Open_To_Buy"],inplace=True,axis=1)
X_val_scaled.drop(labels=["Avg_Open_To_Buy"],inplace=True,axis=1)
X_test_scaled.drop(labels=["Avg_Open_To_Buy"],inplace=True,axis=1)

# save the class weights as variable, which would be used as hyper-parameters in several models without
# over sampling or under sampling

CLASS_WEIGHT_DICT = {
    1: df_bank_impute["Attrition_Flag"].value_counts(normalize=True)[0],
    0: df_bank_impute["Attrition_Flag"].value_counts(normalize=True)[1],
}
CLASS_WEIGHT_DICT

"""# <a id="link4"> D. Build Models with various algorithms</a>


*Go To <a href=#link0>Index</a>*



## <a id="link4-1"> D.1 Logistics Regresssion Classifiers</a>


create re-usable functions firstly
"""

def get_metrics_score(
    model, train, test, train_y, test_y, threshold=0.5, flag=True, roc=False, desc=""
):
    """
    Description:
        Function to calculate different metric scores of the model - Accuracy, Recall, Precision, and F1 score
    Parameters:
        desc: a description that would be added to the output result
        model: classifier to predict values of X
        train, test: Independent features
        train_y,test_y: Dependent variable
        threshold: thresold for classifiying the observation as 1
        flag: If the flag is set to True then only the print statements showing different will be displayed. The default value is set to True.
        roc: If the roc is set to True then only roc score will be displayed. The default value is set to False.
    Returns:
        list with train and test scores
    """

    # defining an empty list to store train and test results

    score_list = []

    pred_train = model.predict_proba(train)[:, 1] > threshold
    pred_test = model.predict_proba(test)[:, 1] > threshold

    pred_train = np.round(pred_train)
    pred_test = np.round(pred_test)

    train_acc = accuracy_score(pred_train, train_y)
    test_acc = accuracy_score(pred_test, test_y)

    train_recall = recall_score(train_y, pred_train)
    test_recall = recall_score(test_y, pred_test)

    train_precision = precision_score(train_y, pred_train)
    test_precision = precision_score(test_y, pred_test)

    train_f1 = f1_score(train_y, pred_train)
    test_f1 = f1_score(test_y, pred_test)

    df_score = pd.DataFrame(dtype="float16")

    df_score["Description"] = (desc,)
    df_score["Threshold"] = (threshold,)
    df_score["Accuracy-Train"] = (train_acc,)
    df_score["Accuracy-Test"] = (test_acc,)
    df_score["Precision-Train"] = (train_precision,)
    df_score["Precision-Test"] = (test_precision,)
    df_score["Recall-Train"] = (train_recall,)
    df_score["Recall-Test"] = (test_recall,)
    df_score["F1-Train"] = (train_f1,)
    df_score["F1-Test"] = (test_f1,)
    df_score["ROC-AUC-Train"] = (np.nan,)
    df_score["ROC-AUC-Test"] = np.nan

    if roc == True:  # calculate ROC-AUC if flag is true
        pred_train_prob = model.predict_proba(train)[:, 1]
        pred_test_prob = model.predict_proba(test)[:, 1]
        df_score.at[0, "ROC-AUC-Train"] = roc_auc_score(train_y, pred_train)
        df_score.at[0, "ROC-AUC-Test"] = roc_auc_score(test_y, pred_test)

    if flag == True:
        print(df_score)

    return df_score


def make_confusion_matrix(model, test_X, y_actual, labels=[1, 0]):
    """
    Description:
        Prints the confusion matrix formatted.
    Parameters:
        model : classifier to predict values of X
        test_X: test set
        y_actual : ground truth  
    Returns:
        Dataframe containing confusion matrix
    """

    y_predict = model.predict(test_X)
    cm = metrics.confusion_matrix(y_actual, y_predict, labels=[1, 0])
    df_cm = pd.DataFrame(
        cm,
        index=[i for i in ["Actual - True", "Actual - False"]],
        columns=[i for i in ["Predicted - Positive", "Predicted - Negetive"]],
    )
    group_counts = ["{0:0.0f}".format(value) for value in cm.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in cm.flatten() / np.sum(cm)]
    labels = [f"{v1}\n{v2}" for v1, v2 in zip(group_counts, group_percentages)]
    labels = np.asarray(labels).reshape(2, 2)
    plt.figure(figsize=(10, 7))
    sns.set_context("talk")  # talk : bolds the plots
    sns.heatmap(df_cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
    sns.set_context("notebook")  # reset back to normal

    return df_cm


def plot_roc_auc(lg, X, y):
    """
    Description:
        Prints the ROC-AUC.
    Parameters:
        lg : Linear Regression model
        X: X_train or X_test
        y: y_train or y_test
    Returns:
        None (Display in Notebook)
    """
    logit_roc_auc = roc_auc_score(y, lg.predict_proba(X)[:, 1])
    fpr, tpr, thresholds = roc_curve(y, lg.predict_proba(X)[:, 1])
    plt.figure(figsize=(7, 5))
    plt.plot(fpr, tpr, label="Logistic Regression (area = %0.2f)" % logit_roc_auc)
    plt.plot([0, 1], [0, 1], "r--")
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("Receiver operating characteristic")
    plt.legend(loc="lower right")
    plt.show()


def plot_prec_recall_vs_tresh(lg, X_train, y_train):
    """
    Description:
        Prints the Precision-Recall curve.
    Parameters:
        lg : Linear Regression model
        X: X_train or X_test
        y: y_train or y_test
    Returns:
        None (Display in Notebook)
    """
    y_scores = lg.predict_proba(X_train)[:, 1]
    precisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)
    plt.plot(thresholds, precisions[:-1], "b--", label="precision")
    plt.plot(thresholds, recalls[:-1], "g--", label="recall")
    plt.xlabel("Threshold")
    plt.legend(loc="upper left")
    plt.ylim([0, 1])

lr = LogisticRegression(random_state=1, class_weight=CLASS_WEIGHT_DICT)
lr.fit(X_train_scaled, y_train)

# K fold cross validator

scoring='recall'
kfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=1)     #Setting number of splits equal to 5
cv_result_bfr=cross_val_score(estimator=lr, X=X_train_scaled, y=y_train, scoring=scoring, cv=kfold)

#Plotting boxplots for CV scores of model defined above
plt.boxplot(cv_result_bfr)
plt.show()

cv_result_bfr

# Calculating different metrics
scores_LR = get_metrics_score(
    lr,
    X_train_scaled,
    X_val_scaled,
    y_train,
    y_val,
    threshold=0.5,
    flag=True,
    roc=True,
    desc="Logistics Regression Basic",
)

df_scores = scores_LR
df_scores

# defining dictionary of model
models = {"Logistics Regression Basic": lr}

# creating confusion matrix
make_confusion_matrix(lr, X_val_scaled, y_val)

"""### <a id ="link4-1-1"> D.1.1  Oversample Logistics Regresssion </a>
    

*Go To <a href=#link0>Index</a>*
    
"""

print("Before Over Sampling, counts of label 'Yes': {}".format(sum(y_train==1)))
print("Before Over Sampling, counts of label 'No': {} \n".format(sum(y_train==0)))

sm = SMOTE(sampling_strategy = 1 ,k_neighbors = 5, random_state=1)   #Synthetic Minority Over Sampling Technique
X_train_over, y_train_over = sm.fit_resample(X_train_scaled, y_train)


print("After Over Sampling, counts of label 'Yes': {}".format(sum(y_train_over==1)))
print("After Over Sampling, counts of label 'No': {} \n".format(sum(y_train_over==0)))


print('After Over Sampling, the shape of train_X: {}'.format(X_train_over.shape))
print('After Over Sampling, the shape of train_y: {} \n'.format(y_train_over.shape))

log_reg_over = LogisticRegression(random_state=1)

# Training the basic logistic regression model with training set
log_reg_over.fit(X_train_over, y_train_over)

# K fold cross validator

scoring = "recall"
kfold = StratifiedKFold(
    n_splits=5, shuffle=True, random_state=1
)  # Setting number of splits equal to 5
cv_result_bfr = cross_val_score(
    estimator=log_reg_over, X=X_train_over, y=y_train_over, scoring=scoring, cv=kfold
)

# Plotting boxplots for CV scores of model defined above
plt.boxplot(cv_result_bfr)
plt.show()

# Calculating different metrics
scores_LR_over = get_metrics_score(
    log_reg_over,
    X_train_over,
    X_val_scaled,
    y_train_over,
    y_val,
    threshold=0.5,
    flag=True,
    roc=True,
    desc="Logistics Regression over sampling",
)

df_scores = pd.concat([df_scores, scores_LR_over], axis=0, ignore_index=True)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

# adding to list of model
models["Logistics Regression over sampling"] = log_reg_over

# creating confusion matrix
make_confusion_matrix(log_reg_over, X_val_scaled, y_val)

"""The Recall Scores in train and test (validation) dataset shows sign of overfitting. We need to regularize to reduce overfitting.

### <a id ="link4-1-2"> D.1.2. Regularize Logistics Regression </a>

*Go To <a href=#link0>Index</a>*


1. Ridge regularization
2. Lasso regularization
3. GridSearchCV would be tried later after watching under sampling.

"""

lr_ridge = Ridge(alpha=0.3)

lr_ridge.fit(X_train_over, y_train_over)

# defining list of model
print("Ridge model coefficients :", (lr_ridge.coef_))
print(lr_ridge.score(X_train_over, y_train_over))
print(lr_ridge.score(X_val_scaled, y_val))

def get_metrics_score_ridge_lasso(
    model, train, test, train_y, test_y, threshold=0.5, flag=True, roc=False, desc=""
):
    """
    Description:
        Function to calculate different metric scores of the model - Accuracy, Recall, Precision, and F1 score
    Parameters:
        desc: a description that would be added to the output result
        model: classifier to predict values of X
        train, test: Independent features
        train_y,test_y: Dependent variable
        threshold: thresold for classifiying the observation as 1
        flag: If the flag is set to True then only the print statements showing different will be displayed. The default value is set to True.
        roc: If the roc is set to True then only roc score will be displayed. The default value is set to False.
    Returns:
        list with train and test scores
    """

    # defining an empty list to store train and test results

    score_list = []

    pred_train = model.predict(train)[:] > threshold
    pred_test = model.predict(test)[:] > threshold

    pred_train = np.round(pred_train)
    pred_test = np.round(pred_test)

    train_acc = accuracy_score(pred_train, train_y)
    test_acc = accuracy_score(pred_test, test_y)

    train_recall = recall_score(train_y, pred_train)
    test_recall = recall_score(test_y, pred_test)

    train_precision = precision_score(train_y, pred_train)
    test_precision = precision_score(test_y, pred_test)

    train_f1 = f1_score(train_y, pred_train)
    test_f1 = f1_score(test_y, pred_test)

    df_score = pd.DataFrame(dtype="float16")

    df_score["Description"] = (desc,)
    df_score["Threshold"] = (threshold,)
    df_score["Accuracy-Train"] = (train_acc,)
    df_score["Accuracy-Test"] = (test_acc,)
    df_score["Precision-Train"] = (train_precision,)
    df_score["Precision-Test"] = (test_precision,)
    df_score["Recall-Train"] = (train_recall,)
    df_score["Recall-Test"] = (test_recall,)
    df_score["F1-Train"] = (train_f1,)
    df_score["F1-Test"] = (test_f1,)
    df_score["ROC-AUC-Train"] = (np.nan,)
    df_score["ROC-AUC-Test"] = np.nan

    if roc == True:  # calculate ROC-AUC if flag is true
        pred_train_prob = model.predict(train)[:]
        pred_test_prob = model.predict(test)[:]
        df_score.at[0, "ROC-AUC-Train"] = roc_auc_score(train_y, pred_train)
        df_score.at[0, "ROC-AUC-Test"] = roc_auc_score(test_y, pred_test)

    if flag == True:
        print(df_score)

    return df_score

# Calculating different metrics
scores_LR_ridge = get_metrics_score_ridge_lasso(
    lr_ridge,
    X_train_over,
    X_val_scaled,
    y_train_over,
    y_val,
    threshold=0.5,
    flag=True,
    roc=True,
    desc="Logistics Regression Ridge",
)

df_scores = pd.concat([df_scores, scores_LR_ridge], axis=0, ignore_index=True)
df_scores

def make_confusion_matrix_ridge_lasso(model, test_X, y_actual, labels=[1, 0]):
    """
    Description:
        Prints the confusion matrix formatted.
    Parameters:
        model : classifier to predict values of X
        test_X: test set
        y_actual : ground truth  
    Returns:
        Dataframe containing confusion matrix
    """

    y_predict = model.predict(test_X)
    #     print(y_predict)
    y_predict = [np.round(i) for i in y_predict]  # converting to (0, 1)
    #     print(y_predict)
    cm = metrics.confusion_matrix(y_actual, y_predict, labels=[1, 0])
    df_cm = pd.DataFrame(
        cm,
        index=[i for i in ["Actual - True", "Actual - False"]],
        columns=[i for i in ["Predicted - Positive", "Predicted - Negetive"]],
    )
    group_counts = ["{0:0.0f}".format(value) for value in cm.flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in cm.flatten() / np.sum(cm)]
    labels = [f"{v1}\n{v2}" for v1, v2 in zip(group_counts, group_percentages)]
    labels = np.asarray(labels).reshape(2, 2)
    plt.figure(figsize=(10, 7))
    sns.set_context("talk")  # talk : bolds the plots
    sns.heatmap(df_cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
    sns.set_context("notebook")  # reset back to normal

    return df_cm

# adding to list of model
models["Logistics Regression Ridge"] = lr_ridge

# creating confusion matrix
make_confusion_matrix_ridge_lasso(lr_ridge, X_val_scaled, y_val)

lr_lasso = Lasso(alpha=0.1)

lr_lasso.fit(X_train_over, y_train_over)

# defining list of model
print("Lasso model coefficients :", (lr_lasso.coef_))
print(lr_lasso.score(X_train_over, y_train_over))
print(lr_lasso.score(X_val_scaled, y_val))

"""### Observation by Sanjib:

1. After applying the LASSO transformation, all the coefficients of Logistics Regression turned into 0.
2. This indicates no contrubution of independent variables to the target variable.
3. Thus, the logistics regresstion model's independent variables have no linear relationship with the target variable, and wont perform well. This observation is consistent with the multivariate analysis during EDA.
5. Let us not save the LASSO model, discarding it.

### <a id ="link4-1-3"> D.1.3 Undersample Logistics Regresssion </a>


*Go To <a href=#link0>Index</a>*
"""

rus = RandomUnderSampler(random_state=1)
X_train_un, y_train_un = rus.fit_resample(X_train_scaled, y_train)

print("Before Under Sampling, counts of label 'Yes': {}".format(sum(y_train == 1)))
print("Before Under Sampling, counts of label 'No': {} \n".format(sum(y_train == 0)))

print("After Under Sampling, counts of label 'Yes': {}".format(sum(y_train_un == 1)))
print("After Under Sampling, counts of label 'No': {} \n".format(sum(y_train_un == 0)))

print("After Under Sampling, the shape of train_X: {}".format(X_train_un.shape))
print("After Under Sampling, the shape of train_y: {} \n".format(y_train_un.shape))

log_reg_un = LogisticRegression(random_state=1)

# Training the basic logistic regression model with training set
log_reg_un.fit(X_train_un, y_train_un)

# K fold cross validator

# scoring = "recall"
# kfold = StratifiedKFold(
#     n_splits=5, shuffle=True, random_state=1
# )  # Setting number of splits equal to 5

cv_result_bfr = cross_val_score(
    estimator=lr, X=X_train_un, y=y_train_un, scoring=scoring, cv=kfold
)

# Plotting boxplots for CV scores of model defined above
plt.boxplot(cv_result_bfr)
plt.show()

# Calculating different metrics
scores_LR_un = get_metrics_score(
    log_reg_un,
    X_train_un,
    X_val_scaled,
    y_train_un,
    y_val,
    threshold=0.5,
    flag=True,
    roc=True,
    desc="Logistics Regression under sampling",
)

# adding to list of model
models["Logistics Regression under sampling"] = log_reg_un

# creating confusion matrix
make_confusion_matrix(log_reg_un, X_val_scaled, y_val)

df_scores = pd.concat([df_scores, scores_LR_un], axis=0, ignore_index=True)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""### Observation by Sanjib:

1. The Recall Test is high in undersampling. 
2. However, undersampling has only handful records, have risk of missing information.
3. Let us try to regularize oversampling.

### <a id="link4-1-4"> D.1.4  Regularization Logistics Regresssion - RandomizedSearchCV on over sampling dataset </a> 

*Go To <a href=#link0>Index</a>*
"""

# Choose the classifier.

print(
    "Randomized Search CV would be performed on the model",
    "Logistics Regression over sampling :",
    models["Logistics Regression over sampling"],
)

lr_over_randomizedSearchCV = models["Logistics Regression over sampling"]

# Commented out IPython magic to ensure Python compatibility.
# %%time 
# 
# # Grid of parameters to choose from
# parameters = {
#     "C": np.arange(0.5, 2.5, 0.1),
#     "solver": ["newton-cg", "lbfgs", "liblinear", "sag", "saga"],
# }
# 
# # Initialize RandomizedSearchCV
# randomCV = RandomizedSearchCV(
#     estimator=lr_over_randomizedSearchCV,
#     param_distributions=parameters,
#     n_iter=40,
#     cv=5,
#     scoring = {"Recall": "recall", "AUC": "roc_auc" },
#     refit="AUC",
# )  # default cv = 3
# 
# # Run the Randomized Search
# randomCV.fit(X_train_over, y_train_over)
# 
# # Set the clf to the best combination of parameters
# lr_over_randomizedSearchCV = randomCV.best_estimator_
# 
# print("Best model parameters =", randomCV.best_params_)
# 
# # Fit the best algorithm to the data.
# lr_over_randomizedSearchCV.fit(X_train_over, y_train_over)

# Calculating different metrics
scores_LR_randomCV = get_metrics_score(
    lr_over_randomizedSearchCV,
    X_train_over,
    X_val_scaled,
    y_train_over,
    y_val,
    threshold=0.5,
    flag=True,
    roc=True,
    desc="Logistics Regression over RandomizedSearchCV",
)

# adding to list of model
models["Logistics Regression over RandomizedSearchCV"] = lr_over_randomizedSearchCV

# creating confusion matrix
make_confusion_matrix(lr_over_randomizedSearchCV, X_val_scaled, y_val)

df_scores = pd.concat([df_scores, scores_LR_randomCV], axis=0, ignore_index=True)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""### <a id ="link4-1-5"> D.1.5  Regularization Logistics Regresssion - GridSearchCV on over sampling dataset</a>


*Go To <a href=#link0>Index</a>*


"""

# Commented out IPython magic to ensure Python compatibility.
# %%time 
# 
# # Choose the type of classifier.
# lr_over_grid = LogisticRegression(random_state=1)
# 
# # Grid of parameters to choose from
# parameters = {
#     "C": np.arange(0.5, 2.5, 0.1),
#     "solver": ["newton-cg", "lbfgs", "liblinear", "sag", "saga"],
# }
# 
# # Run the grid search
# grid_obj = GridSearchCV(
#             estimator=lr_over_grid, 
#             param_grid=parameters, 
#             scoring = {"Recall": "recall", "AUC": "roc_auc" },
#             refit="AUC",
#             cv=5)
# 
# grid_obj = grid_obj.fit(X_train_over, y_train_over)
# 
# # Set the clf to the best combination of parameters
# lr_over_grid = grid_obj.best_estimator_
# 
# # Fit the best algorithm to the data.
# lr_over_grid.fit(X_train_over, y_train_over)

# Calculating different metrics
scores_LR_grid = get_metrics_score(
    lr_over_grid,
    X_train_over,
    X_val_scaled,
    y_train_over,
    y_val,
    threshold=0.5,
    flag=True,
    roc=True,
    desc="Logistics Regression over GridSearchCV",
)

# adding to list of model
models["Logistics Regression over GridSearchCV"] = lr_over_grid

# creating confusion matrix
make_confusion_matrix(lr_over_grid, X_val_scaled, y_val)

df_scores = pd.concat([df_scores, scores_LR_grid], axis=0, ignore_index=True)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""### <a id ="link4-1-6"> D.1.6 Regularization of Logistics Regresssion - RandomizedSearchCV on under sampling dataset </a>


*Go To <a href=#link0>Index</a>*

"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Choose the type of classifier.
# lr_un_randomCV = models["Logistics Regression under sampling"]
# 
# # Grid of parameters to choose from
# parameters = {
#     "C": np.arange(0.5, 3.0, 0.1),
#     "solver": ["newton-cg", "lbfgs", "liblinear", "sag", "saga"],
# }
# 
# 
# # Initialize RandomizedSearchCV
# randomCV = RandomizedSearchCV(
#     estimator=lr_un_randomCV,
#     param_distributions=parameters,
#     n_iter=40,
#     cv=5,
#     scoring = {"Recall": "recall", "AUC": "roc_auc" },
#     refit="AUC"
# )  # default cv = 3
# 
# # Run the Randomized Search
# randomCV.fit(X_train_un, y_train_un)
# 
# # Set the clf to the best combination of parameters
# lr_un_randomCV = randomCV. best_estimator_
# 
# print("Best model parameters =", randomCV.best_params_)
# 
# # Fit the best algorithm to the data.
# lr_un_randomCV.fit(X_train_un, y_train_un)

# Calculating different metrics
scores_LR_un_randomCV = get_metrics_score(
    lr_un_randomCV,
    X_train_un,
    X_val_scaled,
    y_train_un,
    y_val,
    threshold=0.5,
    flag=True,
    roc=True,
    desc="Logistics Regression under RandomizedSearchCV",
)

# adding to list of model
models["Logistics Regression under RandomizedSearchCV"] = lr_un_randomCV

# creating confusion matrix
make_confusion_matrix(lr_un_randomCV, X_val_scaled, y_val)

df_scores = pd.concat([df_scores, scores_LR_un_randomCV], axis=0, ignore_index=True)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""
    We'll skip polynomial feature engineering with Logistics regression, threshold tuning,
    in the interest of time. We'll try more sphisticated algorithms subsequently.
    
"""
# Model coefficients and interpretation

log_odds = models["Logistics Regression under RandomizedSearchCV"].coef_[0]

print("High positive and low negetive values of coefficients are influencers.")
pd.DataFrame(log_odds, X_train_un.columns, columns=["coef"]).sort_values(
    by=["coef"], ascending=False
)  # formatted display

# Odds from coefficients

# converting coefficients to odds
odds = np.exp(models["Logistics Regression under RandomizedSearchCV"].coef_[0])
perc_change_odds = (
    np.exp(models["Logistics Regression under RandomizedSearchCV"].coef_[0]) - 1
) * 100  # finding the percentage change

df_coeff_results = pd.concat(
    [
        pd.DataFrame(odds, X_train_un.columns, columns=["odds"]),
        pd.DataFrame(perc_change_odds, X_train_un.columns, columns=["change_odds_%"]),
    ],
    axis=1,
)
df_coeff_results.sort_values(by=["odds"], ascending=False)

"""## <a id ="link4-2"> D.2 Decision Tree Classifiers </a>


*Go To <a href=#link0>Index</a>*

All the models need to be tried on over-sample, and under-sample dataset to check best performance.

### <a id = #link4-2-1> D.2.1  Basic Decision Tree with class imbalance </a>
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# dTree = DecisionTreeClassifier(
#     criterion="gini", random_state=1, class_weight=CLASS_WEIGHT_DICT
# )  # default values of hyper parameters
# print(dTree.get_params())

# We'll use imbalanced dataset firstly
print(X_train.shape, X_val.shape)

dTree.fit(X_train, y_train)

df_score_dtree = get_metrics_score(
    dTree, X_train, X_val, y_train, y_val, desc="Decision Tree Basic", roc=True,
)

# adding to list of model
models["Decision Tree Basic"] = dTree

make_confusion_matrix(dTree, X_val, y_val)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Visualizing the Decision Tree
# 
# 
# def display_decision_tree(dTree, feature_names):
#     """
#     Description:
#         Plot the decision tree visualization with arrows.
#     Parameters:
#         dTree : the Decision Tree to plot
#         feature_names : list containing names of the features
#     Returns:
#         None
#     """
#     sns.set_context("poster")  # double bolds the plot
#     
#     plt.figure(figsize=(dTree.get_depth()*4, dTree.get_depth()*3))
# 
#     out = tree.plot_tree(
#         dTree,
#         feature_names=feature_names,
#         filled=True,
#         fontsize=9,
#         node_ids=True,
#         class_names=True,
#     )
#     for o in out:
#         arrow = o.arrow_patch
#         if arrow is not None:
#             arrow.set_edgecolor("black")
#             arrow.set_linewidth(1)
# 
# 
#     plt.show()
#     
#     sns.set_context("notebook")  # reset to normal
#     
#     # end of function
#     
# display_decision_tree(dTree, X_train.columns)

# Text report showing the rules of a decision tree -

print(
    tree.export_text(dTree, feature_names=X_train.columns.tolist(), show_weights=True)
)

# re-usable function


def feature_importance_in_tree(dTree, feature_names=[], model_name=""):
    """
    Description:
        Display importance of features in the tree building , by Gini importance
    Parameters:
        dTree : Descision Tree Model
        feature_names : list of independent variable names
    Returns:
        dataframe containing the feature importance, sorted decending
    """

    df_imp = pd.DataFrame(
        dTree.feature_importances_,
        columns=["Importance_" + model_name],
        index=feature_names,
    ).sort_values(by="Importance_" + model_name, ascending=False)
    print(df_imp)

    sns.set_context("talk")  # talk : bolds the plots

    fig, axs = plt.subplots(figsize=(20, 5))
    df_imp.plot(kind="bar", ax=axs, color="Green")
    plt.title("Feature Importances")
    plt.xlabel("Relative Importance")
    fig.show()

    sns.set_context("notebook")  # reset to normal

    return df_imp


df_imp = feature_importance_in_tree(
    dTree=dTree, feature_names=X_train.columns, model_name="Decision_Tree_Basic"
)

df_scores = pd.concat([df_scores, df_score_dtree], axis=0, ignore_index=True)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""### <a id="link4-2-2"> D.2.2  Decision Tree with over sampling </a>

*Go To <a href=#link0> Index </a>* 

"""

dTree = DecisionTreeClassifier(
    criterion="gini", random_state=1,  # No CLASS_WEIGHT_DICT
)  # default values of hyper parameters
print(dTree.get_params())

# We'll use over sampled dataset.
# The previous one was on scaled dataset for logistics regression.
# The decision tree does not require scaling of data.

print(
    "X_train :",
    X_train.shape,
    ". y_train :",
    y_train.shape,
    ". X_val :",
    X_val.shape,
    ". y_val :",
    y_val.shape,
)

print("Before Over Sampling, counts of label 'Yes': {}".format(sum(y_train == 1)))
print("Before Over Sampling, counts of label 'No': {} \n".format(sum(y_train == 0)))

# sm = SMOTE(sampling_strategy = 1 ,k_neighbors = 5, random_state=1)   #Synthetic Minority Over Sampling Technique
X_train_over, y_train_over = sm.fit_resample(X_train, y_train)


print("After Over Sampling, counts of label 'Yes': {}".format(sum(y_train_over == 1)))
print("After Over Sampling, counts of label 'No': {} \n".format(sum(y_train_over == 0)))


print("After Over Sampling, the shape of train_X: {}".format(X_train_over.shape))
print("After Over Sampling, the shape of train_y: {} \n".format(y_train_over.shape))

dTree_over = DecisionTreeClassifier(
    criterion="gini", random_state=1
)  # default values of hyper parameters

print(dTree_over.get_params())

dTree_over.fit(X_train_over, y_train_over)

df_score_dtree_over = get_metrics_score(
    dTree_over,
    X_train_over,
    X_val,
    y_train_over,
    y_val,
    desc="Decision Tree over sample",
    roc=True,
)

# adding to list of model
models["Decision Tree over sample"] = dTree_over

make_confusion_matrix(dTree_over, X_val, y_val)

df_scores = pd.concat([df_scores, df_score_dtree_over], axis=0, ignore_index=True)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""### <a id ="link4-2-3"> D.2.3  Decision Tree with under sampling </a>


*Go To <a href=#link0> Index </a>*


"""

# We'll use under sampled dataset.
# The previous one was on scaled dataset for logistics regression.
# The decision tree does not require scaling of data.

print(
    "X_train :",
    X_train.shape,
    ". y_train :",
    y_train.shape,
    ". X_val :",
    X_val.shape,
    ". y_val :",
    y_val.shape,
)

# rus = RandomUnderSampler(random_state=1)
X_train_un, y_train_un = rus.fit_resample(X_train, y_train)

print("Before Under Sampling, counts of label 'Yes': {}".format(sum(y_train == 1)))
print("Before Under Sampling, counts of label 'No': {} \n".format(sum(y_train == 0)))

print("After Under Sampling, counts of label 'Yes': {}".format(sum(y_train_un == 1)))
print("After Under Sampling, counts of label 'No': {} \n".format(sum(y_train_un == 0)))

print("After Under Sampling, the shape of train_X: {}".format(X_train_un.shape))
print("After Under Sampling, the shape of train_y: {} \n".format(y_train_un.shape))

dTree_un = DecisionTreeClassifier(
    criterion="gini", random_state=1
)  # default values of hyper parameters

print(dTree_un.get_params())

dTree_un.fit(X_train_un, y_train_un)

df_score_dtree_un = get_metrics_score(
    dTree_un,
    X_train_un,
    X_val,
    y_train_un,
    y_val,
    desc="Decision Tree under sample",
    roc=True,
)

print("Checking the models built so far:", models)

# adding to list of model
models["Decision Tree under sample"] = dTree_un

make_confusion_matrix(dTree_un, X_val, y_val)

df_scores = pd.concat([df_scores, df_score_dtree_un], axis=0, ignore_index=True)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""### <a id ="link4-2-4"> D.2.4  Decision Tree Randomized Search CV on under sampled dataset </a>


*Go To <a href=#link0> Index </a>*

Till now, under sampled data with Decision Tree looked most promising performance, but a little overfit. 

We'll try to regularize this model with Randomized Search CV firstly. The cross validation technique would also reduce risk of prediction in unseen (test) dataset.
"""

dTree_un.get_params()

dTree_un.get_depth()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Choose the type of classifier.
# dTree_un_randomSearchCV = DecisionTreeClassifier(
#     random_state=1  
# )
# 
# # Grid of parameters to choose from
# ## add from article
# parameters = {
#     "max_depth": np.arange(3, 12),
#     "min_samples_leaf": [1, 2, 5, 7, 10, 12, 15],
#     "max_leaf_nodes": [ 10, 50, 70, 100, 130],
#     "min_impurity_decrease": [0.0001, 0.001, 0.01, 0.1],
#     "criterion" : ["gini","entropy"]
# }
# 
# # Type of scoring used to compare parameter combinations
# # rec_scorer = metrics.make_scorer(metrics.recall_score)
# 
# # Run the grid search
# randomCV_obj = RandomizedSearchCV(
#     estimator=dTree_un_randomSearchCV, 
#     param_distributions=parameters, 
#     n_iter = 40,
#     scoring = {"Recall": "recall", "AUC": "roc_auc" },
#     refit="AUC",
#     cv=5)
# 
# 
# randomCV_obj = randomCV_obj.fit(X_train_un, y_train_un)
# 
# # Set the classifier to the best combination of parameters
# dTree_un_randomSearchCV = randomCV_obj.best_estimator_
# 
# # Fit the best algorithm to the data.
# dTree_un_randomSearchCV.fit(X_train_un, y_train_un)

df_score_dTree_un_randomCV = get_metrics_score(
    desc="DTree undersampled RandomizedSearchCV",
    model=dTree_un_randomSearchCV,
    train=X_train_un,
    test=X_val,
    train_y=y_train_un,
    test_y=y_val,
    roc=True,
)

# adding to list of model
models["DTree undersampled RandomizedSearchCV"] = dTree_un_randomSearchCV

make_confusion_matrix(dTree_un_randomSearchCV, X_val, y_val)

df_scores = pd.concat(
    [df_scores, df_score_dTree_un_randomCV], axis=0, ignore_index=True
)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""### <a id ="link4-2-5"> D.2.5 Decision Tree Grid Search CV on under sampled dataset </a>


*Go To <a href=#link0> Index </a>*

We'll try to regularize this model with Grid Search CV. The cross validation technique would also reduce risk of prediction in unseen (test) dataset.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Choose the type of classifier.
# dTree_un_GridSearchCV = DecisionTreeClassifier(
#     random_state=1,  criterion="gini",
# )
# 
# # Grid of parameters to choose from
# ## add from article
# parameters = {
#     "max_depth": np.arange(3, 10),
#     "min_samples_leaf": [ 5, 7, 10, 12, 15],
#     "max_leaf_nodes": [ 50, 70, 80, 100],
#     "min_impurity_decrease": [0.0001, 0.001, 0.01],
#     "criterion" : ["gini","entropy"]
# }
# 
# # Type of scoring used to compare parameter combinations
# # rec_scorer = metrics.make_scorer(metrics.recall_score)
# 
# # Run the grid search
# grid_obj = GridSearchCV(
#             estimator=dTree_un_GridSearchCV, 
#             param_grid=parameters, 
#             scoring = {"Recall": "recall", "AUC": "roc_auc" },
#             refit="AUC", 
#             cv=5)
# grid_obj = grid_obj.fit(X_train_un, y_train_un)
# 
# # Set the classifier to the best combination of parameters
# dTree_un_GridSearchCV = grid_obj.best_estimator_
# 
# # Fit the best algorithm to the data.
# dTree_un_GridSearchCV.fit(X_train_un, y_train_un)

df_score_dTree_un_GridSearchCV = get_metrics_score(
    desc="DTree undersampled GridSearchCV",
    model=dTree_un_GridSearchCV,
    train=X_train_un,
    test=X_val,  # unseen dataset
    train_y=y_train_un,
    test_y=y_val,  # unseen dataset
    roc=True,
)

# adding to list of model
models["DTree undersampled GridSearchCV"] = dTree_un_GridSearchCV

make_confusion_matrix(dTree_un_GridSearchCV, X_val, y_val)

df_scores = pd.concat(
    [df_scores, df_score_dTree_un_GridSearchCV], axis=0, ignore_index=True
)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""### <a id = #link4-2-6> D.2.6  Decision Tree Randomized Search CV on over sampled dataset </a>


*Go To <a href=#link0> Index </a>*


We'll try to regularize the decision tree model with Randomized Search CV firstly on the over sampling dataset. The cross validation technique would also reduce risk of prediction in unseen (test) dataset.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Choose the type of classifier.
# dTree_over_randomSearchCV = DecisionTreeClassifier(
#     random_state=1  
# )
# 
# # Grid of parameters to choose from
# ## add from article
# parameters = {
#     "max_depth": np.arange(3, 15),
#     "min_samples_leaf": [1, 2, 5, 7, 10, 12, 15],
#     "max_leaf_nodes": [ 10, 50, 70, 100, 130],
#     "min_impurity_decrease": [0.0001, 0.001, 0.01, 0.1],
#     "criterion" : ["gini","entropy"]
# }
# 
# # Type of scoring used to compare parameter combinations
# # rec_scorer = metrics.make_scorer(metrics.recall_score)
# 
# # Run the grid search
# randomCV_obj = RandomizedSearchCV(
#     estimator=dTree_un_randomSearchCV, 
#     param_distributions=parameters, 
#     n_iter = 50,
#     scoring = {"Recall": "recall", "AUC": "roc_auc" },
#     refit="AUC",
#     cv=5)
# 
# 
# randomCV_obj = randomCV_obj.fit(X_train_over, y_train_over)
# 
# # Set the classifier to the best combination of parameters
# dTree_over_randomSearchCV = randomCV_obj.best_estimator_
# 
# # Fit the best algorithm to the data.
# dTree_over_randomSearchCV.fit(X_train_over, y_train_over)

df_score_dTree_over_randomCV = get_metrics_score(
    desc="DTree oversampled RandomizedSearchCV",
    model=dTree_over_randomSearchCV,
    train=X_train_over,
    test=X_val,
    train_y=y_train_over,
    test_y=y_val,
    roc=True,
)

# adding to list of model
models["DTree oversampled RandomizedSearchCV"] = dTree_over_randomSearchCV

make_confusion_matrix(dTree_over_randomSearchCV, X_val, y_val)

df_scores = pd.concat(
    [df_scores, df_score_dTree_over_randomCV], axis=0, ignore_index=True
)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""### <a id ="link4-2-7"> D.2.7  Decision Tree Grid Search CV on over sampled dataset </a>


*Go To <a href=#link0> Index </a>*


We'll try to regularize the over sampled Decision Tree model with Grid Search CV. The cross validation technique would also reduce risk of prediction in unseen (test) dataset.
"""

dTree_over_randomSearchCV.get_params()

dTree_over_randomSearchCV.get_depth()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Choose the type of classifier.
# dTree_over_GridSearchCV = DecisionTreeClassifier(
#     random_state=1
# )
# 
# # Grid of parameters to choose from
# ## add from article
# parameters = {
#     "max_depth": np.arange(7, 15),
#     "min_samples_leaf": [5, 10, 12, 15, 18],
#     "max_leaf_nodes": [ 50, 70, 100, 130, 150],
#     "min_impurity_decrease": [0.0001, 0.001, 0.01],
#     "criterion" : ["entropy"]
# }
# 
# # Type of scoring used to compare parameter combinations
# # rec_scorer = metrics.make_scorer(metrics.recall_score)
# 
# # Run the grid search
# grid_obj = GridSearchCV(
#             estimator=dTree_over_GridSearchCV, 
#             param_grid=parameters, 
#             scoring = {"Recall": "recall", "AUC": "roc_auc" },
#             refit="AUC",
#             cv=5)
# grid_obj = grid_obj.fit(X_train_over, y_train_over)
# 
# # Set the classifier to the best combination of parameters
# dTree_over_GridSearchCV = grid_obj.best_estimator_
# 
# # Fit the best algorithm to the data.
# dTree_over_GridSearchCV.fit(X_train_over, y_train_over)

# dTree_over_GridSearchCV

df_score_dTree_over_GridSearchCV = get_metrics_score(
    desc="DTree oversampled GridSearchCV",
    model=dTree_over_GridSearchCV,
    train=X_train_over,
    test=X_val,
    train_y=y_train_over,
    test_y=y_val,
    roc=True,
)

# adding to list of model
models["DTree oversampled GridSearchCV"] = dTree_over_GridSearchCV

make_confusion_matrix(dTree_over_GridSearchCV, X_val, y_val)

df_scores = pd.concat(
    [df_scores, df_score_dTree_over_GridSearchCV], axis=0, ignore_index=True
)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""
    In the inrterest of time and complexity, we would skip Decision Tree Cost Complexity Grid Search; 
    which I peerformed in last project. I hope that subsequent sophisticated models would produce good
    perfromance; if not, I'll look back to this step.
"""

models  #  list of models tried so far.

"""
## <a id ="link4-3"> D.3 Ensemble method : Bagging Classifiers </a>


*Go To <a href=#link0> Index </a>*

All the models need to be tried on over-sample, and under-sample dataset to check best performance. 
We'll start hypertuning further with alread tuned models; and evaluate the results to determine the next step.

### <a id="link4-3-1"> D.3.1 Bagging Classifier basic </a>
"""

# base_estimator for bagging classifier is a decision tree by default
bagging_dTree = BaggingClassifier(
    random_state=1,
    base_estimator=models["DTree undersampled RandomizedSearchCV"],
    # Use the model which had best Recall score so far
)  # keeping default values of hyper parameters, mostly
bagging_dTree.fit(X_train_un, y_train_un)

# Using above defined function to get accuracy, recall and precision on train and test set
df_score_bag = get_metrics_score(
    bagging_dTree,
    X_train_un,
    X_val,  # unseen dataset
    y_train_un,
    y_val,  # unseen dataset
    desc="Bagging Classifier under initial",
    roc=True,
)

# adding to list of model
models["Bagging Classifier under initial"] = bagging_dTree

make_confusion_matrix(bagging_dTree, X_val, y_val)

# save to the summary of model performances
df_scores = pd.concat([df_scores, df_score_bag], axis=0, ignore_index=True)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""### <a id = #link4-3-2> D.3.2 Bagging Classifier with Randomized Search CV, under sampling </a>


*Go To <a href=#link0> Index </a>*

"""

bagging_dTree.get_params()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# """
#     Bagging classifier on Decision Tree on under sampling dataset, tuned with 
#     Randomized Search CV was best performing model so far. Hence we use that as base estimator.
#     
# """
# bagging_un_randomCV = models["Bagging Classifier under initial"]
# 
# # Grid of parameters to choose from
# ## add from article
# parameters = {
#     "bootstrap": [True],
#     "max_features": [0.7, 0.8, 0.9, 1],
#     "max_samples": [0.6, 0.7, 0.8, 0.9],
#     "n_estimators": [4, 8, 10, 12, 20, 30],
#     "base_estimator__min_impurity_decrease": [0.0001, 0.001, 0.01, 0.1],
#     "base_estimator__max_depth": np.arange(5,18),
#     "base_estimator__min_impurity_decrease": [0.0001, 0.001, 0.01],
#     "base_estimator__criterion": ["entropy","gini"]
# }
# 
# 
# # Run the Randomized search
# randomCV_obj = RandomizedSearchCV(
#             estimator=bagging_un_randomCV, 
#             param_distributions=parameters, 
#             scoring = {"Recall": "recall", "AUC": "roc_auc" },
#             refit="AUC", 
#             cv=5,
#             n_iter=50)
# 
# randomCV_obj = randomCV_obj.fit(X_train, y_train)
# 
# # Set the clf to the best combination of parameters
# bagging_un_randomCV = randomCV_obj.best_estimator_
# 
# # Fit the best algorithm to the data.
# bagging_un_randomCV.fit(X_train_un, y_train_un)

# Using above defined function to get accuracy, recall and precision on train and test set
df_scores_bag_un_randomCV = get_metrics_score(
    bagging_un_randomCV,
    X_train_un,
    X_val,
    y_train_un,
    y_val,
    desc="Bagging under RandomSearchCV",
    roc=True,
)

# adding to list of model
models["Bagging under RandomSearchCV"] = bagging_un_randomCV

make_confusion_matrix(bagging_un_randomCV, X_val, y_val)

# save to the summary of model perfromances
df_scores = pd.concat([df_scores, df_scores_bag_un_randomCV], axis=0, ignore_index=True)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""### <a id ="link5-3"> D.3.3 Bagging Classifier with Grid Search CV, under sampling </a>


*Go To <a href=#link0> Index </a>*

"""

bagging_un_randomCV.get_params()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# """
#     Bagging classifier on Decision Tree on under sampling dataset, tuned with 
#     Randomized Search CV was best performing model so far. Hence we use that as base estimator.
#     
# """
# bagging_un_gridSearchCV = models["Bagging under RandomSearchCV"]
# 
# # Grid of parameters to choose from
# ## add from article
# parameters = {
#     "bootstrap": [True],
#     "max_features": [ 0.7, 0.8, 0.9],
#     "max_samples": [0.7, 0.8, 0.9],
#     "n_estimators": [10, 20, 30],
#     "base_estimator__max_depth": np.arange(6,10),
#     "base_estimator__min_impurity_decrease": [0.0001],
#     "base_estimator__criterion": ["entropy"]
# }
# 
# 
# # Run the Grid search
# gridCV_obj = GridSearchCV(
#             estimator=bagging_un_gridSearchCV, 
#             param_grid=parameters, 
#             scoring=scoring, # as GridSearchCV was taking long time, I changed to simpler scoring
# #             scoring = {"Recall": "recall", "AUC": "roc_auc" },
# #             refit="AUC", 
#             cv=5)
# 
# gridCV_obj = gridCV_obj.fit(X_train_un, y_train_un)
# 
# # Set the clf to the best combination of parameters
# bagging_un_gridSearchCV = gridCV_obj.best_estimator_
# 
# # Fit the best algorithm to the data.
# bagging_un_gridSearchCV.fit(X_train_un, y_train_un)

# Using above defined function to get accuracy, recall and precision on train and test set
df_scores_bag_un_gridCV = get_metrics_score(
    bagging_un_gridSearchCV,
    X_train_un,
    X_val,  # unseen data
    y_train_un,
    y_val,  # unseen data
    desc="Bagging under GridSearchCV",
    roc=True,
)

# adding to list of model
models["Bagging under GridSearchCV"] = bagging_un_gridSearchCV

make_confusion_matrix(bagging_un_gridSearchCV, X_val, y_val)

# save to the summary of model performances
df_scores = pd.concat([df_scores, df_scores_bag_un_gridCV], axis=0, ignore_index=True)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""### <a id ="link4-3-4"> D.3.4 Bagging Classifier with Randomized Search CV, over sampling </a>


*Go To <a href=#link0> Index </a>*


"""

"""
    Bagging classifier on Decision Tree on over sampling dataset, tuned with 
    Grid Search CV was best performing model so far. Hence we use that as base estimator.
    
"""

bagging_over_randomCV = BaggingClassifier(
    base_estimator=models[
        "DTree oversampled GridSearchCV"
    ],  # use the best dTree with over sampling
    random_state=1,
)  # keeing hyper parameters default values mostly

bagging_over_randomCV.get_params()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Grid of parameters to choose from
# ## add from article
# parameters = {
#     "bootstrap": [True],
#     "max_features": [0.7, 0.8, 0.9, 1],
#     "max_samples": [0.6, 0.7, 0.8, 0.9],
#     "n_estimators": [5, 10, 15, 20, 30, 40, 50],
#     "base_estimator__max_depth": np.arange(5,16),
#     "base_estimator__min_impurity_decrease": [0.0001, 0.001, 0.01],
#     "base_estimator__criterion": ["entropy","gini"]
# }
# 
# 
# # Run the Randomized search
# randomCV_obj = RandomizedSearchCV(
#             estimator=bagging_over_randomCV, 
#             param_distributions=parameters, 
#             scoring = {"Recall": "recall", "AUC": "roc_auc" },
#             refit="AUC", 
#             cv=5, 
#             n_iter=50)
# 
# randomCV_obj = randomCV_obj.fit(X_train_over, y_train_over)
# 
# # Set the clf to the best combination of parameters
# bagging_over_randomCV = randomCV_obj.best_estimator_
# 
# # Fit the best algorithm to the data.
# bagging_over_randomCV.fit(X_train_over, y_train_over)

# Using above defined function to get accuracy, recall and precision on train and test set
df_scores_bag_over_randomCV = get_metrics_score(
    bagging_over_randomCV,
    X_train_un,
    X_val,  # unseen data
    y_train_un,
    y_val,  # unseen data
    desc="Bagging over RandomSearchCV",
    roc=True,
)

# save to the summary of model perfromances
df_scores = pd.concat(
    [df_scores, df_scores_bag_over_randomCV], axis=0, ignore_index=True
)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""### <a id = #link4-3-5> D.3.5 Random Foerest Classifier Basic, under sampling </a>


*Go To <a href=#link0> Index </a>*

As under sampling produced better results in Bagging Classifier, Random Forest model would be tried firstly on under sampled datasets.
"""

rf_un_initial = RandomForestClassifier(
    random_state=1
)  # another combination to try is RF with class_weight and imbalanced data, if performance is unsatisfactory
rf_un_initial.fit(X_train_un, y_train_un)

df_score_rf_un_initial = get_metrics_score(
    rf_un_initial,
    X_train_un,
    X_val,  # unseen data
    y_train_un,
    y_val,  # unseen data
    desc="Random Forest under Basic",
    roc=True,
)

# adding to list of model
models["Random Forest under Basic"] = rf_un_initial

make_confusion_matrix(rf_un_initial, X_val, y_val)

# save to the summary of model perfromances
df_scores = pd.concat([df_scores, df_score_rf_un_initial], axis=0, ignore_index=True)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""### <a id = #link4-3-6> D.3.6 Random Foerest Classifier with Randomized Search CV, under sampling </a>


*Go To <a href=#link0> Index </a>*

As under sampling produced better results in Bagging Classifier, Random Forest model would be tried firstly on under sampled datasets.
"""

rf_un_initial.get_params()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Grid of parameters to choose from
# ## add from article
# parameters = {
#     "bootstrap": [True],
#     "max_depth" : np.arange(6,16),
#     "max_features": [0.5, 0.6, 0.7, 0.8, 0.9],
#     "max_samples": [0.5, 0.6, 0.7, 0.8, 0.9],
#     "n_estimators": [50, 100, 150, 200],
#     "min_impurity_decrease": [0.0001, 0.001, 0.01],
#     "criterion": ["entropy","gini"]
# }
# 
# 
# # Run the Randomized search
# randomCV_obj = RandomizedSearchCV(
#             estimator=rf_un_initial, 
#             param_distributions=parameters, 
#             scoring = {"Recall": "recall", "AUC": "roc_auc" },
#             refit="AUC", 
#             cv=5, 
#             n_iter=20)
# 
# randomCV_obj = randomCV_obj.fit(X_train_un, y_train_un)
# 
# # Set the clf to the best combination of parameters
# rf_un_randomCV = randomCV_obj.best_estimator_
# 
# # Fit the best algorithm to the data.
# rf_un_randomCV.fit(X_train_un, y_train_un)

df_score_rf_un_randomCV = get_metrics_score(
    rf_un_randomCV,
    X_train_un,
    X_val,  # unseen data
    y_train_un,
    y_val,  # unseen data
    desc="Random Forest under RandomSearchCV",
    roc=True,
)

# adding to list of model
models["Random Forest under RandomSearchCV"] = rf_un_randomCV

make_confusion_matrix(rf_un_randomCV, X_val, y_val)

# save to the summary of model perfromances
df_scores = pd.concat([df_scores, df_score_rf_un_randomCV], axis=0, ignore_index=True)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""### <a id ="link4-3-7"> D.3.7 Random Foerest Classifier with Randomized Search CV, over sampling </a>


*Go To <a href=#link0> Index </a>*

"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# rf_over_randomCV = RandomForestClassifier(random_state=1)
# 
# # Grid of parameters to choose from
# ## add from article
# parameters = {
#     "bootstrap": [True],
#     "max_depth" : np.arange(6,18),
#     "max_features": [ 0.7, 0.8, 0.9, 1],
#     "max_samples": [0.7, 0.8, 0.9],
#     "n_estimators": [50, 100, 150, 200],
#     "min_impurity_decrease": [0.0001, 0.001],
#     "criterion": ["entropy","gini"]
# }
# 
# 
# # Run the Randomized search
# randomCV_obj = RandomizedSearchCV(
#             estimator=rf_over_randomCV, 
#             param_distributions=parameters, 
#             scoring = {"Recall": "recall", "AUC": "roc_auc" },
#             refit="AUC", 
#             cv=5, 
#             n_iter=20)
# 
# randomCV_obj = randomCV_obj.fit(X_train_over, y_train_over)
# 
# # Set the clf to the best combination of parameters
# rf_over_randomCV = randomCV_obj.best_estimator_
# 
# # Fit the best algorithm to the data.
# rf_over_randomCV.fit(X_train_over, y_train_over)

df_score_rf_over_randomCV = get_metrics_score(
    rf_over_randomCV,
    X_train_over,
    X_val,  # unseen data
    y_train_over,
    y_val,  # unseen data
    desc="Random Forest over RandomSearchCV",
    roc=True,
)

# adding to list of model
models["Random Forest over RandomSearchCV"] = rf_over_randomCV

make_confusion_matrix(rf_over_randomCV, X_val, y_val)

# save to the summary of model perfromances
df_scores = pd.concat([df_scores, df_score_rf_over_randomCV], axis=0, ignore_index=True)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""## <a id="link4-4"> D.4 Ensemble method : Boosting Models </a>

*Go To <a href=#link0>Index</a>*

### <a id="link-4-4-1"> D.4.1 AdaBoost with Randomized Search CV, under sampling </a> 
"""

adaBoost_un_randomCV = AdaBoostClassifier(
    random_state=1, base_estimator=models["DTree undersampled GridSearchCV"]
)
adaBoost_un_randomCV.get_params()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# 
# # Grid of parameters to choose from
# ## add from article
# parameters = {
#     "learning_rate": [0.01, 0.1, 0.5, 1],
#     "n_estimators": [50, 100, 150, 200],
#     "base_estimator__max_depth" : np.arange(6,14),
# #     "base_estimator__criterion": ["entropy","gini"],
# #     "base_estimator" : [ models["DTree undersampled GridSearchCV"],
# #                          models["DTree undersampled RandomizedSearchCV"],
# #                          models["Decision Tree under sample"] 
# #                        ]
#                          
# }
# 
# 
# # Run the Randomized search
# randomCV_obj = RandomizedSearchCV(
#             estimator=adaBoost_un_randomCV, 
#             param_distributions=parameters, 
#             scoring = {"Recall": "recall", "AUC": "roc_auc" },
#             refit="AUC", 
#             cv=5, 
#             n_iter=20)
# 
# randomCV_obj = randomCV_obj.fit(X_train_un, y_train_un)
# 
# # Set the clf to the best combination of parameters
# adaBoost_un_randomCV = randomCV_obj.best_estimator_
# 
# # Fit the best algorithm to the data.
# adaBoost_un_randomCV.fit(X_train_un, y_train_un)

df_score_adaBoost_un_randomCV = get_metrics_score(
    adaBoost_un_randomCV,
    X_train_un,
    X_val,  # unseen data
    y_train_un,
    y_val,  # unseen data
    desc="AdaBoost under RandomSearchCV",
    roc=True,
)

# adding to list of model
models["AdaBoost under RandomSearchCV"] = adaBoost_un_randomCV

make_confusion_matrix(adaBoost_un_randomCV, X_val, y_val)

# save to the summary of model perfromances
df_scores = pd.concat(
    [df_scores, df_score_adaBoost_un_randomCV], axis=0, ignore_index=True
)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""## <a id="link4-4-2"> D.4.2 GradientBoost with Randomized Search CV, under sampling </a> 


*Go To <a href=#link0> Index </a>*


"""

gradientBoost_un_randomCV = GradientBoostingClassifier(random_state=1)
gradientBoost_un_randomCV.get_params()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# 
# # Grid of parameters to choose from
# ## add from article
# parameters = {
#     "learning_rate": [0.01, 0.1, 0.5, 1],
#     "n_estimators": [50, 100, 150, 200, 250],
#     "max_depth" : np.arange(3,10),
#     "max_features" : [0.4, 0.5, 0.6, 0.7,0.8],
# #     "criterion": ["friedman_mse","mse", "mae"],
#                          
# }
# 
# 
# # Run the Randomized search
# randomCV_obj = RandomizedSearchCV(
#             estimator=gradientBoost_un_randomCV, 
#             param_distributions=parameters, 
#             scoring = {"Recall": "recall", "AUC": "roc_auc" },
#             refit="AUC", 
#             cv=5, 
#             n_iter=20)
# 
# randomCV_obj = randomCV_obj.fit(X_train_un, y_train_un)
# 
# # Set the clf to the best combination of parameters
# gradientBoost_un_randomCV = randomCV_obj.best_estimator_
# 
# # Fit the best algorithm to the data.
# gradientBoost_un_randomCV.fit(X_train_un, y_train_un)

df_score_gradientBoost_un_randomCV = get_metrics_score(
    gradientBoost_un_randomCV,
    X_train_un,
    X_val,  # unseen data
    y_train_un,
    y_val,  # unseen data
    desc="GradientBoost under RandomSearchCV",
    roc=True,
)

# adding to list of model
models["GradientBoost under RandomSearchCV"] = gradientBoost_un_randomCV

make_confusion_matrix(gradientBoost_un_randomCV, X_val, y_val)

# save to the summary of model perfromances
df_scores = pd.concat(
    [df_scores, df_score_gradientBoost_un_randomCV], axis=0, ignore_index=True
)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""Next algorithm XGBoost is a special form of Gardient Boosting. We'll try that model performance and decide on trial of over sampling dataset.


## <a id="link4-4-3"> D.4.3 XGBoost with Randomized Search CV, under sampling </a> 


*Go To <a href=#link0> Index </a>*

"""

xgBoost_un_randomCV = xgb.XGBClassifier(
    random_state=1,
    eval_metric="logloss",
    #     scale_pos_weight=CLASS_WEIGHT_DICT[1] / CLASS_WEIGHT_DICT[0],
)


xgBoost_un_randomCV.get_params()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# 
# # Grid of parameters to choose from
# ## add from article
# parameters = {
#     "learning_rate": [0.01, 0.1, 0.5, 1],
#     "n_estimators": [50, 80, 100, 120, 150, 180, 200],
#     "subsample":[0.7, 0.8, 0.9, 1],
#     "scale_pos_weight":[0,1,5,7,10,12,15],
#     "gamma":[0, 1, 3, 5],
# #     "criterion": ["friedman_mse","mse", "mae"],
#                          
# }
# 
# 
# # Run the Randomized search
# randomCV_obj = RandomizedSearchCV(
#             estimator=xgBoost_un_randomCV, 
#             param_distributions=parameters, 
#             scoring = {"Recall": "recall", "AUC": "roc_auc" },
#             refit="AUC", 
#             cv=5, 
#             n_iter=40)
# 
# randomCV_obj = randomCV_obj.fit(X_train_un, y_train_un)
# 
# # Set the clf to the best combination of parameters
# xgBoost_un_randomCV = randomCV_obj.best_estimator_
# 
# # Fit the best algorithm to the data.
# xgBoost_un_randomCV.fit(X_train_un, y_train_un)

df_score_xgBoost_un_randomCV = get_metrics_score(
    xgBoost_un_randomCV,
    X_train_un,
    X_val,  # unseen data
    y_train_un,
    y_val,  # unseen data
    desc="XGBoost under RandomSearchCV",
    roc=True,
)

# adding to list of model
models["XGBoost under RandomSearchCV"] = xgBoost_un_randomCV

make_confusion_matrix(xgBoost_un_randomCV, X_val, y_val)

# save to the summary of model perfromances
df_scores = pd.concat(
    [df_scores, df_score_xgBoost_un_randomCV], axis=0, ignore_index=True
)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""AdaBoost and XGBoost are best performing models so far. We'll try XGBoost on over sampling dataset.


## <a id="link4-4-4"> D.4.4 XGBoost with Randomized Search CV, over sampling </a> 

*Go To <a href=#link0> Index </a>*


"""

xgBoost_over_randomCV = xgb.XGBClassifier(
    random_state=1,
    eval_metric="logloss",
    #     scale_pos_weight=CLASS_WEIGHT_DICT[1] / CLASS_WEIGHT_DICT[0],
)


xgBoost_over_randomCV.get_params()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# 
# # Grid of parameters to choose from
# ## add from article
# parameters = {
#     "learning_rate": [0.01, 0.1, 0.5, 1],
#     "n_estimators": [50, 80, 100, 120, 150, 180, 200],
#     "subsample":[0.7, 0.8, 0.9, 1],
#     "scale_pos_weight":[0,1,5,7,10,12,15],
#     "gamma":[0, 1, 3, 5],
# #     "criterion": ["friedman_mse","mse", "mae"],
#                          
# }
# 
# 
# # Run the Randomized search
# randomCV_obj = RandomizedSearchCV(
#             estimator=xgBoost_over_randomCV, 
#             param_distributions=parameters, 
#             scoring = {"Recall": "recall", "AUC": "roc_auc" },
#             refit="AUC", 
#             cv=5, 
#             n_iter=40)
# 
# randomCV_obj = randomCV_obj.fit(X_train_over, y_train_over)
# 
# # Set the clf to the best combination of parameters
# xgBoost_over_randomCV = randomCV_obj.best_estimator_
# 
# # Fit the best algorithm to the data.
# xgBoost_over_randomCV.fit(X_train_over, y_train_over)

df_score_xgBoost_over_randomCV = get_metrics_score(
    xgBoost_over_randomCV,
    X_train_over,
    X_val,  # unseen data
    y_train_over,
    y_val,  # unseen data
    desc="XGBoost over RandomSearchCV",
    roc=True,
)

# adding to list of model
models["XGBoost over RandomSearchCV"] = xgBoost_over_randomCV

make_confusion_matrix(xgBoost_over_randomCV, X_val, y_val)

# save to the summary of model perfromances
df_scores = pd.concat(
    [df_scores, df_score_xgBoost_over_randomCV], axis=0, ignore_index=True
)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""AdaBoost and XGBoost are best performing models so far. We'll try AdaBoost on over sampling dataset.


### <a id="link4-4-5"> D.4.5 AdaBoost with Randomized Search CV, over sampling </a> 


*Go To <a href=#link0> Index </a>*

"""

adaBoost_over_randomCV = AdaBoostClassifier(
    random_state=1, base_estimator=models["DTree oversampled GridSearchCV"]
)
adaBoost_un_randomCV.get_params()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# 
# # Grid of parameters to choose from
# ## add from article
# parameters = {
#     "learning_rate": [0.01, 0.1, 0.5, 1],
#     "n_estimators": [50, 100, 150, 200],
#     "base_estimator__max_depth" : np.arange(5,14)
# }
# 
# 
# # Run the Randomized search
# randomCV_obj = RandomizedSearchCV(
#             estimator=adaBoost_over_randomCV, 
#             param_distributions=parameters, 
#             scoring = {"Recall": "recall", "AUC": "roc_auc" },
#             refit="AUC", 
#             cv=5, 
#             n_iter=10)
# 
# randomCV_obj = randomCV_obj.fit(X_train_over, y_train_over)
# 
# # Set the clf to the best combination of parameters
# adaBoost_over_randomCV = randomCV_obj.best_estimator_
# 
# # Fit the best algorithm to the data.
# adaBoost_over_randomCV.fit(X_train_over, y_train_over)

df_score_adaBoost_over_randomCV = get_metrics_score(
    adaBoost_over_randomCV,
    X_train_over,
    X_val,  # unseen data
    y_train_over,
    y_val,  # unseen data
    desc="AdaBoost over RandomSearchCV",
    roc=True,
)

# adding to list of model
models["AdaBoost over RandomSearchCV"] = adaBoost_over_randomCV

make_confusion_matrix(adaBoost_over_randomCV, X_val, y_val)

# save to the summary of model perfromances
df_scores = pd.concat(
    [df_scores, df_score_adaBoost_over_randomCV], axis=0, ignore_index=True
)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""AdaBoost and XGBoost are best performing models so far. We'll try to tune AdaBoost with GridSearchCV, on under sampling dataset.


### <a id=#link4-4-6> D.4.6 AdaBoost with Grid Search CV, under sampling </a> 


*Go To <a href=#link0> Index </a>*


"""

adaBoost_un_gridCV = AdaBoostClassifier(
    random_state=1, base_estimator=models["DTree undersampled GridSearchCV"]
)
adaBoost_un_gridCV.get_params()

models["AdaBoost under RandomSearchCV"].get_params()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# 
# # Grid of parameters to choose from
# ## add from article
# parameters = {
#     "learning_rate": [  0.5, 0.7, 0.9],
#     "n_estimators": [200, 220, 250],
#     "base_estimator__criterion": ["entropy"],
#     "base_estimator__max_depth" : np.arange(9,13),
# #     "base_estimator__min_impurity_decrease": [0.0001, 0.001]
# }
# 
# 
# # Run the Randomized search
# gridCV_obj = GridSearchCV(
#             estimator=adaBoost_un_gridCV, 
#             param_grid=parameters, 
#             scoring = {"Recall": "recall", "AUC": "roc_auc" },
#             refit="AUC", 
#             cv=5)
# 
# gridCV_obj = gridCV_obj.fit(X_train_un, y_train_un)
# 
# # Set the clf to the best combination of parameters
# adaBoost_un_gridCV = gridCV_obj.best_estimator_
# 
# # Fit the best algorithm to the data.
# adaBoost_un_gridCV.fit(X_train_un, y_train_un)

df_score_adaBoost_un_gridCV = get_metrics_score(
    adaBoost_un_gridCV,
    X_train_over,
    X_val,  # unseen data
    y_train_over,
    y_val,  # unseen data
    desc="AdaBoost under GridSearchCV",
    roc=True,
)

# adding to list of model
models["AdaBoost under GridSearchCV"] = adaBoost_un_gridCV

make_confusion_matrix(adaBoost_un_gridCV, X_val, y_val)

# save to the summary of model perfromances
df_scores = pd.concat(
    [df_scores, df_score_adaBoost_un_gridCV], axis=0, ignore_index=True
)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""We'll try tuning XGBoost by GridSearchCV  on under sampling dataset.


### <a id="link4-4-7"> D.4.7 XGBoost with Grid Search CV, under sampling </a> 


*Go To <a href=#link0> Index </a>*

"""

xgBoost_un_gridCV = xgb.XGBClassifier(
    random_state=1,
    eval_metric="logloss",
    #     scale_pos_weight=CLASS_WEIGHT_DICT[1] / CLASS_WEIGHT_DICT[0],
)


xgBoost_un_gridCV.get_params()

xgBoost_un_randomCV.get_params()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# 
# # Grid of parameters to choose from
# ## add from article
# parameters = {
#     "learning_rate": [0.01, 0.1, 0.5],
#     "n_estimators": [130, 150, 180, 200],
#     "subsample":[0.6, 0.7, 0.8, 0.9],
#     "scale_pos_weight":[3,5,7],
# #     "gamma":[0, 1, 3],
# #     "criterion": ["friedman_mse","mse", "mae"],
# }
# 
# 
# # Run the Randomized search
# gridCV_obj = GridSearchCV(
#             estimator=xgBoost_un_gridCV, 
#             param_grid=parameters, 
#             scoring = {"Recall": "recall", "AUC": "roc_auc" },
#             refit="AUC", 
#             cv=5)
# 
# gridCV_obj = gridCV_obj.fit(X_train_un, y_train_un)
# 
# # Set the clf to the best combination of parameters
# xgBoost_un_gridCV = gridCV_obj.best_estimator_
# 
# # Fit the best algorithm to the data.
# xgBoost_un_gridCV.fit(X_train_un, y_train_un)

df_score_xgBoost_un_gridCV = get_metrics_score(
    xgBoost_un_gridCV,
    X_train_over,
    X_val,  # unseen data
    y_train_over,
    y_val,  # unseen data
    desc="XGBoost under GridSearchCV",
    roc=True,
)

# adding to list of model
models["XGBoost under GridSearchCV"] = xgBoost_un_gridCV

make_confusion_matrix(xgBoost_un_gridCV, X_val, y_val)

# save to the summary of model perfromances
df_scores = pd.concat(
    [df_scores, df_score_xgBoost_un_gridCV], axis=0, ignore_index=True
)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""We'll try Stacking Classifier too.


## <a id="link4-5"> D.5 Stacking Classifiers </a> 

*Go To <a href=#link0> Index </a>*

"""

# The best model is chosen as final estimator, other models are as base estimators,
# except similar class of the final estimator
stacking_classifier_un = StackingClassifier(
    estimators=[
        #         ("Logistics Regression Basic", models["Logistics Regression Basic"]),
        (
            "Logistics Regression under sampling",
            models["Logistics Regression under sampling"],
        ),
        (
            "Logistics Regression under RandomizedSearchCV",
            models["Logistics Regression under RandomizedSearchCV"],
        ),
        #         ("Decision Tree Basic", models["Decision Tree Basic"]),
        (
            "DTree undersampled RandomizedSearchCV",
            models["DTree undersampled RandomizedSearchCV"],
        ),
        ("DTree undersampled GridSearchCV", models["DTree undersampled GridSearchCV"]),
        (
            "Bagging Classifier under initial",
            models["Bagging Classifier under initial"],
        ),
        ("Bagging under RandomSearchCV", models["Bagging under RandomSearchCV"]),
        ("Bagging under GridSearchCV", models["Bagging under GridSearchCV"]),
        (
            "Random Forest under RandomSearchCV",
            models["Random Forest under RandomSearchCV"],
        ),
        ("Random Forest under Basic", models["Random Forest under Basic"]),
    ],
    final_estimator=models["XGBoost under GridSearchCV"],  # the best model so far
    cv=5,
)
stacking_classifier_un.get_params()

stacking_classifier_un.fit(X_train_un, y_train_un)

df_score_stacking_classifier_un = get_metrics_score(
    stacking_classifier_un,
    X_train_over,
    X_val,  # unseen data
    y_train_over,
    y_val,  # unseen data
    desc="Stacking Classifier under",
    roc=True,
)

# adding to list of model
models["Stacking Classifier under"] = stacking_classifier_un

make_confusion_matrix(stacking_classifier_un, X_val, y_val)

# save to the summary of model perfromances
df_scores = pd.concat(
    [df_scores, df_score_stacking_classifier_un], axis=0, ignore_index=True
)
df_scores.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""#  <a id ="link5"> E. Model Selection and Analysis </a>

*Go To <a href = #link0>Index</a>*


- Recall in Test dataset needs to be high, as cost of lost oppertunity (False Negetive) is high.
- As business objective is to identify the credit card member, who would churn, the following KPI of model performance are also important:  Accuracy in Test, F1 in Test (balance between Precision and Recall), ROC-AUC in Test (balance between Precision and Recall). 
- We need to watch if the model performance KPI values are close to each other with respect to Train and Test datasets. A deviation over 10% in Accuracy, Precision (not so important in this use case as customers may change mind and purchase after marketting), and Recall would represent high bias (overfitting), which means the model would likely degrade performance in unseen data.
"""

# Select a few Model Performance KPI important to the use-case.
df_scores_short = df_scores[
    [
        "Description",  # Name of the model
        "Recall-Train",
        "Recall-Test",
        "ROC-AUC-Train",  # Balance of Precision-Recall
        "ROC-AUC-Test",  # Balance of Precision-Recall
        "Accuracy-Train",
        "Accuracy-Test",
    ]
]

df_scores_short.sort_values(
    by=["Recall-Test", "ROC-AUC-Test"],
    ascending=False,
    ignore_index=True,
    inplace=True,
)


df_scores_short

# plot top 10 models's performance
fig, axs = plt.subplots(figsize=(20, 5))
df_scores_short.head(10).plot(kind="bar", ax=axs, legend=True)
plt.title("Model Performance ")
plt.xticks(
    ticks=df_scores_short.head(10).index, labels=df_scores_short["Description"].head(10)
)
fig.show()

"""##  <a id ="link5-1">E.1 Feature Importance </a>

*Go To <a href=#link0>Index</a>*

Let us look at the feature impotance of the best Model.
"""

# The top perfroming model 
# Note: The feature importance may vary a little among top 3 best performing models. 
# We'll note top 10 feature importance.
models[ df_scores_short.loc[0, "Description"] ]

# importance of features
df_imp = feature_importance_in_tree(
    dTree=models[df_scores_short.loc[0, "Description"]],
    feature_names=X_train.columns,
    model_name=df_scores_short.loc[0, "Description"],
)

"""### Observation by Sanjib:

1. The three independent variables, where we applied KNN imputation previously, features among less important variables (feature importance less than 0.03). e.g. "Marital_Status", "Education_Level", and "Income_Category".
2. Though XGBoost can handle missing values, we provided KNN imputed values, which wont matter much to the model performance (predicting the target variable).

"""

df_imp.head(10)  # Top 10 influencing factor of the churn

"""##  <a id="link5-2">E.2 Model Performance on unseen data </a>

*Go To <a href =#link0>Index</a>*

We kept aside Test dataset to avoid data leakage. 

Now let us predict on the Test dataset using top 3 models, and observe the results. 

"""

# Top 3 model names
[df_scores_short.loc[x, "Description"] for x in np.arange(0, 3)]

# Top 3 models
[models[df_scores_short.loc[x, "Description"]] for x in np.arange(0, 3)]

# Check model performance on completely unseen dataset

df_score_test = None

for x in np.arange(0, 3):
    print("=" * 80)
    print(x, ". Model =", df_scores_short.loc[x, "Description"])
    df_score_x = get_metrics_score(
        models[df_scores_short.loc[x, "Description"]],
        X_train_un,
        X_test,  # unseen data
        y_train_un,
        y_test,  # unseen data
        desc=df_scores_short.loc[x, "Description"],
        roc=True,
    )
    print(
        make_confusion_matrix(
            models[df_scores_short.loc[x, "Description"]], X_test, y_test
        )
    )
    if x == 0:
        df_score_test = df_score_x
    else:
        df_score_test = pd.concat(
            [df_score_test, df_score_x], axis=0, ignore_index=True
        )

df_score_test.sort_values(by=["Recall-Test", "ROC-AUC-Test"], ascending=False)

"""### Observation by Sanjib:

1. The top 3 models' performance KPI are consistent on Test dataset. The KPI values in Test dataset are close to the KPI values in Validation dataset.
2. Thus the top 3 models are generalized, and can be deployed in production to predict on unseen data.
3. The top model in validation dataset overfits in terms of Precision significantly, which is observed both in validation and test dataset. All the performance KPI of "XGBoost under GridSearchCV" look better overall; because they are consistent accross Valdation and Test dataset; and less overfitting. In the unseen Test dataset, the recall score of "XGBoost under GridSearchCV" is best, having ROC-AUC great too.

## "XGBoost under GridSearchCV" is the selected model

#  <a id ="link6">F. Build pipeline on selected model </a>

*Go To <a href = #link0>Index</a>*
"""

# The top perfroming model
df_score_test.sort_values(
    by=["Recall-Test", "ROC-AUC-Test"],
    ascending=False,
    inplace=True,
    ignore_index=True,
)
df_score_test

"""
    XGBoost model can handle missing values. 
    Hence, during data engineering, missing value treatment is not required.
    Data scaling is not required too.
    Following are the recap of the steps performed in data engg, model fit 
    and transform of the top performing model XGBoost:
    
    1. Drop column "CLIENTNUM", having irrelevant info.
    2. Categorical variable encoding:
        a. Encode "Education_Level", "Income_Category", "Card_Category" into ordinal labels.
        b. Encode target variable "Attrition_Flag" into ordinal values.
        c. Reduce memmory by down casting data types 
        d. Split the dataset into Train, Validation, and Test.
        e. Dummy varibale encoding of 'Gender', 'Marital_Status'
    3. Undersample the dataset using RandomUserSampler.  
    4. Build a XGBoost Classifer with the tuned hyper paramters, as observed in GridSearchCV.
    5. Fit and transform.
    6. Predict. 
"""
selected_model = models[df_score_test.loc[0, "Description"]]
selected_model

"""
    Note : Data Engineering steps can be put into pipeline, which would be useful in Production prediction.
    However, for the sake of simplicity, we are going to put fit, transform and predict steps 
    into Pipeline and test.
"""
pipeline = Pipeline(
    steps=[
        # Scaling of data is not necessary for XGBoost model
        ("XGBoost", selected_model)
    ]
)

# train with fit
pipeline.fit(X_train_un, y_train_un)

# predict on unseen data
y_predict = pipeline.predict(X_test)
# model_score = pipeline.score(X_test, y_test)
# print(model_score)
# print()
# print(metrics.confusion_matrix(y_test, y_predict))

get_metrics_score(
    pipeline,
    X_train_un,
    X_test,
    y_train_un,
    y_test,
    threshold=0.5,
    flag=True,
    roc=True,
    desc="Selected Model XGBoost under GridSearchCV",
)

make_confusion_matrix(pipeline, X_test, y_test)

"""### Observation by Sanjib:

1. The results of the Pipeline, which contains the selected model, matches with the previously executed results of same model.

# <a id ="link7"> G. Business Recomendation </a>

*Go To <a href=#link0> Index </a>*

## <a id ="link7-1"> G.1 Insights on Customers of what profile (characteristics) are more likely to churn </a>

Descriptive analytics from section [C. Exploratory Data Analysis](#link3) and [E. Model Selection and Analysis](#link5)
"""

df_imp # Feature Importance according to top model

df_imp.columns[0]

df_insight = pd.DataFrame([
    {
        "Type"     : "Summary",
        "Importance" : 1,
        "Insight"  : "The top 10 influencing factors of churning (attrition) are in decendeding order as: " + 
                            ", ".join(df_imp.head(10).index.values)
    },
    {
        "Type"     : "Feature",
        "Importance" : df_imp.loc["Customer_Age",df_imp.columns[0]],
        "Insight"  : "The absolute number of attrition is high between age 35 to 60, very high between ages " +
            " 40 and 55. Average age of customers' attritated, is little higher than that of not attrited."
    },
    {
        "Type"     : "Feature",
        "Importance" : df_imp.loc["Dependent_count",df_imp.columns[0]],
        "Insight"  : "The attrition is high among customers with dependent count 1,2,3,4. Average dependent "
        + "count is litte higher in attrited customers compared to non attrited, which holds true for "
        + "Card-Category Blue and Sliver, most sold."
    },
    {
        "Type"     : "Feature",
        "Importance" : df_imp.loc["Education_Level",df_imp.columns[0]],
        "Insight"  : "The absolute number of attrition is high among Education Level Uneducated, High " 
        + "School, Graduate. The proportion of atttited and not attrited does not vary drastically accross " 
        + "various Education-Levels. Attrition is high among Platinum customers with higher Education-Levels."
    },
    {
        "Type"     : "Feature",
        "Importance" : df_imp.loc["Income_Category",df_imp.columns[0]],
        "Insight"  : "The absolute number of attrition is high in Income Category Less than 40K(highest), "
            + "40K - 60K, and 80K - 120K (in dollar). The proportion of atttited vs. not attrited "
            + "does not vary drastically accross various Income Categories, high in Less than 40K."
    },    
    {
        "Type"     : "Feature",
        "Importance" : df_imp.loc["Months_on_book",df_imp.columns[0]],
        "Insight"  : "The absolute number of attrition is high among customers having Months-On-Book between "
            + "30 and 40, 36 being highest. The attrition is low with lower Months-On-Book (< 25), and "
            + "with higher Months-On-Book (> 45). The attrition is high within range 25-45 of Months-On-Book"
    },
    {
        "Type"     : "Feature",
        "Importance" : df_imp.loc["Total_Relationship_Count",df_imp.columns[0]],
        "Insight"  : "The absolute number of attrition is high among customers having Total-Relationship-Count "
            + "3, 2, 1 (lower). The propotion of attrited vs. not attrited is highest with " 
            + "Total-Relationship-Count 1; and lower with 4,5,6. High Total-Relationship-Count seems to be "
            + "related with lower attrition. Blue and Siver, consisting of majority customers, have lower "
            + "average Total-Relationship-Count in attrited customer, compared to that of non-attrited. "
    },
    {
        "Type"     : "Feature",
        "Importance" : df_imp.loc["Months_Inactive_12_mon",df_imp.columns[0]],
        "Insight"  : "The absolute number of attrition is high among customers having Months-Inactive-12-Month "
            + "as 3 and 2, 3 being highest. The propotion of attrited vs. not attrited is high with "
            + "Months-Inactive-12-Month as 4."
    },
    {
        "Type"     : "Feature",
        "Importance" : df_imp.loc["Contacts_Count_12_mon",df_imp.columns[0]],
        "Insight"  : "The absolute number of attrition is high among customers having Contacts-Count-12-Month " 
            + "as 2,3 and 4. The propotion of attrited vs. not attrited is high with Contacts-Count-12-Month "
            + "as 4. In all products average Contacts_Count_12_mon is higher for attrited customers compared "
            + "to that of non attrited. Before churing, customers might have contacted bank frequently, "
            + "or bank may have contacted the customers. "
    },
    {
        "Type"     : "Feature",
        "Importance" : df_imp.loc["Credit_Limit",df_imp.columns[0]],
        "Insight"  : "The absolute number of attrition is high among customers having Credit-Limit lower than " 
            + "5000. It is also high with Credit limit 34,000. The propotion of attrited vs. not attrited " 
            + "is high with Credit-Limit 2,000." 
    },
    {
        "Type"     : "Feature",
        "Importance" : df_imp.loc["Total_Revolving_Bal",df_imp.columns[0]],
        "Insight"  : "The abosulte number of attrition is high among Total-Revolving-Balance as 0, and as " 
            + "2,500. Between Total-Revolving-Balance 125 and 375, approximately, all customers attrited. " 
            + "The proportion of attrited vs. not attrited is lower with other Total-Revolving-Balance. "
            + "In all products, average Total-Revolving-Balance is lower in attrited customers, compared "
            + "to that of non attrited."
    },
    {
        "Type"     : "Feature",
        "Importance" : df_imp.loc["Avg_Open_To_Buy",df_imp.columns[0]],
        "Insight"  : "The abosulte number of attrition is high among Average-Open-To-Buy below 5,000. " 
            + "The proportion of attrited vs. not attrited is high with Average-Open-To-Buy as 34,000." 
            + "The attrition is low between 20,000 and 30,000 of Average-Open-To-Buy." 
    },
    {
        "Type"     : "Feature",
        "Importance" : df_imp.loc["Total_Amt_Chng_Q4_Q1",df_imp.columns[0]],
        "Insight"  : "The abosulte number of attrition is high among Total-Amount-Change-Q4-Q1 between 0.3 and " 
            + "1.1. Arround these two limit values of Total-Amount-Change-Q4-Q1, the proportion of attrited " 
            + "vs. not attrited is high. In Blue and Silver, which consititutes majority of customers, average "
            + "value of Total-Amount-Change-Q4-Q1 is lower among attrited customers, compared to non attrited."
    },    
    {
        "Type"     : "Feature",
        "Importance" : df_imp.loc["Total_Trans_Amt",df_imp.columns[0]],
        "Insight"  : "The abosulte number of attrition is high among Total-Transaction-Amount between 1,500 "  
            + "and 3,000. The proportion of attrited vs. not attrited is high arround 2,300 of " 
            + "Total-Transaction-Amount."
    },
    {
        "Type"     : "Feature",
        "Importance" : df_imp.loc["Total_Trans_Ct",df_imp.columns[0]],
        "Insight"  : "The abosulte number of attrition is high among Total-Transaction-Count between 30 and " 
            + "and 55, being highest arround 40. The proportion of attrited vs. not attrited is high arround " 
            + "40 of Total-Transaction-Count, and lower than 20 Total-Transaction-Count. The average " 
            + "Total-Transaction-Count of attired customers are lower than that of non attrired."
    },    
    {
        "Type"     : "Feature",
        "Importance" : df_imp.loc["Total_Ct_Chng_Q4_Q1",df_imp.columns[0]],
        "Insight"  : "The abosulte number of attrition is high among Total-Count-Change-Q4-Q1 between 0.2 " 
            + "and 1.0. The proportion of attrited vs. not attrited is high below 0.5 of " 
            + "Total-Count-Change-Q4-Q1. The average Total-Count-Change-Q4-Q1 of attired customers are "
            + "lower than that of non attrired, holds true in all products." 
    },
    {
        "Type"     : "Feature",
        "Importance" : df_imp.loc["Avg_Utilization_Ratio",df_imp.columns[0]],
        "Insight"  : "The abosulte number of attrition is high among Average-Utilization-Ratio lower than " 
            + "0.2, highest at 0. The proportion of attrited vs. not attrited is highest at 0 of the " 
            + "Average-Utilization-Ratio." 
    },    
    {
        "Type"     : "Feature",
        "Importance" : df_imp.loc["Card_Category",df_imp.columns[0]],
        "Insight"  : "The Card-Category Platinum and Gold have a very few attrition, but more importantly, " 
            + "they have high proportion of attrited vs. non attrited. The absolute number of " 
            + "attrition is highest among Card-Category Blue, which is most occuring(sold) Card-Category. "
            + "Silver is a mediumly occuring Card-Category, with proportional attrition."
    },
    {
        "Type"     : "Feature",
        "Importance" : df_imp.loc["Gender_M",df_imp.columns[0]],
        "Insight"  : "Female customers are higher in number, and have higher absolute number of " 
            + "attrition, maintaing the propportion between attrited and non attrited. " 
    },    
    {
        "Type"     : "Feature",
        "Importance" : df_imp.loc["Marital_Status_Single",df_imp.columns[0]],
        "Insight"  : "The absolute number of attrition is high among Married customers, who are majority " 
            + "too. The proportion of attrited vs. not attrited is high in Single customers."
    },    
])

print("Insights from data follows,",
      "primarily on the characteristics what attrition(churning) is corelated to.",
      "Note: Importance represents influence of variable to the attrition."   
     )

df_insight.sort_values(by=["Importance"], ascending=False)

"""## <a id ="link7-2"> G.2 Insights on Customer profiles (characeristics) per Product </a>
## : Market Segments

*Go To <a href=#link0> Index </a>*

The insights would be focused primarily on top 10 features that influence attrition.
Descriptive analytics from section [C. Exploratory Data Analysis](#link3) and [E. Model Selection and Analysis](#link5)
"""

print(
    "Insights from data follows, on the various Credit Card Products (Card Categories), focused on attrition.",
    "Note: Other variables of lesser importance have more insights, which can be documented over",
    "excess of time. Also, feature importance changes with re-run of this notebook, as changes models'",
    "performance little.",
)


df_insight_product = pd.DataFrame(
    [
        {
            "Type": "Major",
            "Product Insight": "The Credit Card product Blue is purchased most. The product Platinum and Gold "
            + "have high proportion of attrition vs. not attrited. The product Sliver and Gold are mediumly "
            + "sold. The Blue is a mass product, Platinum and Gold are niche product for handful of high "
            + "income customers. The Silver and Gold fits in middle. The Platinum is least sold.",
        },
        {
            "Type": "Major",
            "Product Insight": "The products Blue, Sliver, Gold, and Platinum have average "
            + "Total-Transaction-Count in the ascending order. Mass product Blue has low average "
            + "Total-Transaction-Count, and lower proportion of atttrited customer compared to non attrited.",
        },
        {
            "Type": "Major",
            "Product Insight": "The products Blue, Sliver, Gold, and Platinum have average "
            + "Total-Transaction-Amount in the ascending order, Credit-Limit too. The average "
            + "Total-Transaction-Amount lower in attrited customers, compared to that of non attrited "
            + ", in all card categories.",
        },
        {
            "Type": "Major",
            "Product Insight": "The product Gold has highest average Total-Revolving-Balance, varies "
            + "significantly. The product Platinum has lowest average Total-Revolving-Balance. The Blue "
            + "has medium average Total-Revolving-Balance, which varies mostly between only 600 and 700.",
        },
        {
            "Type": "Major",
            "Product Insight": "The niche products Gold and Platinum have usually lower average "
            + "Total-Relationship-Count. The Blue and Sliver, consisting of majorty customers, have "
            + "average Total-Relationship-Count lower in attrited customers, compared to that of non "
            + "attrited. ",
        },
        {
            "Type": "Major",
            "Product Insight": "The product platinum has Total-Amount-Change-Q4-Q1 varying mostly between "
            + "only 0.5 and 1.0. The Blue has significant variation in the same. However, attrired customer "
            + "of Blue are mostly concentrated arround 0.7 of Total-Amount-Change-Q4-Q1. Others are between "
            + "0.6 and 0.9, mostly. The Blue and Sliver, consisting of majorty customers, have lower value "
            + "of Total-Amount-Change-Q4-Q1 in attried customers, compared to that of non attrited. ",
        },
        {
            "Type": "Major",
            "Product Insight": "The product platinum has Total-Count-Change-Q4-Q1 varying mostly between "
            + "only 0.4 and 1.1. The Blue has significant variation in the same. However, attrired customer "
            + "of Blue are mostly in range 0.5 to 0.6 of Total-Count-Change-Q4-Q1. Others are between 0.4 "
            + "and 0.7, mostly. The average value of Total-Count-Change-Q4-Q1 is lower in the attried "
            + "customers, compared to that of non attrited, in all products(card categories). ",
        },
        {
            "Type": "Minor",
            "Product Insight": "The product platinum has Contacts-Count-12-month mostly between 1 and 4. "
            + "However, Platimum had attrited customers who had high Contacts-Count-12-month. Other "
            + "products had 0 to 6. The average value of Contacts-Count-12-month in attried customers "
            + "is higher than that of non attrited, in all products(card categories). ",
        },
        {
            "Type": "Major",
            "Product Insight": "The product Gold has normally higher Months-Inactive-12-months. Among the "
            + "attrited customers, the product Blue has highest average Months-Inactive-12-months, about "
            + "2.7. Among the attrited customers, the product Platinum has varying range "
            + "of Months-Inactive-12-months. The average value of Months-Inactive-12-months in attried "
            + "customers is higher than that of non attrited, in all products(card categories). ",
        },
        {
            "Type": "Major",
            "Product Insight": "The product Blue has lowest average Credit-Limit, but varies in great "
            + "range. The niche product Platinum has highest average Credit-Limit, indicating high income. "
            + "Generally In all products, Credit-Limit is higher in higher Income-Categoies. "
            + "Among the attried customers, the Blue product has lowest average credit limit; Gold is "
            + "highest. In Sliver and Gold, average Credit-Limit in attrited customers is little higher "
            + "than that of non attrited. In Platinum and Blue, that is opposite. Blue is most sold card. ",
        },
    ]
)

df_insight_product.sort_values(by=["Type"], ascending=True)

# Based on the above insights, following are the product market segments
# Note : More important features are observed in the insights. With more avaiable time, more insights can be added.

print("Product market segments, with customer characteristics, and recomendations.")

df_insight_prod_seg = pd.DataFrame(
    [
        {
            "Card": "Blue",
            "Customer-Profile": "Most purchased, it is a mass product. It has lower Attrition vs. "
            + "Not Attrition proportion, low Total-Transaction-Count, low "
            + "Total-Transaction-Amount, medium Total-Revolving-Balance, medium Total-Revolving-Balance,"
            + "range of Total-Relationship-Count, high varying Total-Amount-Change-Q4-Q1, "
            + "high varying Total-Count-Change-Q4-Q1, varying Contacts-Count-12-month, "
            + "high average Months-Inactive-12-months on attrition, low Credit-Limit, "
            + "lower average Credit-Limit on attrition.",
            "Recomendation": "FOCUS MORE ON THIS HIGH VOLUME PRODUCT TO ARREST ATTRITION, STABILIZE "
            + "REVENUE FLOW. MODEL WOULD PREDICT CUSTOMERS TO CHURN(ATTRITION).",
        },
        {
            "Card": "Silver",
            "Customer-Profile": "Medium purchased, it is a mid-range product. It has lower Attrition "
            + "vs. Not Attrition proportion, medium Total-Transaction-Count, medium "
            + "medium Total-Transaction-Amount, medium Total-Revolving-Balance,"
            + "range of Total-Relationship-Count, medium varying Total-Amount-Change-Q4-Q1,"
            + "medium varying Total-Count-Change-Q4-Q1, varying Contacts-Count-12-month, "
            + "medium Months-Inactive-12-months, medium Credit-Limit, "
            + "medium average Credit-Limit on attrition.",
            "Recomendation": "FOCUS ON THIS VOLUME PRODUCT TO ARREST ATTRITION. MODEL "
            + "WOULD PREDICT CUSTOMERS TO CHURN(ATTRITION).",
        },
        {
            "Card": "Gold",
            "Customer-Profile": "Low purchased, it is a niche product of higher income. It has "
            + "higher Attrition vs. Not Attrition proportion, high Total-Transaction-Count, high "
            + "Total-Transaction-Amount, high Total-Revolving-Balance, low Total-Relationship-Count, "
            + "medium varying Total-Amount-Change-Q4-Q1, medium varying Total-Count-Change-Q4-Q1, "
            + "varying Contacts-Count-12-month, high Months-Inactive-12-months, high Credit-Limit, "
            + "higher average Credit-Limit on attrition.",
            "Recomendation": "PITCH TO APPROPRIATE CUSTOMER PROFILE AND CUSTOMIZE FEES ACCORDINGLY. "
            + "MODEL WOULD PREDICT CUSTOMERS TO CHURN(ATTRITION).",
        },
        {
            "Card": "Platinum",
            "Customer-Profile": "Very low purchased, it is a niche product of higher income. It has "
            + "higher Attrition vs. Not Attrition proportion, high Total-Transaction-Count, high "
            + "Total-Transaction-Amount, low Total-Revolving-Balance, low Total-Relationship-Count, "
            + "low varying Total-Amount-Change-Q4-Q1, low varying Total-Count-Change-Q4-Q1, "
            + "low varying Contacts-Count-12-month, varying Months-Inactive-12-months on attrition, "
            + "high Credit-Limit, lower average Credit-Lmit on attrition. ",
            "Recomendation": "PLAN TO RETIRE OR REPLACE THIS LOW SOLD PRODUCT.",
        },
    ]
)

df_insight_prod_seg

"""## <a id="link7-3"> G.3 Insights on prediction of Attrition </a>

*Go To <a href=#link0> Index </a>*


Based on the predictive analytics from section [E. Model Selection and Analysis](#link5)

## Given the customer data, the prediction model can accurately predict which are the customers that would churn (attrition). The Bank can target only the predicted customers to mitigate attrition, making mitigation more efficient and cost effective. 

1. The model has low chance of missing a prediction of churning, where customer will actually churn (attrition). In other words, the customers who will actually churn, would mostly be covered by model prediction. 
2. However, the model may predict some additional customers (arround 7%) who would not actually churn. 
3. The accuracy of targetted migitation would be arround 93%, i.e. about 93% of the customers, targetted in mitigation,  would actually wanted to churn (attrition).
4. The mitigation steps may include fees wavier, offering discounts, defferment of payments and fees etc.

# <a id="link8"> H. Appendix </a>

*Go To <a href=#link0 >Index</a>*

## <a id="link8-1"> H.1 References: </a>

I acknowledge and appreciate the following references available for public use.

| Sl. No. | Description | Reference | Section |
| --- | --- | --- | --- |
| 1 | The various Data Types Supported by Numpy. | http://omz-software.com/pythonista/numpy/user/basics.types.html and   https://numpy.org/doc/stable/user/basics.types.html | A.1.2 Optimize memory of Dataframe |

## <a id="link8-2">  H.2 Next Steps </a>

1. In order to productionize the code, the Notebook needs to saved as *.py file (Python).
2. Also, the code needs to be organized in functions and classes (Object Oriented Programming) for re-use. The code till section B is generic to apply in number of use cases, and can be put into a generic function, e.g. method of class 'EDA' (Exploratory Data Analysis), and similarly section C Data Engineering.
3. We need to introduce exception handling in the code, try-catch-finally, and logging.
4. We need to save processed data, results, graphs etc. into persistance store (e.g. disk).

Assumption: Productionization of Code is not in the scope of this assignment, even though I am capable of implementing it.

____________________________________________________________________________________________________________
Thank you for your reading of this Notebook. Please share your suggestions, comments to <sanjib007@gmail.com>

_______________________________________________________________________________________________________________

#  --   The End --
"""

# df_scores_short[
#     [
#         "Recall-Train",
#         "Recall-Test",
#         "Accuracy-Train",
#         "Accuracy-Test",
#         "ROC-AUC-Train",
#         "ROC-AUC-Test",
#     ]
# ]

