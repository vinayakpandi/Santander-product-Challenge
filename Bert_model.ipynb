{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spKnTVVsiZNT",
        "colab_type": "text"
      },
      "source": [
        "# Importing packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWXR27XBCuqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install emoji\n",
        "# !pip install flair\n",
        "import emoji\n",
        "import pandas as pd\n",
        "import re\n",
        "import io\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from flair.data import Sentence\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "import spacy\n",
        "import pickle"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDCAp5SWin7j",
        "colab_type": "text"
      },
      "source": [
        "# Reading file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kAkMsYCCzP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_t = pd.read_csv('FINAL.csv',encoding= 'unicode_escape')\n",
        "df_t = df_t.loc[~(df_t['rumor_veracity']=='not_a_rumor')]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Na37eeit-R",
        "colab_type": "text"
      },
      "source": [
        "# Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8M0CN_gC4AR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocessing class\n",
        "class TextPreprocessor_new():\n",
        "    def __init__(self):\n",
        "        self.contractions = { \n",
        "        \"ain't\": \"am not\",\n",
        "        \"aren't\": \"are not\",\n",
        "        \"can't\": \"cannot\",\n",
        "        \"can't've\": \"cannot have\",\n",
        "        \"'cause\": \"because\",\n",
        "        \"could've\": \"could have\",\n",
        "        \"couldn't\": \"could not\",\n",
        "        \"couldn't've\": \"could not have\",\n",
        "        \"didn't\": \"did not\",\n",
        "        \"doesn't\": \"does not\",\n",
        "        \"don't\": \"do not\",\n",
        "        \"hadn't\": \"had not\",\n",
        "        \"hadn't've\": \"had not have\",\n",
        "        \"hasn't\": \"has not\",\n",
        "        \"haven't\": \"have not\",\n",
        "        \"he'd\": \"he would\",\n",
        "        \"he'd've\": \"he would have\",\n",
        "        \"he'll\": \"he will\",\n",
        "        \"he's\": \"he is\",\n",
        "        \"how'd\": \"how did\",\n",
        "        \"how'll\": \"how will\",\n",
        "        \"how's\": \"how is\",\n",
        "        \"i'd\": \"i would\",\n",
        "        \"i'll\": \"i will\",\n",
        "        \"i'm\": \"i am\",\n",
        "        \"i've\": \"i have\",\n",
        "        \"isn't\": \"is not\",\n",
        "        \"it'd\": \"it would\",\n",
        "        \"it'll\": \"it will\",\n",
        "        \"it's\": \"it is\",\n",
        "        \"let's\": \"let us\",\n",
        "        \"ma'am\": \"madam\",\n",
        "        \"mayn't\": \"may not\",\n",
        "        \"might've\": \"might have\",\n",
        "        \"mightn't\": \"might not\",\n",
        "        \"must've\": \"must have\",\n",
        "        \"mustn't\": \"must not\",\n",
        "        \"needn't\": \"need not\",\n",
        "        \"oughtn't\": \"ought not\",\n",
        "        \"shan't\": \"shall not\",\n",
        "        \"sha'n't\": \"shall not\",\n",
        "        \"she'd\": \"she would\",\n",
        "        \"she'll\": \"she will\",\n",
        "        \"she's\": \"she is\",\n",
        "        \"should've\": \"should have\",\n",
        "        \"shouldn't\": \"should not\",\n",
        "        \"that'd\": \"that would\",\n",
        "        \"that's\": \"that is\",\n",
        "        \"there'd\": \"there had\",\n",
        "        \"there's\": \"there is\",\n",
        "        \"they'd\": \"they would\",\n",
        "        \"they'll\": \"they will\",\n",
        "        \"they're\": \"they are\",\n",
        "        \"they've\": \"they have\",\n",
        "        \"wasn't\": \"was not\",\n",
        "        \"we'd\": \"we would\",\n",
        "        \"we'll\": \"we will\",\n",
        "        \"we're\": \"we are\",\n",
        "        \"we've\": \"we have\",\n",
        "        \"weren't\": \"were not\",\n",
        "        \"what'll\": \"what will\",\n",
        "        \"what're\": \"what are\",\n",
        "        \"what's\": \"what is\",\n",
        "        \"what've\": \"what have\",\n",
        "        \"where'd\": \"where did\",\n",
        "        \"where's\": \"where is\",\n",
        "        \"who'll\": \"who will\",\n",
        "        \"who's\": \"who is\",\n",
        "        \"won't\": \"will not\",\n",
        "        \"wouldn't\": \"would not\",\n",
        "        \"you'd\": \"you would\",\n",
        "        \"you'll\": \"you will\",\n",
        "        \"you're\": \"you are\",\n",
        "        \"thx\"   : \"thanks\",\n",
        "        \"ain’t\": \"am not\",\n",
        "        \"aren’t\": \"are not\",\n",
        "        \"can’t\": \"cannot\",\n",
        "        \"can’t’ve\": \"cannot have\",\n",
        "        \"could’ve\": \"could have\",\n",
        "        \"couldn’t\": \"could not\",\n",
        "        \"couldn’t’ve\": \"could not have\",\n",
        "        \"didn’t\": \"did not\",\n",
        "        \"doesn’t\": \"does not\",\n",
        "        \"don’t\": \"do not\",\n",
        "        \"hadn’t\": \"had not\",\n",
        "        \"hadn’t’ve\": \"had not have\",\n",
        "        \"hasn’t\": \"has not\",\n",
        "        \"haven’t\": \"have not\",\n",
        "        \"he’d\": \"he would\",\n",
        "        \"he’d’ve\": \"he would have\",\n",
        "        \"he’ll\": \"he will\",\n",
        "        \"he’s\": \"he is\",\n",
        "        \"how’d\": \"how did\",\n",
        "        \"how’ll\": \"how will\",\n",
        "        \"how’s\": \"how is\",\n",
        "        \"i’d\": \"i would\",\n",
        "        \"i’ll\": \"i will\",\n",
        "        \"i’m\": \"i am\",\n",
        "        \"i’ve\": \"i have\",\n",
        "        \"isn’t\": \"is not\",\n",
        "        \"it’d\": \"it would\",\n",
        "        \"it’ll\": \"it will\",\n",
        "        \"it’s\": \"it is\",\n",
        "        \"let’s\": \"let us\",\n",
        "        \"ma’am\": \"madam\",\n",
        "        \"mayn’t\": \"may not\",\n",
        "        \"might’ve\": \"might have\",\n",
        "        \"mightn’t\": \"might not\",\n",
        "        \"must’ve\": \"must have\",\n",
        "        \"mustn’t\": \"must not\",\n",
        "        \"needn’t\": \"need not\",\n",
        "        \"oughtn’t\": \"ought not\",\n",
        "        \"shan’t\": \"shall not\",\n",
        "        \"sha’n’t\": \"shall not\",\n",
        "        \"she’d\": \"she would\",\n",
        "        \"she’ll\": \"she will\",\n",
        "        \"she’s\": \"she is\",\n",
        "        \"should’ve\": \"should have\",\n",
        "        \"shouldn’t\": \"should not\",\n",
        "        \"that’d\": \"that would\",\n",
        "        \"that’s\": \"that is\",\n",
        "        \"there’d\": \"there had\",\n",
        "        \"there’s\": \"there is\",\n",
        "        \"they’d\": \"they would\",\n",
        "        \"they’ll\": \"they will\",\n",
        "        \"they’re\": \"they are\",\n",
        "        \"they’ve\": \"they have\",\n",
        "        \"wasn’t\": \"was not\",\n",
        "        \"we’d\": \"we would\",\n",
        "        \"we’ll\": \"we will\",\n",
        "        \"we’re\": \"we are\",\n",
        "        \"we’ve\": \"we have\",\n",
        "        \"weren’t\": \"were not\",\n",
        "        \"what’ll\": \"what will\",\n",
        "        \"what’re\": \"what are\",\n",
        "        \"what’s\": \"what is\",\n",
        "        \"what’ve\": \"what have\",\n",
        "        \"where’d\": \"where did\",\n",
        "        \"where’s\": \"where is\",\n",
        "        \"who’ll\": \"who will\",\n",
        "        \"who’s\": \"who is\",\n",
        "        \"won’t\": \"will not\",\n",
        "        \"wouldn’t\": \"would not\",\n",
        "        \"you’d\": \"you would\",\n",
        "        \"you’ll\": \"you will\",\n",
        "        \"you’re\": \"you are\"\n",
        "        }\n",
        "\n",
        "        self.emoticons_str = r\"\"\"\n",
        "            (?:\n",
        "                [:=;] # Eyes\n",
        "                [oO\\-]? # Nose (optional)\n",
        "                [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
        "            )\"\"\"\n",
        "\n",
        "        self.regex_str = [\n",
        "            self.emoticons_str,\n",
        "            r'<[^>]+>', # HTML tags\n",
        "            r'(?:@[\\w_]+)', # @-mentions\n",
        "            r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
        "            r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
        "\n",
        "            r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
        "        ]\n",
        "\n",
        "        self.tokens_re = re.compile(r'('+'|'.join(self.regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
        "        \n",
        "        self.spacy_nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner']) \n",
        "        self.spacy_nlp.add_pipe(self.remove_punctuations_emoji_symbol, name=\"stopwords\", last=True)\n",
        "    \n",
        "   \n",
        "    \n",
        "    def remove_punctuations_emoji_symbol(self,doc):\n",
        "\n",
        "        # This will remove stopwords and punctuation.\n",
        "        # Use token.text to return strings, which we'll need for Gensim.\n",
        "        doc = \" \".join([token.text.lower() for token in doc ])\n",
        "        doc = emoji.get_emoji_regexp().sub(r'', doc)  # get emoji free text\n",
        "        doc=\" \".join([self.contractions[token] if token in self.contractions.keys() else token for token in doc.split()]) # handle short words\n",
        "\n",
        "        doc = self.tokens_re.sub(r'', doc)   \n",
        "        doc = re.sub(r'\\s\\s+','', doc)\n",
        "        doc = re.sub(r'[ ]{2, }',' ',doc)\n",
        "\n",
        "        doc = re.sub(r'http\\S+','', doc)\n",
        "        doc = re.sub(r'@',' ',doc)\n",
        "        doc = re.sub(r'#',' ',doc)\n",
        "\n",
        "        return doc\n",
        "\n",
        "    def transform(self, X):  \n",
        "        \n",
        "        \n",
        "        X_new = self.spacy_nlp(X)\n",
        "        return  X_new"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLs_pRoEANmZ",
        "colab_type": "text"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYJ8ZRpgC4wy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = 'text'\n",
        "TextPreprocessor_new_obj = TextPreprocessor_new()\n",
        "df_t[x] = df_t[x].apply(lambda x:TextPreprocessor_new_obj.transform(x) )"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZX2M8r11_4m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "49c9dc5e-06e8-4e22-8654-d354fb385f48"
      },
      "source": [
        "df_t.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(181, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dqBUGIpj1OT",
        "colab_type": "text"
      },
      "source": [
        "# Train , Test and Dev split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsEMP8OcVG5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = df_t[0:110]\n",
        "dev = df_t[110:130]\n",
        "test = df_t[130:]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcurFH3iV9nf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.to_csv(\"./data/train.csv\",index=False)\n",
        "dev.to_csv(\"./data/dev.csv\",index=False)\n",
        "test.to_csv(\"./data/test.csv\",index=False)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XjpoDpyYOpV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "ceef47ea-4dc9-4cb4-c770-f46dcc864f3b"
      },
      "source": [
        "train.columns"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['created_at', 'text', 'source', 'is_quote', 'favourites_count',\n",
              "       'retweet_count', 'followers_count', 'friends_count',\n",
              "       'account_created_at', 'verified', 'is_rumor', 'rumor_veracity'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI0a0jelAoa_",
        "colab_type": "text"
      },
      "source": [
        "# Rumour Veracity - Multiple classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnr0t14cj9Aq",
        "colab_type": "text"
      },
      "source": [
        "## Step1: Preparing corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4Q3ULlvVHfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import CSVClassificationCorpus"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZm364jDYQWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "52642958-9ced-4038-b4a7-a8c691a68ecf"
      },
      "source": [
        "data_folder = '/content/data'\n",
        "# column format indicating which columns hold the text and label(s)\n",
        "column_name_map = {1: \"text\", 11: \"label_topic\"}\n",
        "\n",
        "# load corpus containing training, test and dev data and if CSV has a header, you can skip it\n",
        "corpus: Corpus = CSVClassificationCorpus(data_folder,\n",
        "                                         column_name_map,\n",
        "                                         skip_header=True,\n",
        "                                         delimiter=',',    # tab-separated files\n",
        ") "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-28 09:28:25,001 Reading data from /content/data\n",
            "2020-06-28 09:28:25,003 Train: /content/data/train.csv\n",
            "2020-06-28 09:28:25,004 Dev: /content/data/dev.csv\n",
            "2020-06-28 09:28:25,004 Test: /content/data/test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otiOLMgTkFDs",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s89OX7XbmjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim.adam import Adam\n",
        "\n",
        "from flair.data import Corpus\n",
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "from flair.models import TextClassifier\n",
        "from flair.trainers import ModelTrainer"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtlsd5Tkb0AY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "6465226a-a052-473b-f9a8-6ce2a87e01aa"
      },
      "source": [
        "# 2. create the label dictionary\n",
        "label_dict = corpus.make_label_dictionary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-28 09:28:30,937 Computing label dictionary. Progress:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 161/161 [00:00<00:00, 306.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-06-28 09:28:31,842 [b'Unverified', b'TRUE', b'FALSE']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IbcgF-RcIUs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "868f1b16-110c-4518-ba7d-92def505161d"
      },
      "source": [
        "\n",
        "# 3. initialize transformer document embeddings (many models are available)\n",
        "document_embeddings = TransformerDocumentEmbeddings('distilbert-base-uncased', fine_tune=True)\n",
        "\n",
        "# 4. create the text classifier\n",
        "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n",
        "\n",
        "# 5. initialize the text classifier trainer with Adam optimizer\n",
        "trainer = ModelTrainer(classifier, corpus, optimizer=Adam)\n",
        "\n",
        "# 6. start the training\n",
        "trainer.train('resources/MultiClassification',\n",
        "              learning_rate=3e-5, # use very small learning rate\n",
        "              mini_batch_size=16,\n",
        "              mini_batch_chunk_size=4, # optionally set this if transformer is too much for your machine\n",
        "              max_epochs=5, # terminate after 5 epochs\n",
        "              )"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-28 09:28:36,805 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 09:28:36,808 Model: \"TextClassifier(\n",
            "  (document_embeddings): TransformerDocumentEmbeddings(\n",
            "    (model): DistilBertModel(\n",
            "      (embeddings): Embeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (transformer): Transformer(\n",
            "        (layer): ModuleList(\n",
            "          (0): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (1): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (2): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (3): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (4): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (5): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): Linear(in_features=768, out_features=3, bias=True)\n",
            "  (loss_function): CrossEntropyLoss()\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2020-06-28 09:28:36,810 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 09:28:36,812 Corpus: \"Corpus: 110 train + 20 dev + 51 test sentences\"\n",
            "2020-06-28 09:28:36,814 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 09:28:36,815 Parameters:\n",
            "2020-06-28 09:28:36,819  - learning_rate: \"3e-05\"\n",
            "2020-06-28 09:28:36,822  - mini_batch_size: \"16\"\n",
            "2020-06-28 09:28:36,825  - patience: \"3\"\n",
            "2020-06-28 09:28:36,826  - anneal_factor: \"0.5\"\n",
            "2020-06-28 09:28:36,827  - max_epochs: \"5\"\n",
            "2020-06-28 09:28:36,828  - shuffle: \"True\"\n",
            "2020-06-28 09:28:36,829  - train_with_dev: \"False\"\n",
            "2020-06-28 09:28:36,831  - batch_growth_annealing: \"False\"\n",
            "2020-06-28 09:28:36,833 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 09:28:36,834 Model training base path: \"resources/MultiClassification\"\n",
            "2020-06-28 09:28:36,835 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 09:28:36,837 Device: cpu\n",
            "2020-06-28 09:28:36,839 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 09:28:36,840 Embeddings storage mode: cpu\n",
            "2020-06-28 09:28:36,846 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 09:28:46,313 epoch 1 - iter 1/7 - loss 0.84182709 - samples/sec: 1.74\n",
            "2020-06-28 09:28:55,259 epoch 1 - iter 2/7 - loss 0.84700838 - samples/sec: 1.80\n",
            "2020-06-28 09:29:01,871 epoch 1 - iter 3/7 - loss 0.88679610 - samples/sec: 2.44\n",
            "2020-06-28 09:29:07,802 epoch 1 - iter 4/7 - loss 0.86560108 - samples/sec: 2.72\n",
            "2020-06-28 09:29:13,421 epoch 1 - iter 5/7 - loss 0.87616569 - samples/sec: 2.87\n",
            "2020-06-28 09:29:20,016 epoch 1 - iter 6/7 - loss 0.93425835 - samples/sec: 2.44\n",
            "2020-06-28 09:29:25,628 epoch 1 - iter 7/7 - loss 1.04232649 - samples/sec: 2.87\n",
            "2020-06-28 09:29:25,840 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 09:29:25,841 EPOCH 1 done: loss 1.0423 - lr 0.0000300\n",
            "2020-06-28 09:29:27,278 DEV : loss 0.7435272932052612 - score 0.7333\n",
            "2020-06-28 09:29:27,294 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-06-28 09:29:28,090 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 09:29:36,499 epoch 2 - iter 1/7 - loss 0.56168681 - samples/sec: 1.97\n",
            "2020-06-28 09:29:42,439 epoch 2 - iter 2/7 - loss 0.50738013 - samples/sec: 2.72\n",
            "2020-06-28 09:29:49,037 epoch 2 - iter 3/7 - loss 0.69802193 - samples/sec: 2.44\n",
            "2020-06-28 09:29:55,091 epoch 2 - iter 4/7 - loss 0.64636552 - samples/sec: 2.66\n",
            "2020-06-28 09:30:01,244 epoch 2 - iter 5/7 - loss 0.65788420 - samples/sec: 2.62\n",
            "2020-06-28 09:30:07,247 epoch 2 - iter 6/7 - loss 0.74080784 - samples/sec: 2.68\n",
            "2020-06-28 09:30:12,488 epoch 2 - iter 7/7 - loss 0.72452290 - samples/sec: 3.08\n",
            "2020-06-28 09:30:12,735 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 09:30:12,736 EPOCH 2 done: loss 0.7245 - lr 0.0000300\n",
            "2020-06-28 09:30:14,140 DEV : loss 0.7729012370109558 - score 0.7333\n",
            "2020-06-28 09:30:14,156 BAD EPOCHS (no improvement): 1\n",
            "2020-06-28 09:30:14,158 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 09:30:21,943 epoch 3 - iter 1/7 - loss 0.73502719 - samples/sec: 2.15\n",
            "2020-06-28 09:30:27,882 epoch 3 - iter 2/7 - loss 0.74966735 - samples/sec: 2.71\n",
            "2020-06-28 09:30:34,475 epoch 3 - iter 3/7 - loss 0.73148578 - samples/sec: 2.44\n",
            "2020-06-28 09:30:40,661 epoch 3 - iter 4/7 - loss 0.75025199 - samples/sec: 2.60\n",
            "2020-06-28 09:30:46,615 epoch 3 - iter 5/7 - loss 0.65932066 - samples/sec: 2.71\n",
            "2020-06-28 09:30:53,390 epoch 3 - iter 6/7 - loss 0.72046359 - samples/sec: 2.38\n",
            "2020-06-28 09:30:58,338 epoch 3 - iter 7/7 - loss 0.68569781 - samples/sec: 3.26\n",
            "2020-06-28 09:30:58,582 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 09:30:58,583 EPOCH 3 done: loss 0.6857 - lr 0.0000300\n",
            "2020-06-28 09:31:00,018 DEV : loss 0.9417883157730103 - score 0.7\n",
            "2020-06-28 09:31:00,030 BAD EPOCHS (no improvement): 2\n",
            "2020-06-28 09:31:00,032 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 09:31:07,792 epoch 4 - iter 1/7 - loss 0.09862380 - samples/sec: 2.16\n",
            "2020-06-28 09:31:13,804 epoch 4 - iter 2/7 - loss 0.13816934 - samples/sec: 2.68\n",
            "2020-06-28 09:31:19,667 epoch 4 - iter 3/7 - loss 0.23380321 - samples/sec: 2.75\n",
            "2020-06-28 09:31:25,578 epoch 4 - iter 4/7 - loss 0.23099969 - samples/sec: 2.73\n",
            "2020-06-28 09:31:31,655 epoch 4 - iter 5/7 - loss 0.21321741 - samples/sec: 2.65\n",
            "2020-06-28 09:31:37,896 epoch 4 - iter 6/7 - loss 0.29018099 - samples/sec: 2.58\n",
            "2020-06-28 09:31:43,742 epoch 4 - iter 7/7 - loss 0.30986869 - samples/sec: 2.75\n",
            "2020-06-28 09:31:43,985 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 09:31:43,986 EPOCH 4 done: loss 0.3099 - lr 0.0000300\n",
            "2020-06-28 09:31:45,438 DEV : loss 0.9703011512756348 - score 0.7\n",
            "2020-06-28 09:31:45,463 BAD EPOCHS (no improvement): 3\n",
            "2020-06-28 09:31:45,465 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 09:31:53,554 epoch 5 - iter 1/7 - loss 0.10865311 - samples/sec: 2.05\n",
            "2020-06-28 09:31:59,806 epoch 5 - iter 2/7 - loss 0.10457525 - samples/sec: 2.58\n",
            "2020-06-28 09:32:05,713 epoch 5 - iter 3/7 - loss 0.09530226 - samples/sec: 2.73\n",
            "2020-06-28 09:32:12,094 epoch 5 - iter 4/7 - loss 0.10877664 - samples/sec: 2.53\n",
            "2020-06-28 09:32:18,134 epoch 5 - iter 5/7 - loss 0.09514481 - samples/sec: 2.67\n",
            "2020-06-28 09:32:23,868 epoch 5 - iter 6/7 - loss 0.08707142 - samples/sec: 2.81\n",
            "2020-06-28 09:32:29,218 epoch 5 - iter 7/7 - loss 0.12815157 - samples/sec: 3.01\n",
            "2020-06-28 09:32:29,458 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 09:32:29,459 EPOCH 5 done: loss 0.1282 - lr 0.0000300\n",
            "2020-06-28 09:32:30,949 DEV : loss 1.0010870695114136 - score 0.7333\n",
            "Epoch     5: reducing learning rate of group 0 to 1.5000e-05.\n",
            "2020-06-28 09:32:30,965 BAD EPOCHS (no improvement): 4\n",
            "2020-06-28 09:32:31,755 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 09:32:31,761 Testing using best model ...\n",
            "2020-06-28 09:32:31,768 loading file resources/MultiClassification/best-model.pt\n",
            "2020-06-28 09:32:35,372 0.7254901960784313\t0.7254901960784313\t0.7254901960784313\n",
            "2020-06-28 09:32:35,374 \n",
            "MICRO_AVG: acc 0.8169934640522876 - f1-score 0.7254901960784313\n",
            "MACRO_AVG: acc 0.8169934640522875 - f1-score 0.33967582804792107\n",
            "FALSE      tp: 36 - fp: 13 - fn: 1 - tn: 1 - precision: 0.7347 - recall: 0.9730 - accuracy: 0.7255 - f1-score: 0.8372\n",
            "TRUE       tp: 1 - fp: 1 - fn: 8 - tn: 41 - precision: 0.5000 - recall: 0.1111 - accuracy: 0.8235 - f1-score: 0.1818\n",
            "Unverified tp: 0 - fp: 0 - fn: 5 - tn: 46 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9020 - f1-score: 0.0000\n",
            "2020-06-28 09:32:35,375 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dev_loss_history': [0.7435272932052612,\n",
              "  0.7729012370109558,\n",
              "  0.9417883157730103,\n",
              "  0.9703011512756348,\n",
              "  1.0010870695114136],\n",
              " 'dev_score_history': [0.7333333333333333,\n",
              "  0.7333333333333333,\n",
              "  0.7,\n",
              "  0.7,\n",
              "  0.7333333333333333],\n",
              " 'test_score': 0.8169934640522876,\n",
              " 'train_loss_history': [1.0423264929226466,\n",
              "  0.7245229014328548,\n",
              "  0.6856978109904698,\n",
              "  0.3098686912230083,\n",
              "  0.12815156898328237]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEbnyTFatG0W",
        "colab_type": "text"
      },
      "source": [
        "## Step 3:  Prediction on test data set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAsc_zNasW2v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ab5f21d6-ac06-4dcc-86a4-e0589eab8fae"
      },
      "source": [
        "predicted=[]\n",
        "classifier = TextClassifier.load('resources/MultiClassification/final-model.pt')\n",
        "for i,row in test.iterrows():\n",
        "  sent = Sentence(row[\"text\"])\n",
        "  classifier.predict(sent)\n",
        "  predicted.append(str(sent.labels[0]).split(\"(\")[0].replace(\" \",\"\"))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-28 09:32:39,388 loading file resources/MultiClassification/final-model.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuhuQXdhBGuy",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSfg45jFsKQ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "a6945a35-4928-422e-9690-6705daf5adee"
      },
      "source": [
        "print(metrics.classification_report(test['rumor_veracity'].values, predicted))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       FALSE       0.73      0.89      0.80        37\n",
            "        TRUE       0.33      0.22      0.27         9\n",
            "  Unverified       0.00      0.00      0.00         5\n",
            "\n",
            "    accuracy                           0.69        51\n",
            "   macro avg       0.36      0.37      0.36        51\n",
            "weighted avg       0.59      0.69      0.63        51\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6dUx2bOt19l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    }
  ]
}