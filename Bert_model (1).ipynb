{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spKnTVVsiZNT",
        "colab_type": "text"
      },
      "source": [
        "# Importing packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWXR27XBCuqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install emoji\n",
        "# !pip install flair\n",
        "import emoji\n",
        "import pandas as pd\n",
        "import re\n",
        "import io\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from flair.data import Sentence\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "import spacy\n",
        "import pickle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDCAp5SWin7j",
        "colab_type": "text"
      },
      "source": [
        "# Reading file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kAkMsYCCzP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_t = pd.read_csv('FINAL.csv',encoding= 'unicode_escape')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Na37eeit-R",
        "colab_type": "text"
      },
      "source": [
        "# Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8M0CN_gC4AR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocessing class\n",
        "class TextPreprocessor_new():\n",
        "    def __init__(self):\n",
        "        self.contractions = { \n",
        "        \"ain't\": \"am not\",\n",
        "        \"aren't\": \"are not\",\n",
        "        \"can't\": \"cannot\",\n",
        "        \"can't've\": \"cannot have\",\n",
        "        \"'cause\": \"because\",\n",
        "        \"could've\": \"could have\",\n",
        "        \"couldn't\": \"could not\",\n",
        "        \"couldn't've\": \"could not have\",\n",
        "        \"didn't\": \"did not\",\n",
        "        \"doesn't\": \"does not\",\n",
        "        \"don't\": \"do not\",\n",
        "        \"hadn't\": \"had not\",\n",
        "        \"hadn't've\": \"had not have\",\n",
        "        \"hasn't\": \"has not\",\n",
        "        \"haven't\": \"have not\",\n",
        "        \"he'd\": \"he would\",\n",
        "        \"he'd've\": \"he would have\",\n",
        "        \"he'll\": \"he will\",\n",
        "        \"he's\": \"he is\",\n",
        "        \"how'd\": \"how did\",\n",
        "        \"how'll\": \"how will\",\n",
        "        \"how's\": \"how is\",\n",
        "        \"i'd\": \"i would\",\n",
        "        \"i'll\": \"i will\",\n",
        "        \"i'm\": \"i am\",\n",
        "        \"i've\": \"i have\",\n",
        "        \"isn't\": \"is not\",\n",
        "        \"it'd\": \"it would\",\n",
        "        \"it'll\": \"it will\",\n",
        "        \"it's\": \"it is\",\n",
        "        \"let's\": \"let us\",\n",
        "        \"ma'am\": \"madam\",\n",
        "        \"mayn't\": \"may not\",\n",
        "        \"might've\": \"might have\",\n",
        "        \"mightn't\": \"might not\",\n",
        "        \"must've\": \"must have\",\n",
        "        \"mustn't\": \"must not\",\n",
        "        \"needn't\": \"need not\",\n",
        "        \"oughtn't\": \"ought not\",\n",
        "        \"shan't\": \"shall not\",\n",
        "        \"sha'n't\": \"shall not\",\n",
        "        \"she'd\": \"she would\",\n",
        "        \"she'll\": \"she will\",\n",
        "        \"she's\": \"she is\",\n",
        "        \"should've\": \"should have\",\n",
        "        \"shouldn't\": \"should not\",\n",
        "        \"that'd\": \"that would\",\n",
        "        \"that's\": \"that is\",\n",
        "        \"there'd\": \"there had\",\n",
        "        \"there's\": \"there is\",\n",
        "        \"they'd\": \"they would\",\n",
        "        \"they'll\": \"they will\",\n",
        "        \"they're\": \"they are\",\n",
        "        \"they've\": \"they have\",\n",
        "        \"wasn't\": \"was not\",\n",
        "        \"we'd\": \"we would\",\n",
        "        \"we'll\": \"we will\",\n",
        "        \"we're\": \"we are\",\n",
        "        \"we've\": \"we have\",\n",
        "        \"weren't\": \"were not\",\n",
        "        \"what'll\": \"what will\",\n",
        "        \"what're\": \"what are\",\n",
        "        \"what's\": \"what is\",\n",
        "        \"what've\": \"what have\",\n",
        "        \"where'd\": \"where did\",\n",
        "        \"where's\": \"where is\",\n",
        "        \"who'll\": \"who will\",\n",
        "        \"who's\": \"who is\",\n",
        "        \"won't\": \"will not\",\n",
        "        \"wouldn't\": \"would not\",\n",
        "        \"you'd\": \"you would\",\n",
        "        \"you'll\": \"you will\",\n",
        "        \"you're\": \"you are\",\n",
        "        \"thx\"   : \"thanks\",\n",
        "        \"ain’t\": \"am not\",\n",
        "        \"aren’t\": \"are not\",\n",
        "        \"can’t\": \"cannot\",\n",
        "        \"can’t’ve\": \"cannot have\",\n",
        "        \"could’ve\": \"could have\",\n",
        "        \"couldn’t\": \"could not\",\n",
        "        \"couldn’t’ve\": \"could not have\",\n",
        "        \"didn’t\": \"did not\",\n",
        "        \"doesn’t\": \"does not\",\n",
        "        \"don’t\": \"do not\",\n",
        "        \"hadn’t\": \"had not\",\n",
        "        \"hadn’t’ve\": \"had not have\",\n",
        "        \"hasn’t\": \"has not\",\n",
        "        \"haven’t\": \"have not\",\n",
        "        \"he’d\": \"he would\",\n",
        "        \"he’d’ve\": \"he would have\",\n",
        "        \"he’ll\": \"he will\",\n",
        "        \"he’s\": \"he is\",\n",
        "        \"how’d\": \"how did\",\n",
        "        \"how’ll\": \"how will\",\n",
        "        \"how’s\": \"how is\",\n",
        "        \"i’d\": \"i would\",\n",
        "        \"i’ll\": \"i will\",\n",
        "        \"i’m\": \"i am\",\n",
        "        \"i’ve\": \"i have\",\n",
        "        \"isn’t\": \"is not\",\n",
        "        \"it’d\": \"it would\",\n",
        "        \"it’ll\": \"it will\",\n",
        "        \"it’s\": \"it is\",\n",
        "        \"let’s\": \"let us\",\n",
        "        \"ma’am\": \"madam\",\n",
        "        \"mayn’t\": \"may not\",\n",
        "        \"might’ve\": \"might have\",\n",
        "        \"mightn’t\": \"might not\",\n",
        "        \"must’ve\": \"must have\",\n",
        "        \"mustn’t\": \"must not\",\n",
        "        \"needn’t\": \"need not\",\n",
        "        \"oughtn’t\": \"ought not\",\n",
        "        \"shan’t\": \"shall not\",\n",
        "        \"sha’n’t\": \"shall not\",\n",
        "        \"she’d\": \"she would\",\n",
        "        \"she’ll\": \"she will\",\n",
        "        \"she’s\": \"she is\",\n",
        "        \"should’ve\": \"should have\",\n",
        "        \"shouldn’t\": \"should not\",\n",
        "        \"that’d\": \"that would\",\n",
        "        \"that’s\": \"that is\",\n",
        "        \"there’d\": \"there had\",\n",
        "        \"there’s\": \"there is\",\n",
        "        \"they’d\": \"they would\",\n",
        "        \"they’ll\": \"they will\",\n",
        "        \"they’re\": \"they are\",\n",
        "        \"they’ve\": \"they have\",\n",
        "        \"wasn’t\": \"was not\",\n",
        "        \"we’d\": \"we would\",\n",
        "        \"we’ll\": \"we will\",\n",
        "        \"we’re\": \"we are\",\n",
        "        \"we’ve\": \"we have\",\n",
        "        \"weren’t\": \"were not\",\n",
        "        \"what’ll\": \"what will\",\n",
        "        \"what’re\": \"what are\",\n",
        "        \"what’s\": \"what is\",\n",
        "        \"what’ve\": \"what have\",\n",
        "        \"where’d\": \"where did\",\n",
        "        \"where’s\": \"where is\",\n",
        "        \"who’ll\": \"who will\",\n",
        "        \"who’s\": \"who is\",\n",
        "        \"won’t\": \"will not\",\n",
        "        \"wouldn’t\": \"would not\",\n",
        "        \"you’d\": \"you would\",\n",
        "        \"you’ll\": \"you will\",\n",
        "        \"you’re\": \"you are\"\n",
        "        }\n",
        "\n",
        "        self.emoticons_str = r\"\"\"\n",
        "            (?:\n",
        "                [:=;] # Eyes\n",
        "                [oO\\-]? # Nose (optional)\n",
        "                [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
        "            )\"\"\"\n",
        "\n",
        "        self.regex_str = [\n",
        "            self.emoticons_str,\n",
        "            r'<[^>]+>', # HTML tags\n",
        "            r'(?:@[\\w_]+)', # @-mentions\n",
        "            r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
        "            r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
        "\n",
        "            r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
        "        ]\n",
        "\n",
        "        self.tokens_re = re.compile(r'('+'|'.join(self.regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
        "        \n",
        "        self.spacy_nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner']) \n",
        "        self.spacy_nlp.add_pipe(self.remove_punctuations_emoji_symbol, name=\"stopwords\", last=True)\n",
        "    \n",
        "   \n",
        "    \n",
        "    def remove_punctuations_emoji_symbol(self,doc):\n",
        "\n",
        "        # This will remove stopwords and punctuation.\n",
        "        # Use token.text to return strings, which we'll need for Gensim.\n",
        "        doc = \" \".join([token.text.lower() for token in doc ])\n",
        "        doc = emoji.get_emoji_regexp().sub(r'', doc)  # get emoji free text\n",
        "        doc=\" \".join([self.contractions[token] if token in self.contractions.keys() else token for token in doc.split()]) # handle short words\n",
        "\n",
        "        doc = self.tokens_re.sub(r'', doc)   \n",
        "        doc = re.sub(r'\\s\\s+','', doc)\n",
        "        doc = re.sub(r'[ ]{2, }',' ',doc)\n",
        "\n",
        "        doc = re.sub(r'http\\S+','', doc)\n",
        "        doc = re.sub(r'@',' ',doc)\n",
        "        doc = re.sub(r'#',' ',doc)\n",
        "\n",
        "        return doc\n",
        "\n",
        "    def transform(self, X):  \n",
        "        \n",
        "        \n",
        "        X_new = self.spacy_nlp(X)\n",
        "        return  X_new"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLs_pRoEANmZ",
        "colab_type": "text"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYJ8ZRpgC4wy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = 'text'\n",
        "TextPreprocessor_new_obj = TextPreprocessor_new()\n",
        "df_t[x] = df_t[x].apply(lambda x:TextPreprocessor_new_obj.transform(x) )"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZX2M8r11_4m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3b6d2b36-4a99-47ff-b93b-f0a900477805"
      },
      "source": [
        "df_t.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(416, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dqBUGIpj1OT",
        "colab_type": "text"
      },
      "source": [
        "# Train , Test and Dev split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsEMP8OcVG5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = df_t[0:320]\n",
        "dev = df_t[320:350]\n",
        "test = df_t[350:]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcurFH3iV9nf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.to_csv(\"./data/train.csv\",index=False)\n",
        "dev.to_csv(\"./data/dev.csv\",index=False)\n",
        "test.to_csv(\"./data/test.csv\",index=False)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XjpoDpyYOpV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "ceef47ea-4dc9-4cb4-c770-f46dcc864f3b"
      },
      "source": [
        "train.columns"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['created_at', 'text', 'source', 'is_quote', 'favourites_count',\n",
              "       'retweet_count', 'followers_count', 'friends_count',\n",
              "       'account_created_at', 'verified', 'is_rumor', 'rumor_veracity'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI0a0jelAoa_",
        "colab_type": "text"
      },
      "source": [
        "# Rumour Veracity - Multiple classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnr0t14cj9Aq",
        "colab_type": "text"
      },
      "source": [
        "## Step1: Preparing corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4Q3ULlvVHfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import CSVClassificationCorpus"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZm364jDYQWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "ff0e1bc4-1484-4e55-8c8e-d20a4a4915f6"
      },
      "source": [
        "data_folder = '/content/data'\n",
        "# column format indicating which columns hold the text and label(s)\n",
        "column_name_map = {1: \"text\", 11: \"label_topic\"}\n",
        "\n",
        "# load corpus containing training, test and dev data and if CSV has a header, you can skip it\n",
        "corpus: Corpus = CSVClassificationCorpus(data_folder,\n",
        "                                         column_name_map,\n",
        "                                         skip_header=True,\n",
        "                                         delimiter=',',    # tab-separated files\n",
        ") "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-28 04:15:46,767 Reading data from /content/data\n",
            "2020-06-28 04:15:46,768 Train: /content/data/train.csv\n",
            "2020-06-28 04:15:46,769 Dev: /content/data/dev.csv\n",
            "2020-06-28 04:15:46,772 Test: /content/data/test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otiOLMgTkFDs",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s89OX7XbmjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim.adam import Adam\n",
        "\n",
        "from flair.data import Corpus\n",
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "from flair.models import TextClassifier\n",
        "from flair.trainers import ModelTrainer"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtlsd5Tkb0AY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "65c693af-bde2-460c-dd60-e6704ed5b8e7"
      },
      "source": [
        "# 2. create the label dictionary\n",
        "label_dict = corpus.make_label_dictionary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-28 04:15:46,803 Computing label dictionary. Progress:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 386/386 [00:00<00:00, 690.94it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-06-28 04:15:47,666 [b'not_a_rumor', b'Unverified', b'TRUE', b'FALSE']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IbcgF-RcIUs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e85f26d6-d3e0-4f46-e40e-ff049bac4831"
      },
      "source": [
        "\n",
        "# 3. initialize transformer document embeddings (many models are available)\n",
        "document_embeddings = TransformerDocumentEmbeddings('distilbert-base-uncased', fine_tune=True)\n",
        "\n",
        "# 4. create the text classifier\n",
        "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n",
        "\n",
        "# 5. initialize the text classifier trainer with Adam optimizer\n",
        "trainer = ModelTrainer(classifier, corpus, optimizer=Adam)\n",
        "\n",
        "# 6. start the training\n",
        "trainer.train('resources/MultiClassification',\n",
        "              learning_rate=3e-5, # use very small learning rate\n",
        "              mini_batch_size=16,\n",
        "              mini_batch_chunk_size=4, # optionally set this if transformer is too much for your machine\n",
        "              max_epochs=5, # terminate after 5 epochs\n",
        "              )"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-28 04:15:49,556 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 04:15:49,558 Model: \"TextClassifier(\n",
            "  (document_embeddings): TransformerDocumentEmbeddings(\n",
            "    (model): DistilBertModel(\n",
            "      (embeddings): Embeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (transformer): Transformer(\n",
            "        (layer): ModuleList(\n",
            "          (0): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (1): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (2): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (3): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (4): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (5): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): Linear(in_features=768, out_features=4, bias=True)\n",
            "  (loss_function): CrossEntropyLoss()\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2020-06-28 04:15:49,559 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 04:15:49,560 Corpus: \"Corpus: 320 train + 30 dev + 66 test sentences\"\n",
            "2020-06-28 04:15:49,562 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 04:15:49,564 Parameters:\n",
            "2020-06-28 04:15:49,564  - learning_rate: \"3e-05\"\n",
            "2020-06-28 04:15:49,566  - mini_batch_size: \"16\"\n",
            "2020-06-28 04:15:49,573  - patience: \"3\"\n",
            "2020-06-28 04:15:49,574  - anneal_factor: \"0.5\"\n",
            "2020-06-28 04:15:49,575  - max_epochs: \"5\"\n",
            "2020-06-28 04:15:49,583  - shuffle: \"True\"\n",
            "2020-06-28 04:15:49,584  - train_with_dev: \"False\"\n",
            "2020-06-28 04:15:49,586  - batch_growth_annealing: \"False\"\n",
            "2020-06-28 04:15:49,587 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 04:15:49,588 Model training base path: \"resources/MultiClassification\"\n",
            "2020-06-28 04:15:49,589 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 04:15:49,590 Device: cpu\n",
            "2020-06-28 04:15:49,591 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 04:15:49,592 Embeddings storage mode: cpu\n",
            "2020-06-28 04:15:49,595 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 04:16:05,416 epoch 1 - iter 2/20 - loss 1.44055176 - samples/sec: 2.06\n",
            "2020-06-28 04:16:20,212 epoch 1 - iter 4/20 - loss 0.90589710 - samples/sec: 2.18\n",
            "2020-06-28 04:16:34,447 epoch 1 - iter 6/20 - loss 1.06594785 - samples/sec: 2.26\n",
            "2020-06-28 04:16:49,239 epoch 1 - iter 8/20 - loss 1.06756032 - samples/sec: 2.17\n",
            "2020-06-28 04:17:03,283 epoch 1 - iter 10/20 - loss 0.96067127 - samples/sec: 2.29\n",
            "2020-06-28 04:17:17,833 epoch 1 - iter 12/20 - loss 0.92226867 - samples/sec: 2.21\n",
            "2020-06-28 04:17:31,898 epoch 1 - iter 14/20 - loss 0.92015610 - samples/sec: 2.28\n",
            "2020-06-28 04:17:46,627 epoch 1 - iter 16/20 - loss 0.97564041 - samples/sec: 2.18\n",
            "2020-06-28 04:18:01,060 epoch 1 - iter 18/20 - loss 0.95053692 - samples/sec: 2.23\n",
            "2020-06-28 04:18:14,816 epoch 1 - iter 20/20 - loss 0.94368230 - samples/sec: 2.33\n",
            "2020-06-28 04:18:15,026 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 04:18:15,027 EPOCH 1 done: loss 0.9437 - lr 0.0000300\n",
            "2020-06-28 04:18:16,951 DEV : loss 1.880383014678955 - score 0.5\n",
            "2020-06-28 04:18:16,972 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-06-28 04:18:17,429 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 04:18:34,586 epoch 2 - iter 2/20 - loss 0.97234967 - samples/sec: 1.90\n",
            "2020-06-28 04:18:48,814 epoch 2 - iter 4/20 - loss 0.80861662 - samples/sec: 2.26\n",
            "2020-06-28 04:19:05,723 epoch 2 - iter 6/20 - loss 0.73220146 - samples/sec: 2.01\n",
            "2020-06-28 04:19:19,916 epoch 2 - iter 8/20 - loss 0.60388900 - samples/sec: 2.26\n",
            "2020-06-28 04:19:34,582 epoch 2 - iter 10/20 - loss 0.50972317 - samples/sec: 2.19\n",
            "2020-06-28 04:19:48,663 epoch 2 - iter 12/20 - loss 0.48131431 - samples/sec: 2.28\n",
            "2020-06-28 04:20:04,539 epoch 2 - iter 14/20 - loss 0.57074399 - samples/sec: 2.02\n",
            "2020-06-28 04:20:18,895 epoch 2 - iter 16/20 - loss 0.55882507 - samples/sec: 2.24\n",
            "2020-06-28 04:20:32,543 epoch 2 - iter 18/20 - loss 0.52529307 - samples/sec: 2.35\n",
            "2020-06-28 04:20:46,853 epoch 2 - iter 20/20 - loss 0.51116630 - samples/sec: 2.24\n",
            "2020-06-28 04:20:47,090 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 04:20:47,091 EPOCH 2 done: loss 0.5112 - lr 0.0000300\n",
            "2020-06-28 04:20:49,034 DEV : loss 1.1316317319869995 - score 0.85\n",
            "2020-06-28 04:20:49,052 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-06-28 04:20:49,580 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 04:21:06,256 epoch 3 - iter 2/20 - loss 0.68385841 - samples/sec: 1.95\n",
            "2020-06-28 04:21:19,558 epoch 3 - iter 4/20 - loss 0.42869387 - samples/sec: 2.42\n",
            "2020-06-28 04:21:33,666 epoch 3 - iter 6/20 - loss 0.43460784 - samples/sec: 2.28\n",
            "2020-06-28 04:21:48,815 epoch 3 - iter 8/20 - loss 0.36892326 - samples/sec: 2.12\n",
            "2020-06-28 04:22:02,630 epoch 3 - iter 10/20 - loss 0.37186112 - samples/sec: 2.32\n",
            "2020-06-28 04:22:17,454 epoch 3 - iter 12/20 - loss 0.32403449 - samples/sec: 2.17\n",
            "2020-06-28 04:22:31,099 epoch 3 - iter 14/20 - loss 0.38650675 - samples/sec: 2.35\n",
            "2020-06-28 04:22:45,011 epoch 3 - iter 16/20 - loss 0.34778278 - samples/sec: 2.31\n",
            "2020-06-28 04:22:59,175 epoch 3 - iter 18/20 - loss 0.32187818 - samples/sec: 2.27\n",
            "2020-06-28 04:23:13,456 epoch 3 - iter 20/20 - loss 0.31757180 - samples/sec: 2.25\n",
            "2020-06-28 04:23:13,691 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 04:23:13,692 EPOCH 3 done: loss 0.3176 - lr 0.0000300\n",
            "2020-06-28 04:23:15,628 DEV : loss 1.3749545812606812 - score 0.7667\n",
            "2020-06-28 04:23:15,646 BAD EPOCHS (no improvement): 1\n",
            "2020-06-28 04:23:15,647 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 04:23:30,518 epoch 4 - iter 2/20 - loss 0.47402607 - samples/sec: 2.19\n",
            "2020-06-28 04:23:44,799 epoch 4 - iter 4/20 - loss 0.30961064 - samples/sec: 2.25\n",
            "2020-06-28 04:24:00,443 epoch 4 - iter 6/20 - loss 0.24875919 - samples/sec: 2.05\n",
            "2020-06-28 04:24:14,642 epoch 4 - iter 8/20 - loss 0.21194592 - samples/sec: 2.26\n",
            "2020-06-28 04:24:29,626 epoch 4 - iter 10/20 - loss 0.17980422 - samples/sec: 2.14\n",
            "2020-06-28 04:24:43,264 epoch 4 - iter 12/20 - loss 0.15453157 - samples/sec: 2.35\n",
            "2020-06-28 04:24:57,949 epoch 4 - iter 14/20 - loss 0.14496650 - samples/sec: 2.19\n",
            "2020-06-28 04:25:12,810 epoch 4 - iter 16/20 - loss 0.15887536 - samples/sec: 2.16\n",
            "2020-06-28 04:25:26,678 epoch 4 - iter 18/20 - loss 0.14706133 - samples/sec: 2.31\n",
            "2020-06-28 04:25:41,423 epoch 4 - iter 20/20 - loss 0.15766838 - samples/sec: 2.18\n",
            "2020-06-28 04:25:41,651 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 04:25:41,652 EPOCH 4 done: loss 0.1577 - lr 0.0000300\n",
            "2020-06-28 04:25:43,602 DEV : loss 1.3506560325622559 - score 0.7\n",
            "2020-06-28 04:25:43,622 BAD EPOCHS (no improvement): 2\n",
            "2020-06-28 04:25:43,624 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 04:25:59,874 epoch 5 - iter 2/20 - loss 0.15955837 - samples/sec: 2.00\n",
            "2020-06-28 04:26:13,935 epoch 5 - iter 4/20 - loss 0.11795075 - samples/sec: 2.29\n",
            "2020-06-28 04:26:28,353 epoch 5 - iter 6/20 - loss 0.10042376 - samples/sec: 2.23\n",
            "2020-06-28 04:26:42,818 epoch 5 - iter 8/20 - loss 0.09266189 - samples/sec: 2.22\n",
            "2020-06-28 04:26:56,435 epoch 5 - iter 10/20 - loss 0.07712182 - samples/sec: 2.36\n",
            "2020-06-28 04:27:10,192 epoch 5 - iter 12/20 - loss 0.11398809 - samples/sec: 2.33\n",
            "2020-06-28 04:27:24,291 epoch 5 - iter 14/20 - loss 0.10874161 - samples/sec: 2.28\n",
            "2020-06-28 04:27:38,481 epoch 5 - iter 16/20 - loss 0.14719693 - samples/sec: 2.26\n",
            "2020-06-28 04:27:52,799 epoch 5 - iter 18/20 - loss 0.14617115 - samples/sec: 2.24\n",
            "2020-06-28 04:28:07,393 epoch 5 - iter 20/20 - loss 0.13222220 - samples/sec: 2.35\n",
            "2020-06-28 04:28:07,637 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 04:28:07,639 EPOCH 5 done: loss 0.1322 - lr 0.0000300\n",
            "2020-06-28 04:28:09,540 DEV : loss 1.390832781791687 - score 0.75\n",
            "2020-06-28 04:28:09,557 BAD EPOCHS (no improvement): 3\n",
            "2020-06-28 04:28:10,000 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 04:28:10,001 Testing using best model ...\n",
            "2020-06-28 04:28:10,012 loading file resources/MultiClassification/best-model.pt\n",
            "2020-06-28 04:28:14,358 0.5454545454545454\t0.5454545454545454\t0.5454545454545454\n",
            "2020-06-28 04:28:14,359 \n",
            "MICRO_AVG: acc 0.7727272727272727 - f1-score 0.5454545454545454\n",
            "MACRO_AVG: acc 0.7727272727272728 - f1-score 0.1818181818181818\n",
            "FALSE      tp: 36 - fp: 18 - fn: 9 - tn: 3 - precision: 0.6667 - recall: 0.8000 - accuracy: 0.5909 - f1-score: 0.7273\n",
            "TRUE       tp: 0 - fp: 5 - fn: 15 - tn: 46 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.6970 - f1-score: 0.0000\n",
            "Unverified tp: 0 - fp: 0 - fn: 6 - tn: 60 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9091 - f1-score: 0.0000\n",
            "not_a_rumor tp: 0 - fp: 7 - fn: 0 - tn: 59 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.8939 - f1-score: 0.0000\n",
            "2020-06-28 04:28:14,360 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dev_loss_history': [1.880383014678955,\n",
              "  1.1316317319869995,\n",
              "  1.3749545812606812,\n",
              "  1.3506560325622559,\n",
              "  1.390832781791687],\n",
              " 'dev_score_history': [0.5, 0.85, 0.7666666666666667, 0.7, 0.75],\n",
              " 'test_score': 0.7727272727272727,\n",
              " 'train_loss_history': [0.943682300299406,\n",
              "  0.51116629592143,\n",
              "  0.3175717959180474,\n",
              "  0.15766837527044117,\n",
              "  0.13222219847957603]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEbnyTFatG0W",
        "colab_type": "text"
      },
      "source": [
        "## Step 3:  Prediction on test data set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAsc_zNasW2v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d1a96d62-04de-4d9b-f5c5-bb17557f9676"
      },
      "source": [
        "predicted=[]\n",
        "classifier = TextClassifier.load('resources/MultiClassification/final-model.pt')\n",
        "for i,row in test.iterrows():\n",
        "  sent = Sentence(row[\"text\"])\n",
        "  classifier.predict(sent)\n",
        "  predicted.append(str(sent.labels[0]).split(\"(\")[0].replace(\" \",\"\"))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-28 04:28:14,421 loading file resources/MultiClassification/final-model.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuhuQXdhBGuy",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSfg45jFsKQ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "cfe9fbb8-ba04-4f14-838b-330fdc64b711"
      },
      "source": [
        "print(metrics.classification_report(test['rumor_veracity'].values, predicted))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       FALSE       0.65      0.53      0.59        45\n",
            "        TRUE       0.20      0.33      0.25        15\n",
            "  Unverified       0.00      0.00      0.00         6\n",
            " not_a_rumor       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.44        66\n",
            "   macro avg       0.21      0.22      0.21        66\n",
            "weighted avg       0.49      0.44      0.46        66\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6dUx2bOt19l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    }
  ]
}