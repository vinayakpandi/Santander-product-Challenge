{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spKnTVVsiZNT",
        "colab_type": "text"
      },
      "source": [
        "# Importing packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWXR27XBCuqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install emoji\n",
        "# !pip install flair\n",
        "import emoji\n",
        "import pandas as pd\n",
        "import re\n",
        "import io\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from flair.data import Sentence\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "import spacy\n",
        "import pickle"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDCAp5SWin7j",
        "colab_type": "text"
      },
      "source": [
        "# Reading file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kAkMsYCCzP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_t = pd.read_csv('FINAL.csv',encoding= 'unicode_escape')"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Na37eeit-R",
        "colab_type": "text"
      },
      "source": [
        "# Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8M0CN_gC4AR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocessing class\n",
        "class TextPreprocessor_new():\n",
        "    def __init__(self):\n",
        "        self.contractions = { \n",
        "        \"ain't\": \"am not\",\n",
        "        \"aren't\": \"are not\",\n",
        "        \"can't\": \"cannot\",\n",
        "        \"can't've\": \"cannot have\",\n",
        "        \"'cause\": \"because\",\n",
        "        \"could've\": \"could have\",\n",
        "        \"couldn't\": \"could not\",\n",
        "        \"couldn't've\": \"could not have\",\n",
        "        \"didn't\": \"did not\",\n",
        "        \"doesn't\": \"does not\",\n",
        "        \"don't\": \"do not\",\n",
        "        \"hadn't\": \"had not\",\n",
        "        \"hadn't've\": \"had not have\",\n",
        "        \"hasn't\": \"has not\",\n",
        "        \"haven't\": \"have not\",\n",
        "        \"he'd\": \"he would\",\n",
        "        \"he'd've\": \"he would have\",\n",
        "        \"he'll\": \"he will\",\n",
        "        \"he's\": \"he is\",\n",
        "        \"how'd\": \"how did\",\n",
        "        \"how'll\": \"how will\",\n",
        "        \"how's\": \"how is\",\n",
        "        \"i'd\": \"i would\",\n",
        "        \"i'll\": \"i will\",\n",
        "        \"i'm\": \"i am\",\n",
        "        \"i've\": \"i have\",\n",
        "        \"isn't\": \"is not\",\n",
        "        \"it'd\": \"it would\",\n",
        "        \"it'll\": \"it will\",\n",
        "        \"it's\": \"it is\",\n",
        "        \"let's\": \"let us\",\n",
        "        \"ma'am\": \"madam\",\n",
        "        \"mayn't\": \"may not\",\n",
        "        \"might've\": \"might have\",\n",
        "        \"mightn't\": \"might not\",\n",
        "        \"must've\": \"must have\",\n",
        "        \"mustn't\": \"must not\",\n",
        "        \"needn't\": \"need not\",\n",
        "        \"oughtn't\": \"ought not\",\n",
        "        \"shan't\": \"shall not\",\n",
        "        \"sha'n't\": \"shall not\",\n",
        "        \"she'd\": \"she would\",\n",
        "        \"she'll\": \"she will\",\n",
        "        \"she's\": \"she is\",\n",
        "        \"should've\": \"should have\",\n",
        "        \"shouldn't\": \"should not\",\n",
        "        \"that'd\": \"that would\",\n",
        "        \"that's\": \"that is\",\n",
        "        \"there'd\": \"there had\",\n",
        "        \"there's\": \"there is\",\n",
        "        \"they'd\": \"they would\",\n",
        "        \"they'll\": \"they will\",\n",
        "        \"they're\": \"they are\",\n",
        "        \"they've\": \"they have\",\n",
        "        \"wasn't\": \"was not\",\n",
        "        \"we'd\": \"we would\",\n",
        "        \"we'll\": \"we will\",\n",
        "        \"we're\": \"we are\",\n",
        "        \"we've\": \"we have\",\n",
        "        \"weren't\": \"were not\",\n",
        "        \"what'll\": \"what will\",\n",
        "        \"what're\": \"what are\",\n",
        "        \"what's\": \"what is\",\n",
        "        \"what've\": \"what have\",\n",
        "        \"where'd\": \"where did\",\n",
        "        \"where's\": \"where is\",\n",
        "        \"who'll\": \"who will\",\n",
        "        \"who's\": \"who is\",\n",
        "        \"won't\": \"will not\",\n",
        "        \"wouldn't\": \"would not\",\n",
        "        \"you'd\": \"you would\",\n",
        "        \"you'll\": \"you will\",\n",
        "        \"you're\": \"you are\",\n",
        "        \"thx\"   : \"thanks\",\n",
        "        \"ain’t\": \"am not\",\n",
        "        \"aren’t\": \"are not\",\n",
        "        \"can’t\": \"cannot\",\n",
        "        \"can’t’ve\": \"cannot have\",\n",
        "        \"could’ve\": \"could have\",\n",
        "        \"couldn’t\": \"could not\",\n",
        "        \"couldn’t’ve\": \"could not have\",\n",
        "        \"didn’t\": \"did not\",\n",
        "        \"doesn’t\": \"does not\",\n",
        "        \"don’t\": \"do not\",\n",
        "        \"hadn’t\": \"had not\",\n",
        "        \"hadn’t’ve\": \"had not have\",\n",
        "        \"hasn’t\": \"has not\",\n",
        "        \"haven’t\": \"have not\",\n",
        "        \"he’d\": \"he would\",\n",
        "        \"he’d’ve\": \"he would have\",\n",
        "        \"he’ll\": \"he will\",\n",
        "        \"he’s\": \"he is\",\n",
        "        \"how’d\": \"how did\",\n",
        "        \"how’ll\": \"how will\",\n",
        "        \"how’s\": \"how is\",\n",
        "        \"i’d\": \"i would\",\n",
        "        \"i’ll\": \"i will\",\n",
        "        \"i’m\": \"i am\",\n",
        "        \"i’ve\": \"i have\",\n",
        "        \"isn’t\": \"is not\",\n",
        "        \"it’d\": \"it would\",\n",
        "        \"it’ll\": \"it will\",\n",
        "        \"it’s\": \"it is\",\n",
        "        \"let’s\": \"let us\",\n",
        "        \"ma’am\": \"madam\",\n",
        "        \"mayn’t\": \"may not\",\n",
        "        \"might’ve\": \"might have\",\n",
        "        \"mightn’t\": \"might not\",\n",
        "        \"must’ve\": \"must have\",\n",
        "        \"mustn’t\": \"must not\",\n",
        "        \"needn’t\": \"need not\",\n",
        "        \"oughtn’t\": \"ought not\",\n",
        "        \"shan’t\": \"shall not\",\n",
        "        \"sha’n’t\": \"shall not\",\n",
        "        \"she’d\": \"she would\",\n",
        "        \"she’ll\": \"she will\",\n",
        "        \"she’s\": \"she is\",\n",
        "        \"should’ve\": \"should have\",\n",
        "        \"shouldn’t\": \"should not\",\n",
        "        \"that’d\": \"that would\",\n",
        "        \"that’s\": \"that is\",\n",
        "        \"there’d\": \"there had\",\n",
        "        \"there’s\": \"there is\",\n",
        "        \"they’d\": \"they would\",\n",
        "        \"they’ll\": \"they will\",\n",
        "        \"they’re\": \"they are\",\n",
        "        \"they’ve\": \"they have\",\n",
        "        \"wasn’t\": \"was not\",\n",
        "        \"we’d\": \"we would\",\n",
        "        \"we’ll\": \"we will\",\n",
        "        \"we’re\": \"we are\",\n",
        "        \"we’ve\": \"we have\",\n",
        "        \"weren’t\": \"were not\",\n",
        "        \"what’ll\": \"what will\",\n",
        "        \"what’re\": \"what are\",\n",
        "        \"what’s\": \"what is\",\n",
        "        \"what’ve\": \"what have\",\n",
        "        \"where’d\": \"where did\",\n",
        "        \"where’s\": \"where is\",\n",
        "        \"who’ll\": \"who will\",\n",
        "        \"who’s\": \"who is\",\n",
        "        \"won’t\": \"will not\",\n",
        "        \"wouldn’t\": \"would not\",\n",
        "        \"you’d\": \"you would\",\n",
        "        \"you’ll\": \"you will\",\n",
        "        \"you’re\": \"you are\"\n",
        "        }\n",
        "\n",
        "        self.emoticons_str = r\"\"\"\n",
        "            (?:\n",
        "                [:=;] # Eyes\n",
        "                [oO\\-]? # Nose (optional)\n",
        "                [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
        "            )\"\"\"\n",
        "\n",
        "        self.regex_str = [\n",
        "            self.emoticons_str,\n",
        "            r'<[^>]+>', # HTML tags\n",
        "            r'(?:@[\\w_]+)', # @-mentions\n",
        "            r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
        "            r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
        "\n",
        "            r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
        "        ]\n",
        "\n",
        "        self.tokens_re = re.compile(r'('+'|'.join(self.regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
        "        \n",
        "        self.spacy_nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner']) \n",
        "        self.spacy_nlp.add_pipe(self.remove_punctuations_emoji_symbol, name=\"stopwords\", last=True)\n",
        "    \n",
        "   \n",
        "    \n",
        "    def remove_punctuations_emoji_symbol(self,doc):\n",
        "\n",
        "        # This will remove stopwords and punctuation.\n",
        "        # Use token.text to return strings, which we'll need for Gensim.\n",
        "        doc = \" \".join([token.text.lower() for token in doc ])\n",
        "        doc = emoji.get_emoji_regexp().sub(r'', doc)  # get emoji free text\n",
        "        doc=\" \".join([self.contractions[token] if token in self.contractions.keys() else token for token in doc.split()]) # handle short words\n",
        "\n",
        "        doc = self.tokens_re.sub(r'', doc)   \n",
        "        doc = re.sub(r'\\s\\s+','', doc)\n",
        "        doc = re.sub(r'[ ]{2, }',' ',doc)\n",
        "\n",
        "        doc = re.sub(r'http\\S+','', doc)\n",
        "        doc = re.sub(r'@',' ',doc)\n",
        "        doc = re.sub(r'#',' ',doc)\n",
        "\n",
        "        return doc\n",
        "\n",
        "    def transform(self, X):  \n",
        "        \n",
        "        \n",
        "        X_new = self.spacy_nlp(X)\n",
        "        return  X_new"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLs_pRoEANmZ",
        "colab_type": "text"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYJ8ZRpgC4wy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = 'text'\n",
        "TextPreprocessor_new_obj = TextPreprocessor_new()\n",
        "df_t[x] = df_t[x].apply(lambda x:TextPreprocessor_new_obj.transform(x) )"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZX2M8r11_4m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "78a34262-8d3f-4702-e7b0-662e4c9252e9"
      },
      "source": [
        "df_t.shape"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(416, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dqBUGIpj1OT",
        "colab_type": "text"
      },
      "source": [
        "# Train , Test and Dev split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsEMP8OcVG5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = df_t[0:320]\n",
        "dev = df_t[320:350]\n",
        "test = df_t[350:]"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcurFH3iV9nf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.to_csv(\"./data/train.csv\",index=False)\n",
        "dev.to_csv(\"./data/dev.csv\",index=False)\n",
        "test.to_csv(\"./data/test.csv\",index=False)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XjpoDpyYOpV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "09264b57-a6ee-4769-da73-58afd46510c0"
      },
      "source": [
        "train.columns"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['created_at', 'text', 'source', 'is_quote', 'favourites_count',\n",
              "       'retweet_count', 'followers_count', 'friends_count',\n",
              "       'account_created_at', 'verified', 'is_rumor', 'rumor_veracity'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d38j0pBhAeW8",
        "colab_type": "text"
      },
      "source": [
        "# Rumar Checking - Binary classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKjCoRMYBUJK",
        "colab_type": "text"
      },
      "source": [
        "## Step1: Preparing corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acUQ2YTnBU7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import CSVClassificationCorpus"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyUKQ8wOBYKd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "b2f6b29e-3b08-47ae-b241-732d8ff2c894"
      },
      "source": [
        "data_folder = '/content/data'\n",
        "# column format indicating which columns hold the text and label(s)\n",
        "column_name_map = {1: \"text\", 10: \"label_topic\"}\n",
        "\n",
        "# load corpus containing training, test and dev data and if CSV has a header, you can skip it\n",
        "corpus: Corpus = CSVClassificationCorpus(data_folder,\n",
        "                                         column_name_map,\n",
        "                                         skip_header=True,\n",
        "                                         delimiter=',',    # tab-separated files\n",
        ") "
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-28 03:48:15,061 Reading data from /content/data\n",
            "2020-06-28 03:48:15,066 Train: /content/data/train.csv\n",
            "2020-06-28 03:48:15,067 Dev: /content/data/dev.csv\n",
            "2020-06-28 03:48:15,068 Test: /content/data/test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54lOcM_IBdwr",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJXY3PL3Bej9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim.adam import Adam\n",
        "\n",
        "from flair.data import Corpus\n",
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "from flair.models import TextClassifier\n",
        "from flair.trainers import ModelTrainer"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt4cgMaBBir8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "221aa075-0c94-4035-a9b2-edeba7abadae"
      },
      "source": [
        "# 2. create the label dictionary\n",
        "label_dict = corpus.make_label_dictionary()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-28 03:48:30,535 Computing label dictionary. Progress:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 386/386 [00:00<00:00, 657.80it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-06-28 03:48:31,459 [b'False', b'True']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8XK0GoSBmw0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "05520827-c805-4545-8342-8f70996676c4"
      },
      "source": [
        "\n",
        "# 3. initialize transformer document embeddings (many models are available)\n",
        "document_embeddings = TransformerDocumentEmbeddings('distilbert-base-uncased', fine_tune=True)\n",
        "\n",
        "# 4. create the text classifier\n",
        "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n",
        "\n",
        "# 5. initialize the text classifier trainer with Adam optimizer\n",
        "trainer = ModelTrainer(classifier, corpus, optimizer=Adam)\n",
        "\n",
        "# 6. start the training\n",
        "trainer.train('resources/Binary_classification',\n",
        "              learning_rate=3e-5, # use very small learning rate\n",
        "              mini_batch_size=16,\n",
        "              mini_batch_chunk_size=4, # optionally set this if transformer is too much for your machine\n",
        "              max_epochs=5, # terminate after 5 epochs\n",
        "              )"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-28 03:48:47,013 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:48:47,015 Model: \"TextClassifier(\n",
            "  (document_embeddings): TransformerDocumentEmbeddings(\n",
            "    (model): DistilBertModel(\n",
            "      (embeddings): Embeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (transformer): Transformer(\n",
            "        (layer): ModuleList(\n",
            "          (0): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (1): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (2): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (3): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (4): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (5): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): Linear(in_features=768, out_features=2, bias=True)\n",
            "  (loss_function): CrossEntropyLoss()\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2020-06-28 03:48:47,016 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:48:47,019 Corpus: \"Corpus: 320 train + 30 dev + 66 test sentences\"\n",
            "2020-06-28 03:48:47,021 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:48:47,023 Parameters:\n",
            "2020-06-28 03:48:47,026  - learning_rate: \"3e-05\"\n",
            "2020-06-28 03:48:47,027  - mini_batch_size: \"16\"\n",
            "2020-06-28 03:48:47,029  - patience: \"3\"\n",
            "2020-06-28 03:48:47,031  - anneal_factor: \"0.5\"\n",
            "2020-06-28 03:48:47,033  - max_epochs: \"5\"\n",
            "2020-06-28 03:48:47,035  - shuffle: \"True\"\n",
            "2020-06-28 03:48:47,038  - train_with_dev: \"False\"\n",
            "2020-06-28 03:48:47,039  - batch_growth_annealing: \"False\"\n",
            "2020-06-28 03:48:47,041 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:48:47,043 Model training base path: \"resources/Binary_classification\"\n",
            "2020-06-28 03:48:47,045 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:48:47,046 Device: cuda:0\n",
            "2020-06-28 03:48:47,048 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:48:47,050 Embeddings storage mode: cpu\n",
            "2020-06-28 03:48:47,055 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:48:48,395 epoch 1 - iter 2/20 - loss 0.23813771 - samples/sec: 32.57\n",
            "2020-06-28 03:48:49,261 epoch 1 - iter 4/20 - loss 0.30274013 - samples/sec: 42.60\n",
            "2020-06-28 03:48:50,112 epoch 1 - iter 6/20 - loss 0.45956570 - samples/sec: 42.50\n",
            "2020-06-28 03:48:50,967 epoch 1 - iter 8/20 - loss 0.45973747 - samples/sec: 43.24\n",
            "2020-06-28 03:48:51,783 epoch 1 - iter 10/20 - loss 0.50856244 - samples/sec: 44.62\n",
            "2020-06-28 03:48:52,603 epoch 1 - iter 12/20 - loss 0.47437489 - samples/sec: 44.36\n",
            "2020-06-28 03:48:53,452 epoch 1 - iter 14/20 - loss 0.53293799 - samples/sec: 44.11\n",
            "2020-06-28 03:48:54,248 epoch 1 - iter 16/20 - loss 0.52422761 - samples/sec: 45.51\n",
            "2020-06-28 03:48:55,092 epoch 1 - iter 18/20 - loss 0.52736897 - samples/sec: 42.81\n",
            "2020-06-28 03:48:55,888 epoch 1 - iter 20/20 - loss 0.51690564 - samples/sec: 45.90\n",
            "2020-06-28 03:48:56,144 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:48:56,145 EPOCH 1 done: loss 0.5169 - lr 0.0000300\n",
            "2020-06-28 03:48:56,822 DEV : loss 0.17895552515983582 - score 0.9667\n",
            "2020-06-28 03:48:56,849 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-06-28 03:48:57,407 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:48:58,561 epoch 2 - iter 2/20 - loss 0.22678055 - samples/sec: 37.29\n",
            "2020-06-28 03:48:59,437 epoch 2 - iter 4/20 - loss 0.34481403 - samples/sec: 43.21\n",
            "2020-06-28 03:49:00,279 epoch 2 - iter 6/20 - loss 0.26505596 - samples/sec: 43.18\n",
            "2020-06-28 03:49:01,118 epoch 2 - iter 8/20 - loss 0.25995196 - samples/sec: 43.14\n",
            "2020-06-28 03:49:01,934 epoch 2 - iter 10/20 - loss 0.27640288 - samples/sec: 45.62\n",
            "2020-06-28 03:49:02,769 epoch 2 - iter 12/20 - loss 0.24763183 - samples/sec: 43.97\n",
            "2020-06-28 03:49:03,609 epoch 2 - iter 14/20 - loss 0.28741528 - samples/sec: 42.93\n",
            "2020-06-28 03:49:04,448 epoch 2 - iter 16/20 - loss 0.26621050 - samples/sec: 44.19\n",
            "2020-06-28 03:49:05,261 epoch 2 - iter 18/20 - loss 0.28157172 - samples/sec: 44.89\n",
            "2020-06-28 03:49:06,081 epoch 2 - iter 20/20 - loss 0.27388301 - samples/sec: 44.24\n",
            "2020-06-28 03:49:06,347 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:49:06,348 EPOCH 2 done: loss 0.2739 - lr 0.0000300\n",
            "2020-06-28 03:49:07,037 DEV : loss 0.1749493032693863 - score 0.9\n",
            "2020-06-28 03:49:07,060 BAD EPOCHS (no improvement): 1\n",
            "2020-06-28 03:49:07,062 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:49:08,769 epoch 3 - iter 2/20 - loss 0.02197859 - samples/sec: 37.54\n",
            "2020-06-28 03:49:09,616 epoch 3 - iter 4/20 - loss 0.05573642 - samples/sec: 43.83\n",
            "2020-06-28 03:49:10,693 epoch 3 - iter 6/20 - loss 0.04522105 - samples/sec: 32.93\n",
            "2020-06-28 03:49:11,617 epoch 3 - iter 8/20 - loss 0.05088519 - samples/sec: 42.33\n",
            "2020-06-28 03:49:12,478 epoch 3 - iter 10/20 - loss 0.05101957 - samples/sec: 44.13\n",
            "2020-06-28 03:49:13,310 epoch 3 - iter 12/20 - loss 0.04344375 - samples/sec: 44.70\n",
            "2020-06-28 03:49:14,128 epoch 3 - iter 14/20 - loss 0.03962452 - samples/sec: 45.05\n",
            "2020-06-28 03:49:14,935 epoch 3 - iter 16/20 - loss 0.05223742 - samples/sec: 45.64\n",
            "2020-06-28 03:49:15,722 epoch 3 - iter 18/20 - loss 0.04732473 - samples/sec: 46.45\n",
            "2020-06-28 03:49:16,543 epoch 3 - iter 20/20 - loss 0.06330913 - samples/sec: 44.78\n",
            "2020-06-28 03:49:16,810 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:49:16,811 EPOCH 3 done: loss 0.0633 - lr 0.0000300\n",
            "2020-06-28 03:49:17,510 DEV : loss 0.24284397065639496 - score 0.9\n",
            "2020-06-28 03:49:17,528 BAD EPOCHS (no improvement): 2\n",
            "2020-06-28 03:49:17,530 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:49:18,720 epoch 4 - iter 2/20 - loss 0.00271156 - samples/sec: 38.63\n",
            "2020-06-28 03:49:19,599 epoch 4 - iter 4/20 - loss 0.00657528 - samples/sec: 41.97\n",
            "2020-06-28 03:49:20,471 epoch 4 - iter 6/20 - loss 0.01187442 - samples/sec: 43.10\n",
            "2020-06-28 03:49:21,288 epoch 4 - iter 8/20 - loss 0.00937780 - samples/sec: 44.51\n",
            "2020-06-28 03:49:22,132 epoch 4 - iter 10/20 - loss 0.00757813 - samples/sec: 43.59\n",
            "2020-06-28 03:49:22,965 epoch 4 - iter 12/20 - loss 0.00644051 - samples/sec: 44.70\n",
            "2020-06-28 03:49:23,780 epoch 4 - iter 14/20 - loss 0.00555893 - samples/sec: 44.66\n",
            "2020-06-28 03:49:24,608 epoch 4 - iter 16/20 - loss 0.00536501 - samples/sec: 44.04\n",
            "2020-06-28 03:49:25,422 epoch 4 - iter 18/20 - loss 0.00514197 - samples/sec: 45.40\n",
            "2020-06-28 03:49:26,268 epoch 4 - iter 20/20 - loss 0.00532484 - samples/sec: 43.57\n",
            "2020-06-28 03:49:26,521 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:49:26,523 EPOCH 4 done: loss 0.0053 - lr 0.0000300\n",
            "2020-06-28 03:49:27,228 DEV : loss 0.2100742757320404 - score 0.9333\n",
            "2020-06-28 03:49:27,246 BAD EPOCHS (no improvement): 3\n",
            "2020-06-28 03:49:27,247 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:49:28,450 epoch 5 - iter 2/20 - loss 0.00535114 - samples/sec: 36.54\n",
            "2020-06-28 03:49:29,322 epoch 5 - iter 4/20 - loss 0.00985578 - samples/sec: 42.75\n",
            "2020-06-28 03:49:30,152 epoch 5 - iter 6/20 - loss 0.00668733 - samples/sec: 43.87\n",
            "2020-06-28 03:49:31,016 epoch 5 - iter 8/20 - loss 0.00518430 - samples/sec: 43.12\n",
            "2020-06-28 03:49:31,838 epoch 5 - iter 10/20 - loss 0.00434268 - samples/sec: 44.31\n",
            "2020-06-28 03:49:32,621 epoch 5 - iter 12/20 - loss 0.00363982 - samples/sec: 46.79\n",
            "2020-06-28 03:49:33,467 epoch 5 - iter 14/20 - loss 0.00317036 - samples/sec: 42.96\n",
            "2020-06-28 03:49:34,303 epoch 5 - iter 16/20 - loss 0.00280401 - samples/sec: 43.97\n",
            "2020-06-28 03:49:35,128 epoch 5 - iter 18/20 - loss 0.00250276 - samples/sec: 44.17\n",
            "2020-06-28 03:49:35,940 epoch 5 - iter 20/20 - loss 0.00232903 - samples/sec: 45.18\n",
            "2020-06-28 03:49:36,189 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:49:36,190 EPOCH 5 done: loss 0.0023 - lr 0.0000300\n",
            "2020-06-28 03:49:36,872 DEV : loss 0.43733978271484375 - score 0.9\n",
            "Epoch     5: reducing learning rate of group 0 to 1.5000e-05.\n",
            "2020-06-28 03:49:36,899 BAD EPOCHS (no improvement): 4\n",
            "2020-06-28 03:49:37,443 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:49:37,444 Testing using best model ...\n",
            "2020-06-28 03:49:37,446 loading file resources/Binary_classification/best-model.pt\n",
            "2020-06-28 03:49:39,673 0.9393939393939394\t0.9393939393939394\t0.9393939393939394\n",
            "2020-06-28 03:49:39,675 \n",
            "MICRO_AVG: acc 0.9393939393939394 - f1-score 0.9393939393939394\n",
            "MACRO_AVG: acc 0.9393939393939394 - f1-score 0.484375\n",
            "False      tp: 0 - fp: 4 - fn: 0 - tn: 62 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9394 - f1-score: 0.0000\n",
            "True       tp: 62 - fp: 0 - fn: 4 - tn: 0 - precision: 1.0000 - recall: 0.9394 - accuracy: 0.9394 - f1-score: 0.9688\n",
            "2020-06-28 03:49:39,676 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dev_loss_history': [0.17895552515983582,\n",
              "  0.1749493032693863,\n",
              "  0.24284397065639496,\n",
              "  0.2100742757320404,\n",
              "  0.43733978271484375],\n",
              " 'dev_score_history': [0.9666666666666667, 0.9, 0.9, 0.9333333333333333, 0.9],\n",
              " 'test_score': 0.9393939393939394,\n",
              " 'train_loss_history': [0.5169056363403797,\n",
              "  0.2738830089569092,\n",
              "  0.06330912783741952,\n",
              "  0.005324843525886536,\n",
              "  0.002329032123088837]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_RbFXZ5BqzJ",
        "colab_type": "text"
      },
      "source": [
        "## Step 3:  Prediction on test data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXByGb0DBrkj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "734d1e0b-875d-484a-ac68-32997fb2b19d"
      },
      "source": [
        "predicted=[]\n",
        "classifier = TextClassifier.load('resources/Binary_classification/final-model.pt')\n",
        "for i,row in test.iterrows():\n",
        "  sent = Sentence(row[\"text\"])\n",
        "  classifier.predict(sent)\n",
        "  predicted.append(bool(str(sent.labels[0]).split(\"(\")[0].replace(\" \",\"\")))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-28 03:54:05,127 loading file resources/Binary_classification/final-model.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQdMSyOCBvA8",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1x5n79kByvD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "89e6a889-aedc-4b02-ae36-5a2b965f34b2"
      },
      "source": [
        "print(metrics.classification_report(test['is_rumor'].values, predicted))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        True       1.00      1.00      1.00        66\n",
            "\n",
            "    accuracy                           1.00        66\n",
            "   macro avg       1.00      1.00      1.00        66\n",
            "weighted avg       1.00      1.00      1.00        66\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI0a0jelAoa_",
        "colab_type": "text"
      },
      "source": [
        "# Rumour Veracity - Multiple classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnr0t14cj9Aq",
        "colab_type": "text"
      },
      "source": [
        "## Step1: Preparing corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4Q3ULlvVHfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import CSVClassificationCorpus"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZm364jDYQWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "bb22ede2-507a-4524-f606-10f54a2851d7"
      },
      "source": [
        "data_folder = '/content/data'\n",
        "# column format indicating which columns hold the text and label(s)\n",
        "column_name_map = {1: \"text\", 11: \"label_topic\"}\n",
        "\n",
        "# load corpus containing training, test and dev data and if CSV has a header, you can skip it\n",
        "corpus: Corpus = CSVClassificationCorpus(data_folder,\n",
        "                                         column_name_map,\n",
        "                                         skip_header=True,\n",
        "                                         delimiter=',',    # tab-separated files\n",
        ") "
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-28 03:58:22,547 Reading data from /content/data\n",
            "2020-06-28 03:58:22,552 Train: /content/data/train.csv\n",
            "2020-06-28 03:58:22,553 Dev: /content/data/dev.csv\n",
            "2020-06-28 03:58:22,554 Test: /content/data/test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otiOLMgTkFDs",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s89OX7XbmjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim.adam import Adam\n",
        "\n",
        "from flair.data import Corpus\n",
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "from flair.models import TextClassifier\n",
        "from flair.trainers import ModelTrainer"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtlsd5Tkb0AY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "a3e63954-b5a9-44f6-f77b-1567dbea9ce3"
      },
      "source": [
        "# 2. create the label dictionary\n",
        "label_dict = corpus.make_label_dictionary()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-28 03:58:31,786 Computing label dictionary. Progress:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 386/386 [00:00<00:00, 637.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-06-28 03:58:32,732 [b'not_a_rumor', b'Unverified', b'TRUE', b'FALSE']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IbcgF-RcIUs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6bfd9fe6-25dc-4658-a262-df2335b9ba04"
      },
      "source": [
        "\n",
        "# 3. initialize transformer document embeddings (many models are available)\n",
        "document_embeddings = TransformerDocumentEmbeddings('distilbert-base-uncased', fine_tune=True)\n",
        "\n",
        "# 4. create the text classifier\n",
        "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n",
        "\n",
        "# 5. initialize the text classifier trainer with Adam optimizer\n",
        "trainer = ModelTrainer(classifier, corpus, optimizer=Adam)\n",
        "\n",
        "# 6. start the training\n",
        "trainer.train('resources/MultiClassification',\n",
        "              learning_rate=3e-5, # use very small learning rate\n",
        "              mini_batch_size=16,\n",
        "              mini_batch_chunk_size=4, # optionally set this if transformer is too much for your machine\n",
        "              max_epochs=5, # terminate after 5 epochs\n",
        "              )"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-28 03:58:40,160 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:58:40,161 Model: \"TextClassifier(\n",
            "  (document_embeddings): TransformerDocumentEmbeddings(\n",
            "    (model): DistilBertModel(\n",
            "      (embeddings): Embeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (transformer): Transformer(\n",
            "        (layer): ModuleList(\n",
            "          (0): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (1): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (2): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (3): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (4): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (5): TransformerBlock(\n",
            "            (attention): MultiHeadSelfAttention(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (ffn): FFN(\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            )\n",
            "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): Linear(in_features=768, out_features=4, bias=True)\n",
            "  (loss_function): CrossEntropyLoss()\n",
            "  (beta): 1.0\n",
            "  (weights): None\n",
            "  (weight_tensor) None\n",
            ")\"\n",
            "2020-06-28 03:58:40,162 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:58:40,165 Corpus: \"Corpus: 320 train + 30 dev + 66 test sentences\"\n",
            "2020-06-28 03:58:40,167 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:58:40,169 Parameters:\n",
            "2020-06-28 03:58:40,171  - learning_rate: \"3e-05\"\n",
            "2020-06-28 03:58:40,175  - mini_batch_size: \"16\"\n",
            "2020-06-28 03:58:40,176  - patience: \"3\"\n",
            "2020-06-28 03:58:40,179  - anneal_factor: \"0.5\"\n",
            "2020-06-28 03:58:40,181  - max_epochs: \"5\"\n",
            "2020-06-28 03:58:40,183  - shuffle: \"True\"\n",
            "2020-06-28 03:58:40,187  - train_with_dev: \"False\"\n",
            "2020-06-28 03:58:40,188  - batch_growth_annealing: \"False\"\n",
            "2020-06-28 03:58:40,190 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:58:40,191 Model training base path: \"resources/MultiClassification\"\n",
            "2020-06-28 03:58:40,194 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:58:40,196 Device: cuda:0\n",
            "2020-06-28 03:58:40,198 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:58:40,200 Embeddings storage mode: cpu\n",
            "2020-06-28 03:58:40,204 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:58:41,501 epoch 1 - iter 2/20 - loss 1.41221052 - samples/sec: 35.10\n",
            "2020-06-28 03:58:42,364 epoch 1 - iter 4/20 - loss 1.01930700 - samples/sec: 43.97\n",
            "2020-06-28 03:58:43,203 epoch 1 - iter 6/20 - loss 0.80640116 - samples/sec: 44.37\n",
            "2020-06-28 03:58:44,073 epoch 1 - iter 8/20 - loss 0.71563666 - samples/sec: 42.29\n",
            "2020-06-28 03:58:44,883 epoch 1 - iter 10/20 - loss 0.88889694 - samples/sec: 44.92\n",
            "2020-06-28 03:58:45,697 epoch 1 - iter 12/20 - loss 0.86496093 - samples/sec: 44.74\n",
            "2020-06-28 03:58:46,514 epoch 1 - iter 14/20 - loss 0.91435127 - samples/sec: 45.25\n",
            "2020-06-28 03:58:47,322 epoch 1 - iter 16/20 - loss 0.91145203 - samples/sec: 45.62\n",
            "2020-06-28 03:58:48,141 epoch 1 - iter 18/20 - loss 0.94960439 - samples/sec: 44.32\n",
            "2020-06-28 03:58:48,944 epoch 1 - iter 20/20 - loss 0.91537330 - samples/sec: 46.18\n",
            "2020-06-28 03:58:49,219 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:58:49,220 EPOCH 1 done: loss 0.9154 - lr 0.0000300\n",
            "2020-06-28 03:58:49,939 DEV : loss 1.2338298559188843 - score 0.6833\n",
            "2020-06-28 03:58:49,962 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-06-28 03:58:51,353 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:58:52,546 epoch 2 - iter 2/20 - loss 0.76051104 - samples/sec: 36.98\n",
            "2020-06-28 03:58:54,027 epoch 2 - iter 4/20 - loss 0.67474948 - samples/sec: 43.90\n",
            "2020-06-28 03:58:54,852 epoch 2 - iter 6/20 - loss 0.70802061 - samples/sec: 45.15\n",
            "2020-06-28 03:58:55,683 epoch 2 - iter 8/20 - loss 0.70509089 - samples/sec: 43.98\n",
            "2020-06-28 03:58:56,511 epoch 2 - iter 10/20 - loss 0.62364751 - samples/sec: 45.38\n",
            "2020-06-28 03:58:57,365 epoch 2 - iter 12/20 - loss 0.57392723 - samples/sec: 42.51\n",
            "2020-06-28 03:58:58,202 epoch 2 - iter 14/20 - loss 0.56351445 - samples/sec: 44.21\n",
            "2020-06-28 03:58:59,044 epoch 2 - iter 16/20 - loss 0.51071406 - samples/sec: 43.46\n",
            "2020-06-28 03:58:59,842 epoch 2 - iter 18/20 - loss 0.51872195 - samples/sec: 45.57\n",
            "2020-06-28 03:59:00,658 epoch 2 - iter 20/20 - loss 0.53011702 - samples/sec: 45.34\n",
            "2020-06-28 03:59:00,938 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:59:00,939 EPOCH 2 done: loss 0.5301 - lr 0.0000300\n",
            "2020-06-28 03:59:01,652 DEV : loss 1.0922685861587524 - score 0.7833\n",
            "2020-06-28 03:59:01,670 BAD EPOCHS (no improvement): 0\n",
            "saving best model\n",
            "2020-06-28 03:59:02,327 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:59:03,531 epoch 3 - iter 2/20 - loss 1.25424844 - samples/sec: 36.06\n",
            "2020-06-28 03:59:04,389 epoch 3 - iter 4/20 - loss 0.84895106 - samples/sec: 42.87\n",
            "2020-06-28 03:59:05,247 epoch 3 - iter 6/20 - loss 0.59472549 - samples/sec: 42.73\n",
            "2020-06-28 03:59:06,142 epoch 3 - iter 8/20 - loss 0.59491825 - samples/sec: 41.01\n",
            "2020-06-28 03:59:07,008 epoch 3 - iter 10/20 - loss 0.52632971 - samples/sec: 43.14\n",
            "2020-06-28 03:59:07,831 epoch 3 - iter 12/20 - loss 0.47658422 - samples/sec: 44.25\n",
            "2020-06-28 03:59:08,647 epoch 3 - iter 14/20 - loss 0.43906296 - samples/sec: 44.70\n",
            "2020-06-28 03:59:09,468 epoch 3 - iter 16/20 - loss 0.40640143 - samples/sec: 45.12\n",
            "2020-06-28 03:59:10,279 epoch 3 - iter 18/20 - loss 0.36752989 - samples/sec: 44.90\n",
            "2020-06-28 03:59:11,128 epoch 3 - iter 20/20 - loss 0.35251985 - samples/sec: 43.66\n",
            "2020-06-28 03:59:11,392 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:59:11,398 EPOCH 3 done: loss 0.3525 - lr 0.0000300\n",
            "2020-06-28 03:59:12,114 DEV : loss 1.3076529502868652 - score 0.7333\n",
            "2020-06-28 03:59:12,132 BAD EPOCHS (no improvement): 1\n",
            "2020-06-28 03:59:12,136 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:59:13,330 epoch 4 - iter 2/20 - loss 0.07233158 - samples/sec: 39.31\n",
            "2020-06-28 03:59:14,202 epoch 4 - iter 4/20 - loss 0.16562629 - samples/sec: 42.94\n",
            "2020-06-28 03:59:15,059 epoch 4 - iter 6/20 - loss 0.16289840 - samples/sec: 43.05\n",
            "2020-06-28 03:59:15,911 epoch 4 - iter 8/20 - loss 0.15323795 - samples/sec: 43.10\n",
            "2020-06-28 03:59:16,802 epoch 4 - iter 10/20 - loss 0.15908350 - samples/sec: 42.39\n",
            "2020-06-28 03:59:17,629 epoch 4 - iter 12/20 - loss 0.13800005 - samples/sec: 44.14\n",
            "2020-06-28 03:59:18,443 epoch 4 - iter 14/20 - loss 0.13238801 - samples/sec: 45.14\n",
            "2020-06-28 03:59:19,254 epoch 4 - iter 16/20 - loss 0.14403065 - samples/sec: 45.10\n",
            "2020-06-28 03:59:20,072 epoch 4 - iter 18/20 - loss 0.13247817 - samples/sec: 45.56\n",
            "2020-06-28 03:59:20,894 epoch 4 - iter 20/20 - loss 0.12219503 - samples/sec: 44.86\n",
            "2020-06-28 03:59:21,153 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:59:21,154 EPOCH 4 done: loss 0.1222 - lr 0.0000300\n",
            "2020-06-28 03:59:21,891 DEV : loss 1.426966905593872 - score 0.7333\n",
            "2020-06-28 03:59:21,910 BAD EPOCHS (no improvement): 2\n",
            "2020-06-28 03:59:21,911 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:59:23,094 epoch 5 - iter 2/20 - loss 0.01988989 - samples/sec: 37.38\n",
            "2020-06-28 03:59:23,938 epoch 5 - iter 4/20 - loss 0.08300553 - samples/sec: 44.15\n",
            "2020-06-28 03:59:24,805 epoch 5 - iter 6/20 - loss 0.05666110 - samples/sec: 42.37\n",
            "2020-06-28 03:59:25,656 epoch 5 - iter 8/20 - loss 0.05407642 - samples/sec: 43.06\n",
            "2020-06-28 03:59:26,496 epoch 5 - iter 10/20 - loss 0.07609167 - samples/sec: 45.48\n",
            "2020-06-28 03:59:27,389 epoch 5 - iter 12/20 - loss 0.06780780 - samples/sec: 40.96\n",
            "2020-06-28 03:59:28,224 epoch 5 - iter 14/20 - loss 0.08099212 - samples/sec: 43.81\n",
            "2020-06-28 03:59:29,055 epoch 5 - iter 16/20 - loss 0.10516284 - samples/sec: 44.18\n",
            "2020-06-28 03:59:29,896 epoch 5 - iter 18/20 - loss 0.11042814 - samples/sec: 43.60\n",
            "2020-06-28 03:59:30,722 epoch 5 - iter 20/20 - loss 0.13419270 - samples/sec: 44.68\n",
            "2020-06-28 03:59:30,995 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:59:30,996 EPOCH 5 done: loss 0.1342 - lr 0.0000300\n",
            "2020-06-28 03:59:31,740 DEV : loss 1.601783037185669 - score 0.7333\n",
            "2020-06-28 03:59:31,760 BAD EPOCHS (no improvement): 3\n",
            "2020-06-28 03:59:32,438 ----------------------------------------------------------------------------------------------------\n",
            "2020-06-28 03:59:32,483 Testing using best model ...\n",
            "2020-06-28 03:59:32,486 loading file resources/MultiClassification/best-model.pt\n",
            "2020-06-28 03:59:35,138 0.4696969696969697\t0.4696969696969697\t0.4696969696969697\n",
            "2020-06-28 03:59:35,140 \n",
            "MICRO_AVG: acc 0.7348484848484849 - f1-score 0.4696969696969697\n",
            "MACRO_AVG: acc 0.7348484848484849 - f1-score 0.20477272727272727\n",
            "FALSE      tp: 29 - fp: 14 - fn: 16 - tn: 7 - precision: 0.6744 - recall: 0.6444 - accuracy: 0.5455 - f1-score: 0.6591\n",
            "TRUE       tp: 2 - fp: 8 - fn: 13 - tn: 43 - precision: 0.2000 - recall: 0.1333 - accuracy: 0.6818 - f1-score: 0.1600\n",
            "Unverified tp: 0 - fp: 0 - fn: 6 - tn: 60 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.9091 - f1-score: 0.0000\n",
            "not_a_rumor tp: 0 - fp: 13 - fn: 0 - tn: 53 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.8030 - f1-score: 0.0000\n",
            "2020-06-28 03:59:35,145 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dev_loss_history': [1.2338298559188843,\n",
              "  1.0922685861587524,\n",
              "  1.3076529502868652,\n",
              "  1.426966905593872,\n",
              "  1.601783037185669],\n",
              " 'dev_score_history': [0.6833333333333333,\n",
              "  0.7833333333333333,\n",
              "  0.7333333333333333,\n",
              "  0.7333333333333333,\n",
              "  0.7333333333333333],\n",
              " 'test_score': 0.7348484848484849,\n",
              " 'train_loss_history': [0.9153732985258103,\n",
              "  0.5301170244812965,\n",
              "  0.3525198459625244,\n",
              "  0.12219503074884415,\n",
              "  0.13419269770383835]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEbnyTFatG0W",
        "colab_type": "text"
      },
      "source": [
        "## Step 3:  Prediction on test data set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAsc_zNasW2v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c0a8d706-b8c7-4526-fb92-b500e50c714b"
      },
      "source": [
        "predicted=[]\n",
        "classifier = TextClassifier.load('resources/MultiClassification/final-model.pt')\n",
        "for i,row in test.iterrows():\n",
        "  sent = Sentence(row[\"text\"])\n",
        "  classifier.predict(sent)\n",
        "  predicted.append(str(sent.labels[0]).split(\"(\")[0].replace(\" \",\"\"))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-28 03:59:44,306 loading file resources/MultiClassification/final-model.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuhuQXdhBGuy",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSfg45jFsKQ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "fdd4fa02-7feb-4195-b366-b5cc9f82b684"
      },
      "source": [
        "print(metrics.classification_report(test['rumor_veracity'].values, predicted))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       FALSE       0.68      0.58      0.63        45\n",
            "        TRUE       0.25      0.33      0.29        15\n",
            "  Unverified       0.00      0.00      0.00         6\n",
            " not_a_rumor       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.47        66\n",
            "   macro avg       0.23      0.23      0.23        66\n",
            "weighted avg       0.52      0.47      0.49        66\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6dUx2bOt19l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}